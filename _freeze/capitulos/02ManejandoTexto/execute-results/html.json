{
  "hash": "3f80c6c7a81eec2b5e0c9b5f48f5ef9e",
  "result": {
    "markdown": "# Manejando Texto {#sec-manejando-texto}\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {#3cf332e3 .cell execution_count=1}\n``` {.python .cell-code}\nfrom microtc.params import OPTION_GROUP, OPTION_DELETE,\\\n                           OPTION_NONE\nfrom microtc.textmodel import SKIP_SYMBOLS\nfrom b4msa.textmodel import TextModel\nfrom b4msa.lang_dependency import LangDependency\nfrom nltk.stem.snowball import SnowballStemmer\nimport unicodedata\nimport re\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Introducción \n\n\nSe podría suponer que el texto se que se analizará está bien escrito y tiene un formato adecuado para su procesamiento. Desafortunadamente, la realidad es que en la mayoría de aplicaciones el texto que se analiza tiene errores de ortográficos, errores de formato y además no es trivial identificar la unidad mínima de procesamiento que podría ser de manera natural, en el español, las palabras. Por este motivo, esta unidad trata técnicas comunes que se utilizan para normalizar el texto, esta normalización es un proceso previo al desarrollo de los algoritmos de PLN. \n\nLa @fig-pre-procesamiento esquematiza el procedimiento que se presenta en esta unidad, la idea es que se un texto pasa primeramente a un proceso de normalización (@sec-normalizacion y @sec-normalizacion-semantica), para después ser segmentado (ver @sec-segmentacion) y el resultado es lo que se utiliza para modelar el lenguaje. \n\n\n\n```{mermaid}\n%%| echo: false\n%%| fig-cap: Diagrama de Pre-procesamiento\n%%| label: fig-pre-procesamiento\n\nflowchart LR\n    Entrada([Texto]) -->  Norm[Normalización de Texto]\n    Norm --> Seg[Segmentación]\n    Seg --> Terminos(...)\n```\n\n\n\n\n\nLas normalizaciones y segmentaciones descritas en esta unidad se basan principalmente en las utilizadas en los siguientes artículos científicos.\n\n1. [An automated text categorization framework based on hyperparameter optimization](https://www.sciencedirect.com/science/article/pii/S0950705118301217) (@microTC)\n2. [A simple approach to multilingual polarity classification in Twitter](https://www.sciencedirect.com/science/article/abs/pii/S0167865517301721) (@B4MSA)\n3. [A case study of Spanish text transformations for twitter sentiment analysis](https://www.sciencedirect.com/science/article/abs/pii/S0957417417302312) (@TELLEZ2017)\n\n## Normalización de Texto Sintáctica {#sec-normalizacion}\n\nLa descripción de las normalizaciones empieza presentando las que se puede aplicar a nivel de caracteres, sin la necesidad de conocer el significado de las palabras. También se agrupan en este conjunto aquellas transformaciones que se realizan mediante expresiones regulares o su búsqueda en una lista de palabras previamente definidas. \n\n\n### Entidades {#sec-normalizacion-entidades}\n\nLa descripción de diferentes técnicas de normalización empieza con el manejo de entidades en el texto. Algunas entidades que se tratarán serán los nombres de usuario, números o URLs mencionados en un texto. Por otro lado están las acciones que se realizarán a las entidades encontradas, estas acciones corresponden a su borrado o remplazo por algún otro toquen. \n\n#### Usuarios\n\nEn esta sección se trabajará con los nombres de usuarios que siguen el formato usado por Twitter. En un tuit, los nombres de usuarios son aquellas palabras que inician con el caracter @ y terminan con un espacio o caracter terminal. Las acciones que se realizarán con los nombres de usuario encontrados serán su borrado o reemplazo por una etiqueta en particular. \n\nEl procedimiento para encontrar los nombres de usuarios es mediante expresiones regulares, en particular se usa la expresión `@\\S+`, tal y como se muestra en el siguiente ejemplo.\n\n::: {#b266a31e .cell execution_count=3}\n``` {.python .cell-code}\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"\", text)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n'Hola   te está buscando'\n```\n:::\n:::\n\n\nLa segunda acción es reemplazar cada nombre de usuario por una etiqueta particular, en el siguiente ejemplo se reemplaza por la etiqueta `_usr`.\n\n::: {#203b072f .cell execution_count=4}\n``` {.python .cell-code}\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"_usr\", text)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n'Hola _usr _usr te está buscando'\n```\n:::\n:::\n\n\n#### URL\n\nLos ejemplos anteriores se pueden adaptar para manejar URL; solamente es necesario adecuar la expresión regular que identifica una URL. En el siguiente ejemplo se muestra como se pueden borrar las URLs que aparecen en un texto. \n\n::: {#5fb48186 .cell execution_count=5}\n``` {.python .cell-code}\ntext = \"puedes verificar que http://google.com esté funcionando\"\nre.sub(r\"https?://\\S+\", \"\", text)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n'puedes verificar que  esté funcionando'\n```\n:::\n:::\n\n\n#### Números\n\nThe previous code can be modified to deal with numbers and replace the number found with a shared label such as `_num`.\n\n::: {#482bef0e .cell execution_count=6}\n``` {.python .cell-code}\ntext = \"acabamos de ganar 10 M\"\nre.sub(r\"\\d\\d*\\.?\\d*|\\d*\\.\\d\\d*\", \"_num\", text)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n'acabamos de ganar _num M'\n```\n:::\n:::\n\n\n### Ortografía {#sec-normalizacion-ortografia}\n\nEl siguiente bloque de normalizaciones agrupa aquellas modificaciones que se realizan a algún componente de tal manera que aunque impacta en su ortografía puede ser utilizado para reducir la dimensión y se ve reflejado en la complejidad del algoritmo.\n\n#### Mayúsculas y Minúsculas\n\nLa primera de estas transformaciones es convertir todas los caracteres a minúsculas. Como se puede observar esta transformación hace que el vocabulario se reduzca, por ejemplo, las palabras *México* o *MÉXICO* son representados por la palabra *méxico*. Esta operación se puede realizar con la función `lower` tal y cómo se muestra a continuación. \n\n::: {#f6f4ef14 .cell execution_count=7}\n``` {.python .cell-code}\ntext = \"México\"\ntext.lower()\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n'méxico'\n```\n:::\n:::\n\n\n#### Signos de Puntuación\n\nLos signos de puntuación son necesarios para tareas como la generación de textos, pero existen otras aplicaciones donde los signos de puntuación tienen un efecto positivo en el rendimiento del algorithm, este es el caso de tareas de categorización de texto. El efecto que tiene el quitar los signos de puntuación es que el vocabulario se reduce. Los símbolos de puntuación se pueden remover teniendo una lista de los mismos, esta lista de signos de puntuación se encuentra en la variable `SKIP_SYMBOLS` y el siguiente código muestra un procedimiento para quitarlos. \n\n::: {#8d1a4ecb .cell execution_count=8}\n``` {.python .cell-code}\ntext = \"¡Hola! buenos días:\"\noutput = \"\"\nfor x in text:\n    if x in SKIP_SYMBOLS:\n        continue\n    output += x\noutput\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n'Hola buenos días'\n```\n:::\n:::\n\n\n#### Símbolos Diacríticos\n\nContinuando con la misma idea de reducir el vocabulario, es común eliminar los símbolos diacríticos en las palabras. Esta transformación también tiene el objetivo de normalizar aquellos textos informales donde los símbolos diacríticos son usado con una menor frecuencia, en particular los acentos en el caso del español. Por ejemplo, es común encontrar la palabra *México* escrita como *Mexico*. \n\nEl siguiente código muestra un procedimiento para eliminar los símbolos diacríticos.  \n\n::: {#f743847f .cell execution_count=9}\n``` {.python .cell-code}\ntext = 'México'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 <= o and o <= 0x036F:\n        continue\n    output += x\noutput\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\n'Mexico'\n```\n:::\n:::\n\n\n## Normalización Semántica {#sec-normalizacion-semantica}\n\nLas siguientes normalizaciones comparten el objetivo con las normalizaciones presentadas hasta este momento, el cual es la reducción del vocabulario; la diferencia es que las siguientes utilizan el significado o uso de la palabra. \n\n### Palabras Comunes\n\nLas palabras comunes (*stop words*) son palabras utilizadas frecuentemente en el lenguaje, las cuales son necesarias para comunicación, pero no aportan información para discriminar un texto de acuerdo a su significado. \n\nThe stop words are the most frequent words used in the language. These words are essential to communicate but are not so much on tasks where the aim is to discriminate texts according to their meaning. \n\nLas palabras vacías se pueden guardar en un diccionario y el proceso de identificación consiste en buscar la existencia de la palabra en el diccionario. Una vez que la palabra analizada se encuentra en el diccionario, se procede a quitarla o cambiarla por un token particular. El proceso de borrado se muestra en el siguiente código. \n\n::: {#6e6e8e7d .cell execution_count=10}\n``` {.python .cell-code}\nlang = LangDependency('spanish')\n\ntext = '¡Buenos días! El día de hoy tendremos un día cálido.'\noutput = []\nfor word in text.split():\n    if word.lower() in lang.stopwords[len(word)]:\n        continue\n    output.append(word)\noutput = \" \".join(output) \noutput\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n'¡Buenos días! día hoy día cálido.'\n```\n:::\n:::\n\n\n### Lematización y Reducción a la Raíz\n\nLa idea de lematización y reducción a la raíz (*stemming*) es transformar una palabra a su raíz mediante un proceso heurístico o morfológico. Por ejemplo, las palabras *jugando* o *jugaron* se transforman a la palabra *jugar*.\n\nEl siguiente código muestra el proceso de reducción a la raíz utilizando la clase `SnowballStemmer`. \n\n::: {#c8a60fce .cell execution_count=11}\n``` {.python .cell-code}\nstemmer = SnowballStemmer('spanish')\n\ntext = 'Estoy jugando futbol con mis amigos'\noutput = []\nfor word in text.split():\n    w = stemmer.stem(word)\n    output.append(w)\noutput = \" \".join(output) \noutput\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n'estoy jug futbol con mis amig'\n```\n:::\n:::\n\n\n## Segmentación {#sec-segmentacion}\n\nUna vez que el texto ha sido normalizado es necesario segmentarlo (*tokenize*) a sus componentes fundamentales, e.g., palabras o gramas de caracteres (q-grams) o de palabras (n-grams). Existen diferentes métodos para segmentar un texto, probablemente una de las más sencillas es asumir que una palabra está limitada entre dos espacios o signos de puntuación. Partiendo de las palabras encontradas se empiezan a generar los gramas de palabras, e.g., bigramas, o los gramas de caracteres si se desea solo generarlos a partir de las palabras. \n\n### Gramas de Palabras (n-grams)\n\nEl primer método de segmentación revisado es la creación de los gramas de palabras. El primer paso es encontrar las palabras las cuales se pueden encontrar mediante la función `split`; una vez que las palabras están definidas estás se pueden unir para generar los gramas de palabras del tamaño deseado, tal y como se muestra en el siguiente código. \n\n::: {#311c7e90 .cell execution_count=12}\n``` {.python .cell-code}\ntext = 'Estoy jugando futbol con mis amigos'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['Estoy~jugando~futbol',\n 'jugando~futbol~con',\n 'futbol~con~mis',\n 'con~mis~amigos']\n```\n:::\n:::\n\n\n### Gramas de Caracteres (q-grams)\n\nLa segmentación de gramas de caracteres complementa los gramas de palabras. Los gramas de caracteres están definidos como la subcadena de longitud $q$. Este tipo de segmentación tiene la característica de que es agnóstica al lenguaje, es decir, se puede aplicar en cualquier idioma; contrastando, los gramas de palabras se pueden aplicar solo a los lenguajes que tienen definido el concepto de palabra, por ejemplo en el idioma chino las palabras no se pueden identificar como se pueden identificar en el español o inglés. La segunda característica importante es que ayuda en el problema de errores ortográficos, siguiendo una perspectiva de similitud aproximada. \n\nEl código para realizar los gramas de caracteres es similar a la presentada anteriormente, siendo la diferencia que el ciclo está por los caracteres en lugar de la palabras como se había realizado. El siguiente código muestra una implementación para realizar gramas de caracteres. \n\n::: {#831e843c .cell execution_count=13}\n``` {.python .cell-code}\ntext = 'Estoy jugando'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\n['Esto',\n 'stoy',\n 'toy ',\n 'oy j',\n 'y ju',\n ' jug',\n 'juga',\n 'ugan',\n 'gand',\n 'ando']\n```\n:::\n:::\n\n\n## TextModel {#sec-TextModel}\n\nHabiendo descrito diferentes tipos de normalización (sintáctica y semántica) y el proceso de segmentación es momento para describir la librería [B4MSA](https://b4msa.readthedocs.io/) (@B4MSA) que implementa estos procedimientos; específicamente, el punto de acceso de estos procedimientos corresponde a la clase `TextModel`. El método `TextModel.text_transformations` es el que realiza todos los métodos de normalización (@sec-normalizacion y @sec-normalizacion-semantica) y el método `TextModel.tokenize` es el encargado de realizar la segmentación (@sec-segmentacion) siguiendo el flujo mostrado en la @fig-pre-procesamiento.\n\n### Normalizaciones\n\nEl primer conjunto de parámetros que se describen son los que corresponden a las entidades (@sec-normalizacion-entidades). Estos parámetros tiene tres opciones, borrar (`OPTION_DELETE`), remplazar (`OPTION_GROUP`) o ignorar. Los nombres de los parámetros son:\n\n* usr_option \n* url_option\n* num_option\n\nque corresponden al procesamiento de usuarios, URL y números respectivamente. Adicionalmente, `TextModel` trata los emojis, hashtags y nombres, mediante los siguientes parámetros:\n\n* emo_option\n* hashtag_option\n* ent_option\n\nPor ejemplo, el siguiente código muestra como se borra el usuario y se reemplaza un hashtag; se puede observar que en la respuesta se cambian todos los espacios por el caracter `~` y se incluye ese mismo al inicio y final del texto. \n\n::: {#a78f1f3a .cell execution_count=14}\n``` {.python .cell-code}\ntm = TextModel(hashtag_option=OPTION_GROUP,\n               usr_option=OPTION_DELETE)\ntexto = 'mira @xyz estoy triste. #UnDiaLluvioso'\ntm.text_transformations(texto)\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\n'~mira~estoy~triste.~_htag~'\n```\n:::\n:::\n\n\nSiguiendo con las transformaciones sintácticas, toca el tiempo a describir aquellas que relacionadas a la ortografía (@sec-normalizacion-ortografia) las cuales corresponden a la conversión a minúsculas, borrado de signos de puntuación y símbolos diacríticos. Estas normalizaciones se activan con los siguiente parámetros. \n\n* lc \n* del_punc\n* del_diac\n\nEn el siguiente ejemplo se transforman el texto a minúscula y se remueven los signos de puntuación. \n\n::: {#f1d7a979 .cell execution_count=15}\n``` {.python .cell-code}\ntm = TextModel(lc=True,\n               del_punc=True,\n               del_diac=False)\ntexto = 'Hoy está despejado.'\ntm.text_transformations(texto)\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n'~hoy~está~despejado~'\n```\n:::\n:::\n\n\nLas normalizaciones semánticas (@sec-normalizacion-semantica) que se tienen implementadas en la librería corresponden al borrado de palabras comunes y reducción a la raíz; estás se pueden activar con los siguientes parámetros. \n\n* stopwords\n* stemming\n\nPor ejemplo, las siguientes instrucciones quitan las palabras comunes y realizan una reducción a la raíz. \n\n::: {#6e168cd8 .cell execution_count=16}\n``` {.python .cell-code}\ntm = TextModel(lang='es',\n               stopwords=OPTION_DELETE,\n               stemming=True)\ntexto = 'el clima es perfecto'\ntm.text_transformations(texto)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\n'~clim~perfect~'\n```\n:::\n:::\n\n\n### Segmentación\n\nEl paso final es describir el uso de la segmentación. La librería utiliza el parámetro `token_list` para indicar el tipo de segmentación que se desea realizar. El formato es una lista de número, donde el valor indica el tipo de segmentación. El número $1$ indica que se realizará una segmentación por palabras, los número positivo corresponden a los gramas de caracteres y los números negativos a los gramas de palabras. \n\nPor ejemplo, utilizando las normalizaciones que se tienen por defecto, el siguiente código segmenta utilizando gramas de caracteres de tamañan $4.$\n\n::: {#3f9fb5a5 .cell execution_count=17}\n``` {.python .cell-code}\ntm = TextModel(token_list=[4])\ntm.tokenize('buenos días')\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n['q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n```\n:::\n:::\n\n\npara poder identificar cuando se trata de un segmento que corresponde a una palabra o un grama de caracteres, a los últimos se les agrega el prefijo `q:`. Cabe mencionar que por defecto se remueven los símbolos diacríticos.\n\nEl ejemplo anterior, se utiliza para generar un grama de palabras de tamaño $2.$ Como se ha mencionado los gramas de palabras se especifican con números negativos siendo el valor absoluto el tamaño del grama. \n\n::: {#57eb34cd .cell execution_count=18}\n``` {.python .cell-code}\ntm = TextModel(token_list=[-2])\ntm.tokenize('buenos días')\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\n['buenos~dias']\n```\n:::\n:::\n\n\nPara completar la explicación, se combinan la segmentación de gramas de caracteres y palabras además de incluir las palabras en la segmentación. \n\n::: {#ddb2bcd6 .cell execution_count=19}\n``` {.python .cell-code}\ntm = TextModel(token_list=[4, -2, -1])\ntm.tokenize('buenos días')\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n['buenos~dias',\n 'buenos',\n 'dias',\n 'q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n```\n:::\n:::\n\n\n",
    "supporting": [
      "02ManejandoTexto_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}