{
  "hash": "8511aaacf0432cc95ef046e8036ae659",
  "result": {
    "markdown": "# Mezcla de Modelos\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {#83af2b4e .cell execution_count=1}\n``` {.python .cell-code}\nfrom EvoMSA import BoW, DenseBoW, StackGeneralization\nfrom microtc.utils import tweet_iterator\nfrom IngeoML import CI, SelectFromModelCV\nfrom sklearn.metrics import f1_score,\\\n                            recall_score,\\\n                            precision_score\nfrom wordcloud import WordCloud                            \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Conjunto de Datos \n\nEl conjunto de datos se puede conseguir en la página de [Delitos](https://ingeotec.github.io/Delitos) aunque en esta dirección es necesario poblar los textos dado que solamente se encuentra el identificador del Tweet.\n\nPara leer los datos del conjunto de entrenamiento y prueba se utilizan las siguientes instrucciones. En la variable `D` se tiene los datos que se utilizarán para entrenar el clasificador basado en la bolsa de palabras y en `Dtest` los datos del conjunto de prueba, que son usados para medir el rendimiento del clasificador.\n\n::: {#92d63cfe .cell execution_count=4}\n``` {.python .cell-code}\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nfname_test = 'delitos/delitos_ingeotec_Es_test.json'\nD = list(tweet_iterator(fname))\nDtest = list(tweet_iterator(fname_test))\n```\n:::\n\n\nEn la siguiente instrucción se observa el primer elemento del conjunto de entrenamiento. Se puede observar que en el campo `text` se encuentra el texto, el campo `klass` representa la etiqueta o clase, donde $0$ representa la clase negativa y $1$ la clase positiva, es decir, la presencia de un delito. El campo `id` es el identificador del Tweet y `annotations` son las clases dadas por los etiquetadores a ese ejemplo.\n\n::: {#b75b62b6 .cell execution_count=5}\n``` {.python .cell-code}\nD[81]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n{'annotations': [0, 0, 0],\n 'id': 1107040319986696195,\n 'klass': 0,\n 'text': 'To loco'}\n```\n:::\n:::\n\n\n## Bolsa de Palabras Dispersa \n\nSe inicia con la creación de un clasificador basado en una bolsa de palabras dispersa, el clasificador es una máquina de soporte vectorial lineal (`LinearSVC`). La siguiente instrucción usa la clase `BoW` para crear este clasificador de texto. El primer paso es seleccionar el lenguaje, en este caso español (es) y después se entrena usando el método `fit`.\n\n::: {#092dbec6 .cell execution_count=6}\n``` {.python .cell-code}\nbow = BoW(lang='es').fit(D)\n```\n:::\n\n\n::: {#d4205f20 .cell execution_count=7}\n``` {.python .cell-code}\ntxt = 'me golpearon y robaron la bicicleta en la noche'\nbow.predict([txt])\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray([1])\n```\n:::\n:::\n\n\n::: {#efa941c4 .cell execution_count=8}\n``` {.python .cell-code}\nhy_bow = bow.predict(Dtest)\n```\n:::\n\n\n::: {#cbedc155 .cell execution_count=9}\n``` {.python .cell-code}\ny = np.r_[[x['klass'] for x in Dtest]]\nf1_score(y, hy_bow, average=None)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```\narray([0.94612795, 0.74603175])\n```\n:::\n:::\n\n\n::: {#96945d42 .cell execution_count=10}\n``` {.python .cell-code}\nci = CI(statistic=lambda y, hy: f1_score(y, hy, \n                                         average=None))\nci_izq, ci_der = ci(y, hy_bow)\n```\n:::\n\n\n\n\nEl intervalo izquierdo es $[0.9272, 0.6481]$ y el derecho tiene los valores $[0.9632, 0.8190]$.\n\n::: {#cell-hist-f1-bow .cell execution_count=12}\n``` {.python .cell-code code-fold=\"true\"}\ndf_bow = pd.DataFrame(ci.statistic_samples, columns=['f1-neg', 'f1-pos'])\ndf_bow['Tipo'] = 'BoW'\nsns.set_style('whitegrid')\nsns.displot(df_bow, kde=True)\n```\n\n::: {.cell-output .cell-output-display}\n![Histograma de f1 por clase](06MezclaModelos_files/figure-html/hist-f1-bow-output-1.png){#hist-f1-bow width=564 height=470}\n:::\n:::\n\n\n::: {#d514e8ba .cell execution_count=13}\n``` {.python .cell-code}\nws = bow.estimator_instance.coef_[0]\nidfs = bow.weights\n```\n:::\n\n\n::: {#804106d5 .cell execution_count=14}\n``` {.python .cell-code}\ntokens_pos = {name: w * idf\n              for name, idf, w in zip(bow.names,\n                                      idfs, ws)\n              if w > 0}\ntokens_neg = {name: w * idf * -1\n              for name, idf, w in zip(bow.names,\n                                      idfs, ws)\n              if w < 0}\n```\n:::\n\n\n::: {#cell-fig-nube-tokens .cell execution_count=15}\n``` {.python .cell-code code-fold=\"true\"}\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_pos, word_neg],\n                     [ax1, ax2],\n                     ['Positivas', 'Negativas']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nubes de tokens positivos y negativos](06MezclaModelos_files/figure-html/fig-nube-tokens-output-1.png){#fig-nube-tokens width=540 height=157}\n:::\n:::\n\n\n## Bolsa de Palabras Densas \n\n::: {#b2a29e33 .cell execution_count=16}\n``` {.python .cell-code}\ndense = DenseBoW(lang='es',\n                 voc_size_exponent=15,\n                 dataset=False)\n```\n:::\n\n\n::: {#dae20ca9 .cell execution_count=17}\n``` {.python .cell-code}\nmacro_f1 = lambda y, hy: f1_score(y, hy, average='macro')\nkwargs = dense.estimator_kwargs\nestimator = dense.estimator_class(**kwargs)\nkwargs = dict(estimator=estimator,\n              scoring=macro_f1)\ndense.select(D=D,\n             feature_selection=SelectFromModelCV,\n             feature_selection_kwargs=kwargs)\ndense.fit(D)\n```\n:::\n\n\n::: {#0f628be2 .cell execution_count=18}\n``` {.python .cell-code}\nselect = dense.feature_selection\nperf = select.cv_results_\n```\n:::\n\n\n::: {#cell-fig-dense-k .cell execution_count=19}\n``` {.python .cell-code code-fold=\"true\"}\n_ = [{'d': k, 'macro-f1': v} for k, v in perf.items()]\ndf = pd.DataFrame(_)\nax = sns.lineplot(df, x='d', y='macro-f1')\nsns.set_style('whitegrid')\n```\n\n::: {.cell-output .cell-output-display}\n![Rendimiento Variando el Número de Características](06MezclaModelos_files/figure-html/fig-dense-k-output-1.png){#fig-dense-k width=585 height=427}\n:::\n:::\n\n\n::: {#f7e7b4a8 .cell execution_count=20}\n``` {.python .cell-code}\nhy_dense = dense.predict(Dtest)\n```\n:::\n\n\n::: {#47c8b9c0 .cell execution_count=21}\n``` {.python .cell-code}\nf1_score(y, hy_dense, average=None)\n```\n\n::: {.cell-output .cell-output-display execution_count=21}\n```\narray([0.94158076, 0.75362319])\n```\n:::\n:::\n\n\n::: {#cell-hist-f1-bow-dense .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nci(y, hy_dense)\ndf_dense = pd.DataFrame(ci.statistic_samples, columns=['f1-neg', 'f1-pos'])\ndf_dense['Tipo'] = 'Dense'\n\n_ = df_bow.melt(id_vars=['Tipo'], value_name='value', var_name='f1')\n_2 = df_dense.melt(id_vars=['Tipo'], value_name='value', var_name='f1')\n_ = pd.concat((_, _2))\nsns.set_style(\"whitegrid\")\nfig = sns.displot(_, x='value', hue='f1', kde=True, col='Tipo')\n# plt.grid()\n```\n\n::: {.cell-output .cell-output-display}\n![Histogramas de f1 por clase](06MezclaModelos_files/figure-html/hist-f1-bow-dense-output-1.png){#hist-f1-bow-dense width=1044 height=471}\n:::\n:::\n\n\n::: {#c6b012eb .cell execution_count=23}\n``` {.python .cell-code}\nw = dense.estimator_instance.coef_[0]\nnames = np.array(dense.names)\ncarac_pos = {k: v for k, v in zip(names, w) if v > 0}\ncarac_neg = {k: v * -1 for k, v in zip(names, w) if v < 0}\n```\n:::\n\n\n::: {#cell-fig-nube-densa .cell execution_count=24}\n``` {.python .cell-code code-fold=\"true\"}\nword_pos = WordCloud().generate_from_frequencies(carac_pos)\nword_neg = WordCloud().generate_from_frequencies(carac_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_pos, word_neg],\n                     [ax1, ax2],\n                     ['Positivas', 'Negativas']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características positivas y negativas](06MezclaModelos_files/figure-html/fig-nube-densa-output-1.png){#fig-nube-densa width=540 height=157}\n:::\n:::\n\n\n## Análisis Mediante Ejemplos \n\n::: {#463f17ac .cell execution_count=25}\n``` {.python .cell-code}\nbow_norm = np.linalg.norm(bow.estimator_instance.coef_[0])\ntxt = 'Asesinan a persona en Jalisco.'\nbow.decision_function([txt]) / bow_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```\narray([[0.03104446]])\n```\n:::\n:::\n\n\n::: {#c38d086e .cell execution_count=26}\n``` {.python .cell-code}\ndense_norm = np.linalg.norm(dense.estimator_instance.coef_[0])\ndense.decision_function([txt]) / dense_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```\narray([[0.00906055]])\n```\n:::\n:::\n\n\n::: {#cd098dc0 .cell execution_count=27}\n``` {.python .cell-code}\ntxt = 'La asesina vivía en Jalisco.'\nbow.decision_function([txt]) / bow_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```\narray([[-0.03643006]])\n```\n:::\n:::\n\n\n::: {#303d990b .cell execution_count=28}\n``` {.python .cell-code}\ndense.decision_function([txt]) / dense_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=28}\n```\narray([[-0.03598119]])\n```\n:::\n:::\n\n\n::: {#c2387607 .cell execution_count=29}\n``` {.python .cell-code}\ntxt = 'Asesinan a persona en Jalisco.'\nbow.decision_function([txt]) / bow_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=29}\n```\narray([[0.03104446]])\n```\n:::\n:::\n\n\n::: {#e83060cc .cell execution_count=30}\n``` {.python .cell-code}\nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\nsorted([(bow.names[k], w[k] * v) for k, v in vec],\n       key=lambda x: np.fabs(x[1]), reverse=True)[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=30}\n```\n[('asesinan', 0.21959402998283195),\n ('asesinan~a', 0.20828271208543045),\n ('q:sina', 0.1451177919904085),\n ('q:n~a~', 0.08388029043033413),\n ('q:an~a', 0.07344376981807058)]\n```\n:::\n:::\n\n\n::: {#6365697f .cell execution_count=31}\n``` {.python .cell-code}\ndense.decision_function([txt]) / dense_norm\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\narray([[0.00906055]])\n```\n:::\n:::\n\n\n::: {#4a29dd26 .cell execution_count=32}\n``` {.python .cell-code}\nw = dense.estimator_instance.coef_[0]\nvec = dense.transform([txt])[0] * w\nsorted([(dense.names[k], v) for k, v in enumerate(vec)],\n       key=lambda x: np.fabs(x[1]), reverse=True)[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=32}\n```\n[('ocurrir', 0.06683457750173856),\n ('muere', 0.053880044741064774),\n ('consiguio', -0.05168798790090579),\n ('critican', -0.04459207389659752),\n ('hubieses', -0.04426955144485894)]\n```\n:::\n:::\n\n\n::: {#cell-fig-nube-ej-pos .cell execution_count=33}\n``` {.python .cell-code code-fold=\"true\"}\ndef codifica(names, vec):\n    carac_pos = dict()\n    for k, v in zip(names, vec):\n        if v > 0:\n            key = f'{k.upper()}'\n        else:\n            key = k\n        carac_pos[key] = np.fabs(v)\n    return carac_pos\ntxt = 'Asesinan a persona en Jalisco.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] > 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características ejemplo positivo](06MezclaModelos_files/figure-html/fig-nube-ej-pos-output-1.png){#fig-nube-ej-pos width=540 height=157}\n:::\n:::\n\n\n::: {#cell-fig-nube-ej .cell execution_count=34}\n``` {.python .cell-code code-fold=\"true\"}\ntxt = 'La asesina vivía en Jalisco.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] > 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características ejemplo negativo](06MezclaModelos_files/figure-html/fig-nube-ej-output-1.png){#fig-nube-ej width=540 height=157}\n:::\n:::\n\n\n::: {#cell-fig-nube-ej-error .cell execution_count=35}\n``` {.python .cell-code code-fold=\"true\"}\ntxt = 'Le acaban de robar la bicicleta a mi hijo.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] > 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características en un ejemplo positivo con predicción negativa](06MezclaModelos_files/figure-html/fig-nube-ej-error-output-1.png){#fig-nube-ej-error width=540 height=157}\n:::\n:::\n\n\n## Combinando Modelos\n\n::: {#948cfb24 .cell execution_count=36}\n``` {.python .cell-code}\nstack = StackGeneralization([bow, dense]).fit(D)\n```\n:::\n\n\n::: {#0a1ba35d .cell execution_count=37}\n``` {.python .cell-code}\nhy_stack = stack.predict(Dtest)\n```\n:::\n\n\n::: {#9daa50c2 .cell execution_count=38}\n``` {.python .cell-code}\nf1_score(y, hy_stack, average=None)\n```\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\narray([0.94791667, 0.79166667])\n```\n:::\n:::\n\n\n::: {#tbl-performance-recall-precision-delitos .cell tbl-cap='Rendimiento' execution_count=39}\n\n::: {.cell-output .cell-output-display execution_count=39}\n|              |   Recall neg | Recall pos | Precision neg | Precision pos |\n|--------------|--------------|------------|---------------|---------------|\n|`bow`|$0.9894$ | $0.6184$ | $0.9065$ | $0.9400$|\n|`dense`|$0.9648$ | $0.6842$ | $0.9195$ | $0.8387$|\n|`stack`|$0.9613$ | $0.7500$ | $0.9349$ | $0.8382$|\n:::\n:::\n\n\n::: {#tbl-performance-f1-delitos .cell tbl-cap='Rendimiento' execution_count=40}\n\n::: {.cell-output .cell-output-display execution_count=40}\n|              | f1 neg | f1 pos | macro-f1 |\n|--------------|--------|--------|----------|\n|`bow`|$0.9461$ | $0.7460$ | $0.8461$|\n|`dense`|$0.9416$ | $0.7536$ | $0.8476$|\n|`stack`|$0.9479$ | $0.7917$ | $0.8698$|\n:::\n:::\n\n\n",
    "supporting": [
      "06MezclaModelos_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}