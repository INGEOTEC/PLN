{
  "hash": "4524500d2ace97a163eb5332ac042394",
  "result": {
    "markdown": "# Fundamentos de Clasificación de Texto\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom microtc.utils import tweet_iterator, load_model, save_model\nfrom b4msa.textmodel import TextModel\nfrom EvoMSA.tests.test_base import TWEETS\nfrom EvoMSA.utils import bootstrap_confidence_interval\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.stats import norm, multinomial, multivariate_normal\nfrom scipy.special import logsumexp\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nfrom os.path import join\nimport numpy as np\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Introducción \n\nText Categorization is an NLP task that deals with creating algorithms capable of identifying the category of a text from a set of predefined categories. For example, sentiment analysis belongs to this task, and the aim is to detect the polarity (e.g., positive, neutral, or negative) of a text. Furthermore, different NLP tasks that initially seem unrelated to this problem can be formulated as a classification one such as question answering and sentence entailment, to mention a few. \n\nText Categorization can be tackled from different perspectives; the one followed here is to treat it as a supervised learning problem. As in any supervised learning problem, the starting point is a set of pairs, where the first element of the pair is the input and the second one corresponds to the output. Let $\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}$ where $y \\in \\{c_1, \\ldots c_K\\}$ and $\\text{text}_i$ is a text. \n\n## Modelado Probabilistico (Distribución Categórica) {#sec-categorical-distribution}\n\n### Problema Sintético\n\nThe description of Bayes’ theorem continues with an example of a Categorical distribution. A Categorical distribution can simulate the drawn of $K$ events that can be encoded as characters, and $\\ell$ repetitions can be represented as a sequence of characters. Consequently, the distribution can illustrate the generation sequences associated with different classes, e.g., positive or negative.\n\nThe first step is to create the dataset. As done previously, two distributions are defined, one for each class; it can be observed that each distribution has different parameters. The second step is to sample these distributions; the distributions are sampled 1000 times with the following procedure. Each time, a random variable representing the number of outcomes taken from each distribution is drawn from a Normal $\\mathcal N(15, 3)$ and stored in the variable `length.` The random variable indicates the number of outcomes for each Categorical distribution; the results are transformed into a sequence, associated to the label corresponding to the positive and negative class, and stored in the list `D.`\n\n```python\npos = multinomial(1, [0.20, 0.20, 0.35, 0.25])\nneg = multinomial(1, [0.35, 0.20, 0.25, 0.20])\nlength = norm(loc=15, scale=3)\nD = []\nm = {k: chr(122 - k) for k in range(4)}\nid2w = lambda x: \" \".join([m[_] for _ in x.argmax(axis=1)])\nfor l in length.rvs(size=1000):\n    D.append((id2w(pos.rvs(round(l))), 1))\n    D.append((id2w(neg.rvs(round(l))), 0))\n```\n\nThe following table shows four examples of this process; the first column contains the sequence, and the second the associated label.\n\n|Text          |Label    |\n|--------------|---------|\n|x w x x z w y | positive       |\n|y w z z z x w | negative       |\n|z x x x z x z w x w | positive |\n|x w z w y z z z z w | negative |\n\nAs done previously, the first step is to compute the likelihood given that dataset; considering that the data comes from a Categorical distribution, the procedure to estimate the parameters is similar to the ones used to estimate the prior. The following code estimates the data parameters corresponding to the positive class. It can be observed that the parameters estimated are similar to the ones used to generate the dataset. \n\n```python\nD_pos = []\n[D_pos.extend(data.split()) for data, k in D if k == 1]\nwords, l_pos = np.unique(D_pos, return_counts=True)\nw2id = {v: k for k, v in enumerate(words)}\nl_pos = l_pos / l_pos.sum()\nl_pos\narray([0.25489421, 0.33854064, 0.20773186, 0.1988333 ])\n```\n\nAn equivalent procedure is performed to calculate the likelihood of the negative class.\n\n```python\nD_neg = []\n[D_neg.extend(data.split()) for data, k in D if k == 0]\n_, l_neg = np.unique(D_neg, return_counts=True)\nl_neg = l_neg / l_neg.sum()\n```\n\nThe prior is estimated with the following code, equivalent to the one used on all the examples seen so far. \n\n```python\n_, priors = np.unique([k for _, k in D], return_counts=True)\nN = priors.sum()\nprior_pos = priors[1] / N\nprior_neg = priors[0] / N\n```\n\nOnce the parameters have been identified, these can be used to predict the class of a given sequence. The first step is to compute the likelihood, e.g., $\\mathbb P($w w x z$\\mid \\mathcal Y)$. It can be observed that the sequence needs to be transformed into tokens which can be done with the `split` method. Then, the token is converted into an index using the mapping `w2id`; once the index is retrieved, it can be used to obtain the parameter associated with the word. The likelihood is the product of all the probabilities; however, this product is computed in log space. \n\n```python\ndef likelihood(params, txt):\n    params = np.log(params)\n    _ = [params[w2id[x]] for x in txt.split()]\n    tot = sum(_)\n    return np.exp(tot)\n```\n\nThe likelihood combined with the prior for all the classes produces the evidence, which subsequently is used to calculate the posterior distribution. The posterior is then used to predict the class for all the sequences in $\\mathcal D$. The predictions are stored in the variable `hy`.\n\n```python\npost_pos = [likelihood(l_pos, x) * prior_pos for x, _ in D]\npost_neg = [likelihood(l_neg, x) * prior_neg for x, _ in D]\nevidence = np.vstack([post_pos, post_neg]).sum(axis=0)\npost_pos /= evidence\npost_neg /= evidence\nhy = np.where(post_pos > post_neg, 1, 0)\n```\n\n### Clasificador de Texto {#sec-tc-categorical }\n\nThe approach followed on text categorization is to treat it as supervised learning problem where the starting point is a dataset $\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}$ where $y \\in \\{c_1, \\ldots c_K\\}$ and $\\text{text}_i$ is a text. For example, the next code uses a toy sentiment analysis dataset with four classes: negative (N), neutral (NEU), absence of polarity (NONE), and positive (P).\n\n```python\nD = [(x['text'], x['klass']) for x in tweet_iterator(TWEETS)]\n```\n\nAs can be observed, $\\mathcal D$ is equivalent to the one used in the [Categorical Distribution](#sec:categorical-distribution) example. The difference is that sequence of letters is changed with a sentence. Nonetheless, a feasible approach is to obtain the tokens using the `split` method. Another approach is to retrieve the tokens using a Tokenizer, as covered in the [Text Normalization](/NLP-Course/topics/05TextNormalization) Section. \n\nThe following code uses the `TextModel` class to tokenize the text using words as the tokenizer; the tokenized text is stored in the variable `D.`\n\n```python\ntm = TextModel(token_list=[-1])\ntok = tm.tokenize\nD = [(tok(x), y) for x, y in D]\n```\n\nBefore estimating the likelihood parameters, it is needed to encode the tokens using an index; by doing it, it is possible to store the parameters in an array and compute everything `numpy` operations. The following code encodes each token with a unique index; the mapping is in the dictionary `w2id`. \n\n```python\nwords = set()\n[words.update(x) for x, y in D]\nw2id = {v: k for k, v in enumerate(words)}\n```\n\nPreviously, the classes have been represented using natural numbers. The positive class has been associated with the number $1$, whereas the negative class with $0$. However, in this dataset, the classes are strings. It was decided to encode them as numbers to facilitate subsequent operations. The encoding process can be performed simultaneously with the estimation of the prior of each class. Please note that the priors are stored using the logarithm in the variable `priors.` \n\n```python\nuniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\npriors = np.log(priors / priors.sum())\nuniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n```\n\nIt is time to estimate the likelihood parameters for each of the classes. It is assumed that the data comes from a Categorical distribution and that each token is independent. The likelihood parameters can be stored in a matrix (variable `l_tokens`) with $K$ rows, each row contains the parameters of the class, and the number of columns corresponds to the vocabulary's size. The first step is to calculate the frequency of each token per class which can be done with the following code. \n\n```python\nl_tokens = np.zeros((len(uniq_labels), len(w2id)))\nfor x, y in D:\n    w = l_tokens[uniq_labels[y]]\n    cnt = Counter(x)\n    for i, v in cnt.items():\n        w[w2id[i]] += v\n```\n\nThe next step is to normalize the frequency. However, before normalizing it, it is being used a Laplace smoothing with a value $0.1$. Therefore, the constant $0.1$ is added to all the matrix elements. The next step is to normalize (second line), and finally, the parameters are stored using the logarithm. \n\n```python\nl_tokens += 0.1\nl_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\nl_tokens = np.log(l_tokens)\n```\n\n#### Prediction\n\nOnce all the parameters have been estimated, it is time to use the model to classify any text. The following function computes the posterior distribution. The first step is to tokenize the text (second line) and compute the frequency of each token in the text. The frequency stored in the dictionary `cnt` is converted into the vector `x` using the mapping function `w2id`. The final step is to compute the product of the likelihood and the prior. The product is computed in log-space; thus, this is done using the likelihood and the prior sum. The last step is to compute the evidence and normalize the result; the evidence is computed with the function `logsumexp.` \n\n```python\ndef posterior(txt):\n    x = np.zeros(len(w2id))\n    cnt = Counter(tm.tokenize(txt))\n    for i, v in cnt.items():\n        try:\n            x[w2id[i]] += v\n        except KeyError:\n            continue\n    _ = (x * l_tokens).sum(axis=1) + priors\n    l = np.exp(_ - logsumexp(_))\n    return l\n```\n\nThe posterior function can predict all the text in $\\mathcal D$; the predictions are used to compute the model's accuracy. In order to compute the accuracy, the classes in $\\mathcal D$ need to be transformed using the nomenclature of the likelihood matrix and priors vector; this is done with the `uniq_labels` dictionary (second line). \n\n```python\nhy = np.array([posterior(x).argmax() for x, _ in D])\ny = np.array([uniq_labels[y] for _, y in D])\n(y == hy).mean()\n0.974\n```\n\n#### Training\n\nSolving supervised learning problems requires two phases; one is the training phase, and the other is the prediction. The posterior function handles the later phase, and it is missing to organize the code described in a training function. The following code describes the training function; it requires the dataset's parameters and an instance of `TextModel.`\n\n```python\ndef training(D, tm):\n    tok = tm.tokenize\n    D =[(tok(x), y) for x, y in D]\n    words = set()\n    [words.update(x) for x, y in D]\n    w2id = {v: k for k, v in enumerate(words)}\n    uniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\n    priors = np.log(priors / priors.sum())\n    uniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n    l_tokens = np.zeros((len(uniq_labels), len(w2id)))\n    for x, y in D:\n        w = l_tokens[uniq_labels[y]]\n        cnt = Counter(x)\n        for i, v in cnt.items():\n            w[w2id[i]] += v\n    l_tokens += 0.1\n    l_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\n    l_tokens = np.log(l_tokens)\n    return w2id, uniq_labels, l_tokens, priors\n```\n\n## Modelado Vectorial {#sec-tc-vectorial} \n\nxxx\n\n",
    "supporting": [
      "04ClasificacionTexto_files"
    ],
    "filters": []
  }
}