{
  "hash": "750c16382d6ec2f68c482944e79f71c5",
  "result": {
    "markdown": "# Fundamentos de Clasificación de Texto\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {#ac1c8a68 .cell execution_count=1}\n``` {.python .cell-code}\nfrom microtc.utils import tweet_iterator, load_model, save_model\nfrom b4msa.textmodel import TextModel\nfrom EvoMSA.tests.test_base import TWEETS\nfrom EvoMSA.utils import bootstrap_confidence_interval\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.stats import norm, multinomial, multivariate_normal\nfrom scipy.special import logsumexp\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nfrom os.path import join\nimport numpy as np\n```\n:::\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Introducción \n\nEl problema de categorización (clasificación) de texto es una tarea de PLN que desarrolla algoritmos capaces de identificar la categoría de un texto de un conjunto de categorías previamente definidas. Por ejemplo, en análisis de sentimientos pertenece a esta tarea y su objetivo es el detectar la polaridad (e.g., positiva, neutral, o negativa) del texto. Cabe mencionar, que diferentes tareas de PLN pueden ser formuladas como problemas de clasificación, e.g., la tarea de preguntas y respuestas, vinculación de enunciados, entre otras.  \n\nEl problema de clasificación de texto se puede resolver desde diferentes perspectivas; el camino que se seguirá corresponde a aprendizaje supervisado. Los problemas de aprendizaje supervisado comienzan con un conjunto de pares, donde el primer elementos del par corresponde a las entradas (variables independientes) y el segundo es la respuesta (variable dependiente). Sea $\\mathcal D = \\{(\\text{texto}_i, y_i) \\mid i=1,\\ldots, N\\}$ donde $y \\in \\{c_1, \\ldots c_K\\}$ y $\\text{texto}_i$ contiene el texto. \n\n## Teorema de Bayes\n\nUna manera de modelar este problema es modelando la probabilidad de observar la clase $\\mathcal Y$ dada la entrada, es decir, $\\mathbb P(\\mathcal Y \\mid \\mathcal X)$. El Teorema de Bayes ayuda a expresa esta expresión en términos de elementos que se pueden medir de un conjunto de entrenamiento. \n\nLa probabilidad conjunta se puede expresar como $\\mathbb P(\\mathcal X, \\mathcal Y)$, esta probabilidad es conmutativa por lo que $\\mathbb P(\\mathcal X, \\mathcal Y)=\\mathbb P(\\mathcal Y, \\mathcal X).$ En este momento se puede utilizar la definición de **probabilidad condicional** que es $\\mathbb P(\\mathcal Y, \\mathcal X)=\\mathbb P(\\mathcal Y \\mid \\mathcal X) \\mathbb P(\\mathcal X).$ Utilizando estas ecuaciones el **Teorema de Bayes** queda como\n\n$$\n\\mathbb P(\\mathcal Y \\mid \\mathcal X) = \\frac{ \\mathbb P(\\mathcal X \\mid \\mathcal Y) \\mathbb P(\\mathcal Y)}{\\mathbb P(\\mathcal X)},\n$$ {#eq-teorema-bayes}\n\ndonde al término $\\mathbb P(\\mathcal X \\mid \\mathcal Y)$ se le conoce como **verosimilitud**, $\\mathbb P(\\mathcal Y)$ es la probabilidad **a priori** y $\\mathbb P(\\mathcal X)$ es la **evidencia**. \n\nEs importante mencionar que la evidencia se puede calcular mediante la probabilidad\ntotal, es decir:\n\n$$\n\\mathbb P(\\mathcal X) = \\sum_{y \\in \\mathcal Y} \\mathbb P(\\mathcal X \\mid \\mathcal Y=y) \\mathbb P(\\mathcal Y=y).\n$$ {#eq-evidencia}\n\n## Modelado Probabilistico (Distribución Categórica) {#sec-categorical-distribution}\n\nSe inicia la descripción de clasificación de texto presentando un ejemplo sintético que ejemplifica los supuestos que se realizan en el modelo. La distribución categórica modela el evento de seleccionar $K$ eventos, los cuales pueden estar codificados como caracteres. Si esta selección se realiza $\\ell$ veces se cuenta con una secuencia de eventos representados por caracteres. Por ejemplo, los $K$ eventos pueden ser representados por los caracteres *w*, *x*, *y* y *z*. Utilizando este proceso se puede utilizar para ejemplificar el proceso de asociar una secuencia a una clase, e.g., positiva o negativa.\n\nEl primer paso es seleccionar los parámetros de dos distribuciones tal y como se muestra en las siguientes primeras dos líneas. Cada distribución se asume que es la generadora de una clase. El segundo paso es tomar una muestra de cada distribución, en particular se toman $1000$ muestras con el siguiente procedimiento. En cada iteración se toma una muestra de una distribución Gausiana ($\\mathcal N(15, 3)$), la variable aleatoria se guarda en la variable `length`. Esta variable aleatoria representa la longitud de la secuencia. El tercer paso es sacar la muestra de las distribuciones categóricas definidas previamente. Las muestras son guardadas en la lista `D` junto con la clase a la que pertenece $0$ y $1.$ \n\n::: {#7be77143 .cell execution_count=3}\n``` {.python .cell-code}\npos = multinomial(1, [0.20, 0.20, 0.35, 0.25])\nneg = multinomial(1, [0.35, 0.20, 0.25, 0.20])\nlength = norm(loc=15, scale=3)\nD = []\nm = {k: chr(122 - k) for k in range(4)}\nid2w = lambda x: \" \".join([m[_] for _ in x.argmax(axis=1)])\nfor l in length.rvs(size=1000):\n    D.append((id2w(pos.rvs(round(l))), 1))\n    D.append((id2w(neg.rvs(round(l))), 0))\n```\n:::\n\n\nLa @tbl-clasificacion-texto-generado muestra los primeros cuatro ejemplos generados con el procedimiento anterior. La primera columna muestra la secuencia y asociada a cada secuencia se muestra la clase que corresponde a la secuencia. \n\n::: {#tbl-clasificacion-texto-generado .cell tbl-cap='Conjunto generado de clasificación de texto' execution_count=4}\n\n::: {.cell-output .cell-output-display execution_count=4}\n|Texto         |Clase    |\n|--------------|---------|\n|z w z x x y w z y y|Positivo|\n|z y z z x z x y w z|Negativo|\n|x w z x x z x z w x y x x w|Positivo|\n|w y z x y z w x y y z w x z|Negativo|\n\n:::\n:::\n\n\nEl primer paso es encontrar la verosimilitud dado el conjunto de datos `D`. El siguiente código calcula la verosimilitud de la clase positiva. \n\n::: {#468bdfc2 .cell execution_count=5}\n``` {.python .cell-code}\nD_pos = []\n[D_pos.extend(data.split()) for data, k in D if k == 1]\nwords, l_pos = np.unique(D_pos, return_counts=True)\nw2id = {v: k for k, v in enumerate(words)}\nl_pos = l_pos / l_pos.sum()\nl_pos\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\narray([0.2484998 , 0.35244699, 0.19715962, 0.20189359])\n```\n:::\n:::\n\n\nUn procedimiento equivalente se puede realizar para obtener la verosimilitud de la clase negativa. \n\n::: {#756fdbb2 .cell execution_count=6}\n``` {.python .cell-code}\nD_neg = []\n[D_neg.extend(data.split()) for data, k in D if k == 0]\n_, l_neg = np.unique(D_neg, return_counts=True)\nl_neg = l_neg / l_neg.sum()\nl_neg\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\narray([0.19155887, 0.2490332 , 0.19942659, 0.35998133])\n```\n:::\n:::\n\n\nLa probabilidad a priori se puede calcular con la siguientes instrucciones. \n\n::: {#e50fe41b .cell execution_count=7}\n``` {.python .cell-code}\n_, priors = np.unique([k for _, k in D], return_counts=True)\nN = priors.sum()\nprior_pos = priors[1] / N\nprior_neg = priors[0] / N\n```\n:::\n\n\nUna ves que se han identificador los parámetros, estos pueden ser utilizados para predecir la clase dada una secuencia. El primer paso es calcular la verosimilitud, e.g., $\\mathbb P($w w x z$\\mid \\mathcal Y)$. Se observa que la secuencia tiene se tiene que transformar en términos, esto se puede realizar con el método `split`. Después, los términos se convierten al identificador que corresponde al parámetro del token con el mapa `w2id`. Una vez que se identifica el índice se conoce el valor del parámetro, se calcula el producto (como o la suma si se hace todo en términos del logaritmo) y se regresa el valor de la verosimilitud. \n\n```python\ndef likelihood(params, txt):\n    params = np.log(params)\n    _ = [params[w2id[x]] for x in txt.split()]\n    tot = sum(_)\n    return np.exp(tot)\n```\n\nLa verosimilitud se combina con la probabilidad a priori, con esta información se calcula la evidencia y para obtener la probabilidad a posteriori tanto para la clase positiva (`post_pos`) como para la negativa (`post_neg`). La clase corresponde a la etiqueta que presenta la máxima probabilidad, última línea (`hy`).\n\n```python\npost_pos = [likelihood(l_pos, x) * prior_pos for x, _ in D]\npost_neg = [likelihood(l_neg, x) * prior_neg for x, _ in D]\nevidence = np.vstack([post_pos, post_neg]).sum(axis=0)\npost_pos /= evidence\npost_neg /= evidence\nhy = np.where(post_pos >= post_neg, 1, 0)\n```\n\n### Clasificador de Texto {#sec-tc-categorical }\n\n\nEn la sección anterior se trabajo desde la creación de un conjunto de datos sintético que fue generado mediante dos distribuciones Categóricas, donde a cada distribución se le asignó una clase, e.g., positiva o negativa. Esto permitió observar todas las partes de modelado, en la realidad se desconoce el procedimiento que genera los textos y el proceso de aprendizaje empieza con un conjunto de datos, en este ejemplo se utilizará un conjunto de datos \n\n\nThe approach followed on text categorization is to treat it as supervised learning problem where the starting point is a dataset $\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}$ where $y \\in \\{c_1, \\ldots c_K\\}$ and $\\text{text}_i$ is a text. For example, the next code uses a toy sentiment analysis dataset with four classes: negative (N), neutral (NEU), absence of polarity (NONE), and positive (P).\n\n```python\nD = [(x['text'], x['klass']) for x in tweet_iterator(TWEETS)]\n```\n\nAs can be observed, $\\mathcal D$ is equivalent to the one used in the [Categorical Distribution](#sec:categorical-distribution) example. The difference is that sequence of letters is changed with a sentence. Nonetheless, a feasible approach is to obtain the tokens using the `split` method. Another approach is to retrieve the tokens using a Tokenizer, as covered in the [Text Normalization](/NLP-Course/topics/05TextNormalization) Section. \n\nThe following code uses the `TextModel` class to tokenize the text using words as the tokenizer; the tokenized text is stored in the variable `D.`\n\n```python\ntm = TextModel(token_list=[-1])\ntok = tm.tokenize\nD = [(tok(x), y) for x, y in D]\n```\n\nBefore estimating the likelihood parameters, it is needed to encode the tokens using an index; by doing it, it is possible to store the parameters in an array and compute everything `numpy` operations. The following code encodes each token with a unique index; the mapping is in the dictionary `w2id`. \n\n```python\nwords = set()\n[words.update(x) for x, y in D]\nw2id = {v: k for k, v in enumerate(words)}\n```\n\nPreviously, the classes have been represented using natural numbers. The positive class has been associated with the number $1$, whereas the negative class with $0$. However, in this dataset, the classes are strings. It was decided to encode them as numbers to facilitate subsequent operations. The encoding process can be performed simultaneously with the estimation of the prior of each class. Please note that the priors are stored using the logarithm in the variable `priors.` \n\n```python\nuniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\npriors = np.log(priors / priors.sum())\nuniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n```\n\nIt is time to estimate the likelihood parameters for each of the classes. It is assumed that the data comes from a Categorical distribution and that each token is independent. The likelihood parameters can be stored in a matrix (variable `l_tokens`) with $K$ rows, each row contains the parameters of the class, and the number of columns corresponds to the vocabulary's size. The first step is to calculate the frequency of each token per class which can be done with the following code. \n\n```python\nl_tokens = np.zeros((len(uniq_labels), len(w2id)))\nfor x, y in D:\n    w = l_tokens[uniq_labels[y]]\n    cnt = Counter(x)\n    for i, v in cnt.items():\n        w[w2id[i]] += v\n```\n\nThe next step is to normalize the frequency. However, before normalizing it, it is being used a Laplace smoothing with a value $0.1$. Therefore, the constant $0.1$ is added to all the matrix elements. The next step is to normalize (second line), and finally, the parameters are stored using the logarithm. \n\n```python\nl_tokens += 0.1\nl_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\nl_tokens = np.log(l_tokens)\n```\n\n#### Prediction\n\nOnce all the parameters have been estimated, it is time to use the model to classify any text. The following function computes the posterior distribution. The first step is to tokenize the text (second line) and compute the frequency of each token in the text. The frequency stored in the dictionary `cnt` is converted into the vector `x` using the mapping function `w2id`. The final step is to compute the product of the likelihood and the prior. The product is computed in log-space; thus, this is done using the likelihood and the prior sum. The last step is to compute the evidence and normalize the result; the evidence is computed with the function `logsumexp.` \n\n```python\ndef posterior(txt):\n    x = np.zeros(len(w2id))\n    cnt = Counter(tm.tokenize(txt))\n    for i, v in cnt.items():\n        try:\n            x[w2id[i]] += v\n        except KeyError:\n            continue\n    _ = (x * l_tokens).sum(axis=1) + priors\n    l = np.exp(_ - logsumexp(_))\n    return l\n```\n\nThe posterior function can predict all the text in $\\mathcal D$; the predictions are used to compute the model's accuracy. In order to compute the accuracy, the classes in $\\mathcal D$ need to be transformed using the nomenclature of the likelihood matrix and priors vector; this is done with the `uniq_labels` dictionary (second line). \n\n```python\nhy = np.array([posterior(x).argmax() for x, _ in D])\ny = np.array([uniq_labels[y] for _, y in D])\n(y == hy).mean()\n0.974\n```\n\n#### Training\n\nSolving supervised learning problems requires two phases; one is the training phase, and the other is the prediction. The posterior function handles the later phase, and it is missing to organize the code described in a training function. The following code describes the training function; it requires the dataset's parameters and an instance of `TextModel.`\n\n```python\ndef training(D, tm):\n    tok = tm.tokenize\n    D =[(tok(x), y) for x, y in D]\n    words = set()\n    [words.update(x) for x, y in D]\n    w2id = {v: k for k, v in enumerate(words)}\n    uniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\n    priors = np.log(priors / priors.sum())\n    uniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n    l_tokens = np.zeros((len(uniq_labels), len(w2id)))\n    for x, y in D:\n        w = l_tokens[uniq_labels[y]]\n        cnt = Counter(x)\n        for i, v in cnt.items():\n            w[w2id[i]] += v\n    l_tokens += 0.1\n    l_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\n    l_tokens = np.log(l_tokens)\n    return w2id, uniq_labels, l_tokens, priors\n```\n\n## Modelado Vectorial {#sec-tc-vectorial} \n\nxxx\n\n",
    "supporting": [
      "04ClasificacionTexto_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}