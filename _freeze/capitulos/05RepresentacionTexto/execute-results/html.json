{
  "hash": "8addc5356cb9d6e6b8ebc36b1f7a11b4",
  "result": {
    "markdown": "# Representación de Texto\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {#74a546fb .cell execution_count=1}\n``` {.python .cell-code}\nfrom EvoMSA import BoW,\\\n                   DenseBoW\nfrom microtc.utils import tweet_iterator\nfrom wordcloud import WordCloud                            \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Introducción \n\n## Bolsa de Palabras Dispersa \n\nLa idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (@sec-manejando-texto), cada token $t$ sea asociado a un vector único $\\mathbf{v_t} \\in \\mathbb R^d$ donde la $i$-ésima componente, i.e., $\\mathbf{v_t}_i$, es diferente de cero y $\\forall_{j \\neq i} \\mathbf{v_t}_j=0$. Es decir la $i$-ésima componente está asociada al token $t$, se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token $t$ está en la posición $i$. Por otro lado el valor que contiene la componente se usa para representar alguna característica del token. \n\nEl conjunto de vectores $\\mathbf v$ corresponde al vocabulario, teniendo $d$ diferentes token en el mismo y por definición $\\forall_{i \\neq j} \\mathbf{v_i} \\cdot \\mathbf{v_j} = 0$, donde $\\mathbf{v_i} \\in \\mathbb R^d$, $\\mathbf{v_j} \\in \\mathbb R^d$, y $(\\cdot)$ es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado. \n\nUsando esta notación, un texto $x$ está representado por una secuencia de términos, i.e., $(t_1, t_2, \\ldots)$; la secuencia puede tener repeticiones es decir, $t_j = t_k$. Utilizando la característica de que cada token está asociado a un vector $\\mathbf v$, se transforma la secuencia de términos a una secuencia de vectores (manteniendo las repeticiones), i.e., $(\\mathbf{v_{t_1}}, \\mathbf{v_{t_2}}, \\ldots)$. Finalmente, el texto $x$ se representa como:\n\n$$\n\\mathbf x = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n$$ {#eq-bolsa-palabras}\n\ndonde la suma se hace para todos los elementos de la secuencia, $\\mathbf x \\in \\mathbb R^d$, y $\\lVert \\mathbf w \\rVert$ es la norma Euclideana del vector $\\mathbf w.$\n\n\n\n```{mermaid}\n%%| echo: false\n%%| fig-cap: Diagrama Bolsa de Palabras Dispersa\n%%| label: fig-repr-texto-bolsa-dispersa\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados -->  A[Asociación]\n    Terminos --> Entrenamiento[Estimación\\n de Pesos]\n    Corpus([Corpus]) -.-> Entrenamiento\n    Entrenamiento --> A\n    A --> Repr([Representación])\n```\n\n\n\nAntes de iniciar la descripción detallada del proceso de representación utilizando una bolsa de palabras dispersas, es conveniente ilustrar este proceso mediante la @fig-repr-texto-bolsa-dispersa. El **texto segmentado** es el resultado del proceso ilustrado en @fig-pre-procesamiento. El texto segmentado puede seguir dos caminos, en la parte superior se encuentra el caso cuando los pesos han sido identificados previamente y en la parte inferior es el procedimiento cuando los pesos se estiman mediante un corpus específico que normalmente es un conjunto de entrenamiento. \n\n### Pesado de Términos \n\nComo se había mencionado el valor que tiene la componente $i$-ésima del vector $\\mathbf{v_t}_i$ corresponde a una característica del término asociado, este procedimiento se le conoce como el **esquema de pesado**. Por ejemplo, si el valor es $1$ (i.e., $\\mathbf{v_{t_i}} = 1$) entonces el valor está indicando solo la presencia del término, este es el caso más simple. Considerando la @eq-bolsa-palabras se observa que el resultado, $\\mathbf x$, cuenta las repeticiones de cada término, por esta característica a este esquema se le conoce como **frecuencia de términos** (*term frequency (TF)*). \n\nUna medida que complementa la información que tiene la frecuencia de términos es el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)*) en la colección, esta medida propuesta por @Jones1972 se usa en un método de pesado descrito por @Salton1973 el cual es conocido como **TFIDF**. Este método de pesado propone el considerar el producto de la frecuencia del término y el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)* ) en la colección como el peso del término.\n\n\n### Ejemplos \n\n\n\n::: {#d3d9bc95 .cell execution_count=5}\n``` {.python .cell-code}\ntm = BoW(lang='es').bow\nvec = tm['Buen día']\nvec[:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n[(11219, 0.3984336285263178),\n (11018, 0.3245843730253675),\n (24409, 0.2377856890280623)]\n```\n:::\n:::\n\n\n::: {#c99581cf .cell execution_count=6}\n``` {.python .cell-code}\n[(tm.id2token[k], v)\n for k, v in vec[:3]]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[('buen~dia', 0.3984336285263178),\n ('buen', 0.3245843730253675),\n ('dia', 0.2377856890280623)]\n```\n:::\n:::\n\n\n::: {#d41509ea .cell execution_count=7}\n``` {.python .cell-code}\ntxt = 'Buen día colegas'\n[(tm.id2token[k], v)\n for k, v in tm[txt][:4]]\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n[('buen~dia', 0.24862785236357487),\n ('buen', 0.20254494048246244),\n ('dia', 0.1483814139998851),\n ('colegas', 0.3538047214393573)]\n```\n:::\n:::\n\n\n\n\nSe puede observar como los valores de IDF de los términos comunes cambiaron, por ejemplo para el caso de *buen~dia* cambio de $0.3984$ a $0.2486$. Este es el resultado de que los valores están normalizados tal como se muestra en la @eq-bolsa-palabras.\n\nLa @fig-repr-texto-nube muestra la nube de palabras generada con los términos y sus respectivos valores IDF del texto *Es un placer estar platicando con ustedes.*\n\n::: {#cell-fig-repr-texto-nube .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\ntxt = 'Es un placer estar platicando con ustedes.'\ntokens = {tm.id2token[id]: v for id, v in tm[txt]}\nword_cloud = WordCloud().generate_from_frequencies(tokens)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.grid(False)\nplt.tick_params(left=False, right=False, labelleft=False,\n                labelbottom=False, bottom=False)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de términos](05RepresentacionTexto_files/figure-html/fig-repr-texto-nube-output-1.png){#fig-repr-texto-nube width=540 height=280}\n:::\n:::\n\n\n::: {#e802965e .cell execution_count=10}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.01645519294478695\n```\n:::\n:::\n\n\n::: {#f749f5a2 .cell execution_count=11}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'Estoy dando una platica en Morelia.'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.2035427118119315\n```\n:::\n:::\n\n\n::: {#6fd688c7 .cell execution_count=12}\n``` {.python .cell-code}\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nD = list(tweet_iterator(fname))\n```\n:::\n\n\n::: {#7cd39981 .cell execution_count=13}\n``` {.python .cell-code}\nX = tm.transform(D)\ndis = np.dot(X, X.T)\n```\n:::\n\n\n::: {#cell-fig-text-repr-similitud-bow .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nsns.displot(dis.data)\n```\n\n::: {.cell-output .cell-output-display}\n![Histograma de la similitud](05RepresentacionTexto_files/figure-html/fig-text-repr-similitud-bow-output-1.png){#fig-text-repr-similitud-bow width=471 height=470}\n:::\n:::\n\n\n## Bolsa de Palabras Densa \n\nLa @fig-repr-texto-bolsa-densa muestra el procedimiento que se sigue para representar un texto en una bolsa de palabras dispersa. En primer lugar la bolsa de palabras densa considera que los vectores asociados a los términos se encuentra pre-entrenados y en general no es factible entrenarlos en el momento, esto por el tiempo que lleva estimar estos vectores. \n\n\n\n```{mermaid}\n%%| echo: false\n%%| fig-cap: Diagrama Bolsa de Palabras Densa\n%%| label: fig-repr-texto-bolsa-densa\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados -->  A[Asociación]\n    A --> Repr([Representación])\n```\n\n\n\nEl texto se representa como el vector $\\mathbf u$ que se calcula usando la @eq-bolsa-densas donde se observa que es la suma de los vectores asociados a cada término más un coeficiente $\\mathbf{w_0}$. En particular el coeficiente $\\mathbf{w_0} \\in \\mathbb R^{M}$ no se encuentra en todas las representaciones densas, pero en la representación que se usará contiene este vector, $M$ es la dimensión de la representación densa. \n\n$$\n\\mathbf u = \\sum_t \\mathbf{u_t} + \\mathbf{w_0}.\n$$ {#eq-bolsa-densas}\n\nEn vector $\\mathbf {u_t}$ está asociado al término $t$, en particular este vector en la representación densa que se describirá está definido en términos de una bolsa de palabras dispersa (@eq-bolsa-palabras) como se puede observar en la @eq-bolsa-densa-ut\n\n$$\n\\mathbf{u_t} = \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n$$ {#eq-bolsa-densa-ut}\n\ndonde $\\mathbf W \\in \\mathbb R^{M \\times d}$ es la matriz que hace la proyección de la representación dispersa a la representación densa, se puede observar esa operación está normalizada con la norma Euclideana de la representación dispersa. \n\nCombinando las @eq-bolsa-densas y @eq-bolsa-densa-ut queda la \n\n$$\n\\begin{split}\n\\mathbf{u_t} &= \\sum_t \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0} \\\\\n&= \\mathbf W \\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0},\n\\end{split}\n$$\n\ndonde se puede observar la representación dispersa (@eq-bolsa-palabras), i.e., $\\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}$ lo cual resulta en la @eq-bolsa-densa-texto\n\n$$\n\\mathbf u = \\mathbf W \\mathbf x + \\mathbf{w_0},\n$$ {#eq-bolsa-densa-texto}\n\nque representa un texto en el vector $\\mathbf u \\in \\mathbb R^M.$\n\n### Ejemplos \n\n::: {#7f00d829 .cell execution_count=15}\n``` {.python .cell-code}\ndense = DenseBoW(lang='es',\n                 voc_size_exponent=15,\n                 emoji=False, keyword=True,\n                 distance_hyperplane=True,\n                 dataset=False)\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad.'\ntxt3 = 'Estoy dando una platica en Morelia.'\nX = dense.transform([txt1, txt2, txt3])\nnp.dot(X[0], X[1]), np.dot(X[0], X[2])\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```\n(0.7728943423183761, 0.8721107462230386)\n```\n:::\n:::\n\n\nLa @fig-repr-texto-nube-densa muestra la nube de palabras generada con las características y sus respectivos valores del texto *Es un placer estar platicando con ustedes.* y *La lluvia genera un caos en la ciudad.*\n\n::: {#cell-fig-repr-texto-nube-densa .cell execution_count=16}\n``` {.python .cell-code code-fold=\"true\"}\nvalues = dense.transform([txt1, txt2])\nnames = dense.names\ntokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v > 0}\ntokens_neg = {names[id]: v for id, v in enumerate(values[1]) if v > 0}\n\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_pos, word_neg],\n                     [ax1, ax2],\n                     ['Es un ... ustedes.', \n                      'La lluvia ... ciudad.']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características positivas y negativas](05RepresentacionTexto_files/figure-html/fig-repr-texto-nube-densa-output-1.png){#fig-repr-texto-nube-densa width=540 height=157}\n:::\n:::\n\n\n::: {#784829db .cell execution_count=17}\n``` {.python .cell-code}\nX = dense.transform(D)\nunit = np.linalg.norm(X, axis=1)\nX = X / np.atleast_2d(unit).T\ndis = np.dot(X, X.T)\n```\n:::\n\n\n::: {#cell-fig-text-repr-similitud-dense .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nsns.displot(dis.flatten())\n```\n\n::: {.cell-output .cell-output-display}\n![Histograma de la similitud usando bolsa de palabras densas](05RepresentacionTexto_files/figure-html/fig-text-repr-similitud-dense-output-1.png){#fig-text-repr-similitud-dense width=471 height=470}\n:::\n:::\n\n\n",
    "supporting": [
      "05RepresentacionTexto_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}