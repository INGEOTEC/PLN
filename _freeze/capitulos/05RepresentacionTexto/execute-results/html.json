{
  "hash": "10cdf5b877ee10468c0a7121bd3c7baf",
  "result": {
    "markdown": "# Representación de Texto\n\nEl **objetivo** de la unidad es \n\n## Paquetes usados {.unnumbered}\n\n::: {#171b6641 .cell execution_count=1}\n``` {.python .cell-code}\nfrom EvoMSA import BoW,\\\n                   DenseBoW\nfrom microtc.utils import tweet_iterator\nfrom wordcloud import WordCloud                            \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n```\n:::\n\n\n\n\n\n\n::: {.content-visible when-format=\"html\"}\n\n---\n\n**Video explicando la unidad**\n\n---\n\n:::\n\n## Bolsa de Palabras Dispersa \n\nLa idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (@sec-manejando-texto), cada token $t$ sea asociado a un vector único $\\mathbf{v_t} \\in \\mathbb R^d$ donde la $i$-ésima componente, i.e., $\\mathbf{v_t}_i$, es diferente de cero y $\\forall_{j \\neq i} \\mathbf{v_t}_j=0$. Es decir la $i$-ésima componente está asociada al token $t$, se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token $t$ está en la posición $i$. Por otro lado el valor que contiene la componente se usa para representar alguna característica del token. \n\nEl conjunto de vectores $\\mathbf v$ corresponde al vocabulario, teniendo $d$ diferentes token en el mismo y por definición $\\forall_{i \\neq j} \\mathbf{v_i} \\cdot \\mathbf{v_j} = 0$, donde $\\mathbf{v_i} \\in \\mathbb R^d$, $\\mathbf{v_j} \\in \\mathbb R^d$, y $(\\cdot)$ es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado. \n\nUsando esta notación, un texto $x$ está representado por una secuencia de términos, i.e., $(t_1, t_2, \\ldots)$; la secuencia puede tener repeticiones es decir, $t_j = t_k$. Utilizando la característica de que cada token está asociado a un vector $\\mathbf v$, se transforma la secuencia de términos a una secuencia de vectores (manteniendo las repeticiones), i.e., $(\\mathbf{v_{t_1}}, \\mathbf{v_{t_2}}, \\ldots)$. Finalmente, el texto $x$ se representa como:\n\n$$\n\\mathbf x = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n$$ {#eq-bolsa-palabras}\n\ndonde la suma se hace para todos los elementos de la secuencia, $\\mathbf x \\in \\mathbb R^d$, y $\\lVert \\mathbf w \\rVert$ es la norma Euclideana del vector $\\mathbf w.$\n\n\n\n```{mermaid}\n%%| echo: false\n%%| fig-cap: Diagrama Bolsa de Palabras Dispersa\n%%| label: fig-repr-texto-bolsa-dispersa\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados -->  A[Asociación]\n    Terminos --> Entrenamiento[Estimación\\n de Pesos]\n    Corpus([Corpus]) -.-> Entrenamiento\n    Entrenamiento --> A\n    A --> Repr([Representación])\n```\n\n\n\nAntes de iniciar la descripción detallada del proceso de representación utilizando una bolsa de palabras dispersas, es conveniente ilustrar este proceso mediante la @fig-repr-texto-bolsa-dispersa. El **texto segmentado** es el resultado del proceso ilustrado en @fig-pre-procesamiento. El texto segmentado puede seguir dos caminos, en la parte superior se encuentra el caso cuando los pesos han sido identificados previamente y en la parte inferior es el procedimiento cuando los pesos se estiman mediante un corpus específico que normalmente es un conjunto de entrenamiento. \n\n### Pesado de Términos \n\nComo se había mencionado el valor que tiene la componente $i$-ésima del vector $\\mathbf{v_t}_i$ corresponde a una característica del término asociado, este procedimiento se le conoce como el **esquema de pesado**. Por ejemplo, si el valor es $1$ (i.e., $\\mathbf{v_{t_i}} = 1$) entonces el valor está indicando solo la presencia del término, este es el caso más simple. Considerando la @eq-bolsa-palabras se observa que el resultado, $\\mathbf x$, cuenta las repeticiones de cada término, por esta característica a este esquema se le conoce como **frecuencia de términos** (*term frequency (TF)*). \n\nUna medida que complementa la información que tiene la frecuencia de términos es el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)*) en la colección, esta medida propuesta por @Jones1972 se usa en un método de pesado descrito por @Salton1973 el cual es conocido como **TFIDF**. Este método de pesado propone el considerar el producto de la frecuencia del término y el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)* ) en la colección como el peso del término.\n\n\n### Ejemplos {#sec-bolsa-dispersa-ejemplos}\n\n\n\nEn los siguientes ejemplos se usa una bolsa de palabras con un pesado TFIDF pre-entrenada, los datos de esta bolsa de palabras se encuentra en el atributo `BoW.bow`. El tamaño del vocabulario es $131072$, que está compuesto por palabras, gramas de palabras y caracteres. En el siguiente ejemplo se muestran los primeros tres gramas con sus respectivos valores TFIDF de la frase *Buen día*. Se puede observar que el `tm` regresa una lista de pares, donde la primera parte es el identificador del término, e.g., $11219$ y el segundo es el valor TFIDF, e.g., $0.3984$. La lista tiene un tamaño de $27$ elementos, el resto de los  $131072$ componentes son cero dado que no se encuentran en el texto representado. \n\n::: {#a9fdc099 .cell execution_count=5}\n``` {.python .cell-code}\nbow = BoW(lang='es')\ntm = bow.bow\nvec = tm['Buen día']\nvec[:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\n[(11219, 0.3984336285263178),\n (11018, 0.3245843730253675),\n (24409, 0.2377856890280623)]\n```\n:::\n:::\n\n\nEl uso del identificador del término se puede reemplazar por el término para poder visualizar mejor la representación del texto en el espacio vectorial. El diccionario que se encuentra en `BoW.names` hace la relación identificador a término. Se puede ver que el primer elemento del vector es el bigrama *buen~dia*, seguido por *buen* y el tercer término es *dia*. Los siguientes términos que no se muestran corresponden a gramas de caracteres. El valor TFIDF no indica la importancia del término, mientras mayor sea el valor, se considera más importante de acuerdo al TFIDF. En este ejemplo el bigrama tiene más importancia que las palabras y la palabra *buen* es más significativa que *dia*.\n\n::: {#7e2cd66d .cell execution_count=6}\n``` {.python .cell-code}\n[(bow.names[k], v)\n for k, v in vec[:3]]\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```\n[('buen~dia', 0.3984336285263178),\n ('buen', 0.3245843730253675),\n ('dia', 0.2377856890280623)]\n```\n:::\n:::\n\n\n\n\nCon el objetivo de ilustrar una heurística que ha dado buenos resultados en el siguiente ejemplo se presentan las primeras cuatro componentes del texto *Buen día colegas*. Se puede observar como los valores de IDF de los términos comunes cambiaron, por ejemplo para el caso de *buen~dia* cambio de $0.3984$ a $0.2486$. Este es el resultado de que los valores están normalizados tal como se muestra en la @eq-bolsa-palabras. Por otro lado, se observa que ahora el término más significativo es la palabra *colegas*. \n\n::: {#9e3a1268 .cell execution_count=8}\n``` {.python .cell-code}\ntxt = 'Buen día colegas'\n[(tm.id2token[k], v)\n for k, v in tm[txt][:4]]\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n[('buen~dia', 0.24862785236357487),\n ('buen', 0.20254494048246244),\n ('dia', 0.1483814139998851),\n ('colegas', 0.3538047214393573)]\n```\n:::\n:::\n\n\nUna manera de visualizar la representación es creando una nube de palabras de los términos, donde el tamaño del termino corresponde al valor TFIDF. En la @fig-repr-texto-nube muestra la nube de palabras generada con los términos y sus respectivos valores IDF del texto *Es un placer estar platicando con ustedes.*\n\n::: {#cell-fig-repr-texto-nube .cell execution_count=9}\n``` {.python .cell-code code-fold=\"true\"}\ntxt = 'Es un placer estar platicando con ustedes.'\ntokens = {tm.id2token[id]: v for id, v in tm[txt]}\nword_cloud = WordCloud().generate_from_frequencies(tokens)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.grid(False)\nplt.tick_params(left=False, right=False, labelleft=False,\n                labelbottom=False, bottom=False)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de términos](05RepresentacionTexto_files/figure-html/fig-repr-texto-nube-output-1.png){#fig-repr-texto-nube width=540 height=280}\n:::\n:::\n\n\nEl texto se representa en un espacio vectorial, entonces es posible comparar la similitud entre dos textos en esta representación, por ejemplo, en el siguiente ejemplo se compara la similitud coseno entre los textos *Es un placer estar platicando con ustedes.* y *La lluvia genera un caos en la ciudad.* El valor obtenido es cercano a cero indicando que estos textos no son similares. \n\n::: {#08ac3949 .cell execution_count=10}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad.'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\n0.01645519294478695\n```\n:::\n:::\n\n\nComplementando el ejemplo anterior, en esta ocasión se comparan dos textos que comparten el concepto *plática*, estos son *Es un placer estar platicando con ustedes.* y *Estoy dando una platica en Morelia.* se puede observar que estos textos son más similares que los ejemplos anteriores. \n\n::: {#d68600e0 .cell execution_count=11}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'Estoy dando una platica en Morelia.'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```\n0.2035427118119315\n```\n:::\n:::\n\n\n Habiendo realizado la similitud entre algunos textos lleva a preguntarse cómo será la distribución de similitud entre varios textos, para poder contestar esta pregunta, se utilizarán los datos de [Delitos](https://ingeotec.github.io/Delitos), los cuales se guardan en la variable `D` tal y como se en las siguientes instrucciones. \n\n::: {#6c0fed00 .cell execution_count=12}\n``` {.python .cell-code}\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nD = list(tweet_iterator(fname))\n```\n:::\n\n\nEl primer paso es representar todos los textos en el espacio vectorial de la bolsa de palabras, lo cual se logra con el método `BoW.transform` (primera linea), el segundo paso es calcular la similitud entre todos los textos, como se muestra en la segunda linea. \n\n::: {#18cecca8 .cell execution_count=13}\n``` {.python .cell-code}\nX = tm.transform(D)\nsim = np.dot(X, X.T)\n```\n:::\n\n\nLa distribución de similitud se muestra en la @fig-text-repr-similitud-bow se puede observar que las similitudes se encuentran concentradas cerca del cero, esto indica que la mayoría de los textos están distantes, esto es el resultado de la bolsa de palabras discreta que se enfoca en modelar las palabras y no el significado de las mismas. \n\n::: {#cell-fig-text-repr-similitud-bow .cell execution_count=14}\n``` {.python .cell-code code-fold=\"true\"}\nsns.displot(sim.data)\n```\n\n::: {.cell-output .cell-output-display}\n![Histograma de la similitud](05RepresentacionTexto_files/figure-html/fig-text-repr-similitud-bow-output-1.png){#fig-text-repr-similitud-bow width=471 height=470}\n:::\n:::\n\n\n## Bolsa de Palabras Densa \n\nLa @fig-repr-texto-bolsa-densa muestra el procedimiento que se sigue para representar un texto en una bolsa de palabras dispersa. En primer lugar la bolsa de palabras densa considera que los vectores asociados a los términos se encuentra pre-entrenados y en general no es factible entrenarlos en el momento, esto por el tiempo que lleva estimar estos vectores. \n\n\n\n```{mermaid}\n%%| echo: false\n%%| fig-cap: Diagrama Bolsa de Palabras Densa\n%%| label: fig-repr-texto-bolsa-densa\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados -->  A[Asociación]\n    A --> Repr([Representación])\n```\n\n\n\nEl texto se representa como el vector $\\mathbf u$ que se calcula usando la @eq-bolsa-densas donde se observa que es la suma de los vectores asociados a cada término más un coeficiente $\\mathbf{w_0}$. En particular el coeficiente $\\mathbf{w_0} \\in \\mathbb R^{M}$ no se encuentra en todas las representaciones densas, pero en la representación que se usará contiene este vector, $M$ es la dimensión de la representación densa. \n\n$$\n\\mathbf u = \\sum_t \\mathbf{u_t} + \\mathbf{w_0}.\n$$ {#eq-bolsa-densas}\n\nEn vector $\\mathbf {u_t}$ está asociado al término $t$, en particular este vector en la representación densa que se describirá está definido en términos de una bolsa de palabras dispersa (@eq-bolsa-palabras) como se puede observar en la @eq-bolsa-densa-ut\n\n$$\n\\mathbf{u_t} = \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n$$ {#eq-bolsa-densa-ut}\n\ndonde $\\mathbf W \\in \\mathbb R^{M \\times d}$ es la matriz que hace la proyección de la representación dispersa a la representación densa, se puede observar esa operación está normalizada con la norma Euclideana de la representación dispersa. \n\nCombinando las @eq-bolsa-densas y @eq-bolsa-densa-ut queda la \n\n$$\n\\begin{split}\n\\mathbf{u_t} &= \\sum_t \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0} \\\\\n&= \\mathbf W \\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0},\n\\end{split}\n$$\n\ndonde se puede observar la representación dispersa (@eq-bolsa-palabras), i.e., $\\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}$ lo cual resulta en la @eq-bolsa-densa-texto\n\n$$\n\\mathbf u = \\mathbf W \\mathbf x + \\mathbf{w_0},\n$$ {#eq-bolsa-densa-texto}\n\nque representa un texto en el vector $\\mathbf u \\in \\mathbb R^M.$\n\nPara algunas representaciones densas, las componentes de la matriz de transformación $\\mathcal W$ están asociadas a conceptos, en el caso que se analiza estas están asociadas a palabras claves o emojis. \n\n### Ejemplos \n\nContinuando con los ejemplos presentados para la bolsa dispersa (@sec-bolsa-dispersa-ejemplos) en esta sección se hace el análisis con la representación de palabras densa. El primer paso es inicializar la clase que contiene las representaciones densas, esto se hace con la siguiente instrucción. \n\n::: {#feff6f37 .cell execution_count=15}\n``` {.python .cell-code}\ndense = DenseBoW(lang='es',\n                 voc_size_exponent=15,\n                 emoji=False, keyword=True,\n                 distance_hyperplane=True,\n                 dataset=False)\n```\n:::\n\n\nPara representar un texto en el espacio vectorial denso se utiliza el método `transform`, por ejemplo la siguiente instrucción representa el texto *Es un placer estar platicando con ustedes.* Solo se visualizan los valores de las primeras tres componentes.\n\n::: {#df5742af .cell execution_count=16}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ndense.transform([txt1])[0, :3]\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n```\narray([-0.0042934 , -0.00429635, -0.00515905])\n```\n:::\n:::\n\n\nLo primero que se observa es que los valores son negativos, a diferencia del caso disperso donde todos los valores son positivos. En este tipo de representación cada componente está asociada a una palabra las cuales se pueden conocer en el atributo `names`. El siguiente código muestra las tres primeras palabras asociadas al ejemplo anterior. \n\n::: {#10126283 .cell execution_count=17}\n``` {.python .cell-code}\ndense.names[:3]\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```\n['semanas', 'cuatro', 'piensa']\n```\n:::\n:::\n\n\nSiguiente la idea de utilizar una nube de palabras para visualizar el vector que representa el texto modelado, La @fig-repr-texto-nube-densa muestra las nubes de palabras generada con las características y sus respectivos valores del texto *Es un placer estar platicando con ustedes.* Durante la generación de la nube de palabras se decidió representar genera una nube de palabras con las palabras con coeficiente negativo más significativo y aquellas con los coeficientes positivos más significativos. Se puede observar que las palabras positivas contienen componentes que están relacionados al enunciado, pero al mismo tiempo leyendo los términos positivos es complicado construir el texto representado. Adicionalmente las términos negativos que se observan en la nube de palabras en su mayoría son hashtags que tiene muy poca relación al texto representado. \n\n::: {#cell-fig-repr-texto-nube-densa .cell execution_count=18}\n``` {.python .cell-code code-fold=\"true\"}\nvalues = dense.transform([txt1])\nnames = dense.names\ntokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v > 0}\ntokens_neg = {names[id]: v * -1 for id, v in enumerate(values[0]) if v < 0}\n\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_neg, word_pos],\n                     [ax1, ax2],\n                     ['Negativas', \n                      'Positivas']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características para el texto *Es un placer estar platicando con ustedes.*](05RepresentacionTexto_files/figure-html/fig-repr-texto-nube-densa-output-1.png){#fig-repr-texto-nube-densa width=540 height=157}\n:::\n:::\n\n\nEsta representación también permite comparación de similitud entre textos, en el siguiente ejemplo se calcula la similitud entre el texto *Es un placer estar platicando con ustedes.* y los textos *La lluvia genera un caos en la ciudad.* y *Estoy dando una platica en Morelia.* tal y como se hizo para la representación dispersa. Se puede observar que existe una mayor similitud entre los textos que contienen el concepto **plática**, lo cual es equivalente a lo que se observó en el ejemplo con bolsa de palabras discretas, pero los valores son significativamente mayores que en ese caso.\n\n::: {#24d526d4 .cell execution_count=19}\n``` {.python .cell-code}\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad.'\ntxt3 = 'Estoy dando una platica en Morelia.'\nX = dense.transform([txt1, txt2, txt3])\nnp.dot(X[0], X[1]), np.dot(X[0], X[2])\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n(0.7728943423183761, 0.8721107462230386)\n```\n:::\n:::\n\n\nLos valores de similitud entre los enunciados anteriores, se puede visualizar en una nube de palabras, utilizando solo las características positivas. La @fig-repr-texto-nube-densa-comp muestra las nubes de palabras generadas, en ellas es complicado comprender la razón por la cual la frases que tiene el concepto *plática* están más cercanas, es probable que la cola de la distribución, es decir, las palabras menos significativas son las que acercan las dos oraciones. \n\n::: {#cell-fig-repr-texto-nube-densa-comp .cell execution_count=20}\n``` {.python .cell-code code-fold=\"true\"}\nvalues = dense.transform([txt1, txt2, txt3])\nnames = dense.names\ntokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v > 0}\ntokens_neg = {names[id]: v for id, v in enumerate(values[1]) if v > 0}\ntokens_otro = {names[id]: v for id, v in enumerate(values[2]) if v > 0}\n\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\nword_otro = WordCloud().generate_from_frequencies(tokens_otro)\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n\nfor cloud, ax, title in zip([word_pos, word_neg, word_otro],\n                     [ax1, ax2, ax3],\n                     ['Es un ... ustedes.', \n                      'La lluvia ... ciudad.',\n                      'Estoy ... Morelia.']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n```\n\n::: {.cell-output .cell-output-display}\n![Nube de características positivas.](05RepresentacionTexto_files/figure-html/fig-repr-texto-nube-densa-comp-output-1.png){#fig-repr-texto-nube-densa-comp width=540 height=115}\n:::\n:::\n\n\nAl igual que en el caso disperso se puede calcular la distribución de similitud. Las siguientes instrucciones calcula la similitud coseno entre todos los ejemplos del conjunto de entrenamiento ($\\mathcal T$). \n\n::: {#57e495f2 .cell execution_count=21}\n``` {.python .cell-code}\nX = dense.transform(D)\nsim = np.dot(X, X.T)\n```\n:::\n\n\nLa @fig-text-repr-similitud-dense muestra el histograma de las similitudes calculada mediante la bolsa densa. Aquí se puede observar que la gran mayoría de los ejemplos tiene una similitud mayor y tiene una desviación estándar mayor que la vista en la @fig-text-repr-similitud-bow. \n\n::: {#cell-fig-text-repr-similitud-dense .cell execution_count=22}\n``` {.python .cell-code code-fold=\"true\"}\nsns.displot(sim.flatten())\n```\n\n::: {.cell-output .cell-output-display}\n![Histograma de la similitud usando bolsa de palabras densas](05RepresentacionTexto_files/figure-html/fig-text-repr-similitud-dense-output-1.png){#fig-text-repr-similitud-dense width=471 height=470}\n:::\n:::\n\n\n",
    "supporting": [
      "05RepresentacionTexto_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}