# Representación de Texto

El **objetivo** de la unidad es 

## Paquetes usados {.unnumbered}

```{python}
#| echo: true
from EvoMSA import BoW,\
                   DenseBoW
from microtc.utils import tweet_iterator
from wordcloud import WordCloud                            
import numpy as np
import pandas as pd
from matplotlib import pylab as plt
import seaborn as sns
```

```{python}
#| echo: false
from IPython.display import Markdown
```

```{python}
#| echo: false
#| output: false
from os.path import isfile, isdir
from EvoMSA.utils import Download
from EvoMSA import utils
utils.USE_TQDM = False
if not isfile('delitos.zip'):
    Download('https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip',
             'delitos.zip')
if not isdir('delitos'):
    !unzip -Pingeotec delitos.zip
```


::: {.content-visible when-format="html"}
---

**Video explicando la unidad**

---
:::

## Bolsa de Palabras Dispersa 

La idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (@sec-manejando-texto), cada token $t$ sea asociado a un vector único $\mathbf{v_t} \in \mathbb R^d$ donde la $i$-ésima componente, i.e., $\mathbf{v_t}_i$, es diferente de cero y $\forall_{j \neq i} \mathbf{v_t}_j=0$. Es decir la $i$-ésima componente está asociada al token $t$, se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token $t$ está en la posición $i$. Por otro lado el valor que contiene la componente se usa para representar alguna característica del token. 

El conjunto de vectores $\mathbf v$ corresponde al vocabulario, teniendo $d$ diferentes token en el mismo y por definición $\forall_{i \neq j} \mathbf{v_i} \cdot \mathbf{v_j} = 0$, donde $\mathbf{v_i} \in \mathbb R^d$, $\mathbf{v_j} \in \mathbb R^d$, y $(\cdot)$ es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado. 

Usando esta notación, un texto $x$ está representado por una secuencia de tokens, i.e., $(t_1, t_2, \ldots)$; la secuencia puede tener repeticiones es decir, $t_j = t_k$. Utilizando la característica de que cada token está asociado a un vector $\mathbf v$, se transforma la secuencia de tokens a una secuencia de vectores (manteniendo las repeticiones), i.e., $(\mathbf{v_{t_1}}, \mathbf{v_{t_2}}, \ldots)$. Finalmente, el texto $x$ se representa como:

$$
\mathbf x = \frac{\sum_t \mathbf{v_t}}{\lVert \sum_t \mathbf{v_t} \rVert},
$$ {#eq-bolsa-palabras}

donde la suma se hace para todos los elementos de la secuencia, $\mathbf x \in \mathbb R^d$, y $\lVert \mathbf w \rVert$ es la norma Euclideana del vector $\mathbf w.$