# Representación de Texto

El **objetivo** de la unidad es 

## Paquetes usados {.unnumbered}

```{python}
#| echo: true
from EvoMSA import BoW,\
                   DenseBoW
from microtc.utils import tweet_iterator
from wordcloud import WordCloud                            
import numpy as np
import pandas as pd
from matplotlib import pylab as plt
import seaborn as sns
```

```{python}
#| echo: false
from IPython.display import Markdown
sns.set_style('whitegrid')
```

```{python}
#| echo: false
#| output: false
from os.path import isfile, isdir
from EvoMSA.utils import Download
from EvoMSA import utils
utils.USE_TQDM = False
if not isfile('delitos.zip'):
    Download('https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip',
             'delitos.zip')
if not isdir('delitos'):
    !unzip -Pingeotec delitos.zip
```


::: {.content-visible when-format="html"}
---

**Video explicando la unidad**

---
:::

## Bolsa de Palabras Dispersa 

La idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (@sec-manejando-texto), cada token $t$ sea asociado a un vector único $\mathbf{v_t} \in \mathbb R^d$ donde la $i$-ésima componente, i.e., $\mathbf{v_t}_i$, es diferente de cero y $\forall_{j \neq i} \mathbf{v_t}_j=0$. Es decir la $i$-ésima componente está asociada al token $t$, se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token $t$ está en la posición $i$. Por otro lado el valor que contiene la componente se usa para representar alguna característica del token. 

El conjunto de vectores $\mathbf v$ corresponde al vocabulario, teniendo $d$ diferentes token en el mismo y por definición $\forall_{i \neq j} \mathbf{v_i} \cdot \mathbf{v_j} = 0$, donde $\mathbf{v_i} \in \mathbb R^d$, $\mathbf{v_j} \in \mathbb R^d$, y $(\cdot)$ es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado. 

Usando esta notación, un texto $x$ está representado por una secuencia de términos, i.e., $(t_1, t_2, \ldots)$; la secuencia puede tener repeticiones es decir, $t_j = t_k$. Utilizando la característica de que cada token está asociado a un vector $\mathbf v$, se transforma la secuencia de términos a una secuencia de vectores (manteniendo las repeticiones), i.e., $(\mathbf{v_{t_1}}, \mathbf{v_{t_2}}, \ldots)$. Finalmente, el texto $x$ se representa como:

$$
\mathbf x = \frac{\sum_t \mathbf{v_t}}{\lVert \sum_t \mathbf{v_t} \rVert},
$$ {#eq-bolsa-palabras}

donde la suma se hace para todos los elementos de la secuencia, $\mathbf x \in \mathbb R^d$, y $\lVert \mathbf w \rVert$ es la norma Euclideana del vector $\mathbf w.$

```{mermaid}
%%| echo: false
%%| fig-cap: Diagrama Bolsa de Palabras Dispersa
%%| label: fig-repr-texto-bolsa-dispersa

flowchart LR
    Terminos([Texto\n Segmentado]) -- Pre-entrenados -->  A[Asociación]
    Terminos --> Entrenamiento[Estimación\n de Pesos]
    Corpus([Corpus]) -.-> Entrenamiento
    Entrenamiento --> A
    A --> Repr([Representación])
```

Antes de iniciar la descripción detallada del proceso de representación utilizando una bolsa de palabras dispersas, es conveniente ilustrar este proceso mediante la @fig-repr-texto-bolsa-dispersa. El **texto segmentado** es el resultado del proceso ilustrado en @fig-pre-procesamiento. El texto segmentado puede seguir dos caminos, en la parte superior se encuentra el caso cuando los pesos han sido identificados previamente y en la parte inferior es el procedimiento cuando los pesos se estiman mediante un corpus específico que normalmente es un conjunto de entrenamiento. 

### Pesado de Términos 

Como se había mencionado el valor que tiene la componente $i$-ésima del vector $\mathbf{v_t}_i$ corresponde a una característica del término asociado, este procedimiento se le conoce como el **esquema de pesado**. Por ejemplo, si el valor es $1$ (i.e., $\mathbf{v_{t_i}} = 1$) entonces el valor está indicando solo la presencia del término, este es el caso más simple. Considerando la @eq-bolsa-palabras se observa que el resultado, $\mathbf x$, cuenta las repeticiones de cada término, por esta característica a este esquema se le conoce como **frecuencia de términos** (*term frequency (TF)*). 

Una medida que complementa la información que tiene la frecuencia de términos es el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)*) en la colección, esta medida propuesta por @Jones1972 se usa en un método de pesado descrito por @Salton1973 el cual es conocido como **TFIDF**. Este método de pesado propone el considerar el producto de la frecuencia del término y el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)* ) en la colección como el peso del término.


### Ejemplos {#sec-bolsa-dispersa-ejemplos}

```{python} 
#| echo: false
bow = BoW(lang='es')
tm = bow.bow
vec = tm['Buen día']
key_f = Markdown(f'${vec[0][0]}$')
idf_f = Markdown(f'${vec[0][1]:0.4f}$')
len_f = Markdown(f'${len(vec)}$')
voc_f = Markdown(f'${len(bow.names)}$')
```

En los siguientes ejemplos se usa una bolsa de palabras con un pesado TFIDF pre-entrenada, los datos de esta bolsa de palabras se encuentra en el atributo `BoW.bow`. El tamaño del vocabulario es `{python} voc_f`, que está compuesto por palabras, gramas de palabras y caracteres. En el siguiente ejemplo se muestran los primeros tres gramas con sus respectivos valores TFIDF de la frase *Buen día*. Se puede observar que el `tm` regresa una lista de pares, donde la primera parte es el identificador del término, e.g., `{python} key_f` y el segundo es el valor TFIDF, e.g., `{python} idf_f`. La lista tiene un tamaño de `{python} len_f` elementos, el resto de los  `{python} voc_f` componentes son cero dado que no se encuentran en el texto representado. 

```{python} 
#| echo: true
tm = BoW(lang='es').bow
vec = tm['Buen día']
vec[:3]
```

El uso del identificador del término se puede reemplazar por el término para poder visualizar mejor la representación del texto en el espacio vectorial. El diccionario que se encuentra en `tm.id2token` hace la relación identificador a término. Se puede ver que el primer elemento del vector es el bigrama *buen~dia*, seguido por *buen* y el tercer término es *dia*. Los siguientes términos que no se muestran corresponden a gramas de caracteres. El valor TFIDF no indica la importancia del término, mientras mayor sea el valor, se considera más importante de acuerdo al TFIDF. En este ejemplo el bigrama tiene más importancia que las palabras y la palabra *buen* es más significativa que *dia*.

```{python} 
#| echo: true
[(tm.id2token[k], v)
 for k, v in vec[:3]]
```


```{python} 
#| echo: false
txt = 'Buen día colegas'
vec2 = tm[txt]
idf2_f = Markdown(f'${vec2[0][1]:0.4f}$')
```

Con el objetivo de ilustrar una heurística que ha dado buenos resultados en el siguiente ejemplo se presentan las primeras cuatro componentes del texto *Buen día colegas*. Se puede observar como los valores de IDF de los términos comunes cambiaron, por ejemplo para el caso de `{python} key_f` cambio de `{python} idf_f` a `{python} idf2_f`. Este es el resultado de que los valores están normalizados tal como se muestra en la @eq-bolsa-palabras. Por otro lado, se observa que ahora el término más significativo es la palabra *colegas*. 

```{python} 
#| echo: true
txt = 'Buen día colegas'
[(tm.id2token[k], v)
 for k, v in tm[txt][:4]]
```

Una manera de visualizar la representación es creando una nube de palabras de los términos, donde el tamaño del termino corresponde al valor TFIDF. En la @fig-repr-texto-nube muestra la nube de palabras generada con los términos y sus respectivos valores IDF del texto *Es un placer estar platicando con ustedes.*

```{python}
#| code-fold: true
#| fig-cap: Nube de términos
#| label: fig-repr-texto-nube
txt = 'Es un placer estar platicando con ustedes.'
tokens = {tm.id2token[id]: v for id, v in tm[txt]}
word_cloud = WordCloud().generate_from_frequencies(tokens)
plt.imshow(word_cloud, interpolation='bilinear')
plt.grid(False)
plt.tick_params(left=False, right=False, labelleft=False,
                labelbottom=False, bottom=False)
```

El texto se representa en un espacio vectorial, entonces es posible comparar la similitud entre dos textos en esta representación, por ejemplo, en el siguiente ejemplo se compara la similitud coseno entre los textos *Es un placer estar platicando con ustedes.* y *La lluvia genera un caos en la ciudad.* El valor obtenido es cercano a cero indicando que estos textos no son similares. 

```{python} 
#| echo: true
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'La lluvia genera un caos en la ciudad.'
vec1 = tm[txt1]
vec2 = tm[txt2]
f = {k: v for k, v in vec1}
np.sum([f[k] * v for k, v in vec2 if k in f])
```

Complementando el ejemplo anterior, en esta ocasión se comparan dos textos que comparten el concepto *plática*, estos son *Es un placer estar platicando con ustedes.* y *Estoy dando una platica en Morelia.* se puede observar que estos textos son más similares que los ejemplos anteriores. 

```{python} 
#| echo: true
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'Estoy dando una platica en Morelia.'
vec1 = tm[txt1]
vec2 = tm[txt2]
f = {k: v for k, v in vec1}
np.sum([f[k] * v for k, v in vec2 if k in f])
```
 
 Habiendo realizado la similitud entre algunos textos lleva a preguntarse cómo será la distribución de similitud entre varios textos, para poder contestar esta pregunta, se utilizarán los datos de [Delitos](https://ingeotec.github.io/Delitos), los cuales se guardan en la variable `D` tal y como se en las siguientes instrucciones. 

```{python}
#| echo: true
fname = 'delitos/delitos_ingeotec_Es_train.json'
D = list(tweet_iterator(fname))
```

El primer paso es representar todos los textos en el espacio vectorial de la bolsa de palabras, lo cual se logra con el método `BoW.transform` (primera linea), el segundo paso es calcular la similitud entre todos los textos, como se muestra en la segunda linea. 

```{python}
#| echo: true
X = tm.transform(D)
sim = np.dot(X, X.T)
```

La distribución de similitud se muestra en la @fig-text-repr-similitud-bow se puede observar que las similitudes se encuentran concentradas cerca del cero, esto indica que la mayoría de los textos están distantes, esto es el resultado de la bolsa de palabras discreta que se enfoca en modelar las palabras y no el significado de las mismas. 


```{python}
#| code-fold: true
#| fig-cap: Histograma de la similitud
#| label: fig-text-repr-similitud-bow
sns.displot(sim.data)
```

## Bolsa de Palabras Densa 

La @fig-repr-texto-bolsa-densa muestra el procedimiento que se sigue para representar un texto en una bolsa de palabras dispersa. En primer lugar la bolsa de palabras densa considera que los vectores asociados a los términos se encuentra pre-entrenados y en general no es factible entrenarlos en el momento, esto por el tiempo que lleva estimar estos vectores. 

```{mermaid}
%%| echo: false
%%| fig-cap: Diagrama Bolsa de Palabras Densa
%%| label: fig-repr-texto-bolsa-densa

flowchart LR
    Terminos([Texto\n Segmentado]) -- Pre-entrenados -->  A[Asociación]
    A --> Repr([Representación])
```

El texto se representa como el vector $\mathbf u$ que se calcula usando la @eq-bolsa-densas donde se observa que es la suma de los vectores asociados a cada término más un coeficiente $\mathbf{w_0}$. En particular el coeficiente $\mathbf{w_0} \in \mathbb R^{M}$ no se encuentra en todas las representaciones densas, pero en la representación que se usará contiene este vector, $M$ es la dimensión de la representación densa. 

$$
\mathbf u = \sum_t \mathbf{u_t} + \mathbf{w_0}.
$$ {#eq-bolsa-densas}

En vector $\mathbf {u_t}$ está asociado al término $t$, en particular este vector en la representación densa que se describirá está definido en términos de una bolsa de palabras dispersa (@eq-bolsa-palabras) como se puede observar en la @eq-bolsa-densa-ut

$$
\mathbf{u_t} = \frac{\mathbf W \mathbf {v_t}}{\lVert \sum_t \mathbf{v_t} \rVert},
$$ {#eq-bolsa-densa-ut}

donde $\mathbf W \in \mathbb R^{M \times d}$ es la matriz que hace la proyección de la representación dispersa a la representación densa, se puede observar esa operación está normalizada con la norma Euclideana de la representación dispersa. 

Combinando las @eq-bolsa-densas y @eq-bolsa-densa-ut queda la 

$$
\begin{split}
\mathbf{u_t} &= \sum_t \frac{\mathbf W \mathbf {v_t}}{\lVert \sum_t \mathbf{v_t} \rVert} + \mathbf{w_0} \\
&= \mathbf W \frac{\sum_t \mathbf {v_t}}{\lVert \sum_t \mathbf{v_t} \rVert} + \mathbf{w_0},
\end{split}
$$

donde se puede observar la representación dispersa (@eq-bolsa-palabras), i.e., $\frac{\sum_t \mathbf {v_t}}{\lVert \sum_t \mathbf{v_t} \rVert}$ lo cual resulta en la @eq-bolsa-densa-texto

$$
\mathbf u = \mathbf W \mathbf x + \mathbf{w_0},
$$ {#eq-bolsa-densa-texto}

que representa un texto en el vector $\mathbf u \in \mathbb R^M.$

### Ejemplos 

```{python} 
#| echo: true
dense = DenseBoW(lang='es',
                 voc_size_exponent=15,
                 emoji=False, keyword=True,
                 distance_hyperplane=True,
                 dataset=False)
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'La lluvia genera un caos en la ciudad.'
txt3 = 'Estoy dando una platica en Morelia.'
X = dense.transform([txt1, txt2, txt3])
np.dot(X[0], X[1]), np.dot(X[0], X[2])
```


La @fig-repr-texto-nube-densa muestra la nube de palabras generada con las características y sus respectivos valores del texto *Es un placer estar platicando con ustedes.* y *La lluvia genera un caos en la ciudad.*

```{python}
#| code-fold: true
#| fig-cap: Nube de características positivas y negativas
#| label: fig-repr-texto-nube-densa
values = dense.transform([txt1, txt2])
names = dense.names
tokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v > 0}
tokens_neg = {names[id]: v for id, v in enumerate(values[1]) if v > 0}

word_pos = WordCloud().generate_from_frequencies(tokens_pos)
word_neg = WordCloud().generate_from_frequencies(tokens_neg)

fig, (ax1, ax2) = plt.subplots(1, 2)

for cloud, ax, title in zip([word_pos, word_neg],
                     [ax1, ax2],
                     ['Es un ... ustedes.', 
                      'La lluvia ... ciudad.']):
    ax.imshow(cloud, interpolation='bilinear')
    ax.grid(False)
    ax.tick_params(left=False, right=False, labelleft=False,
                   labelbottom=False, bottom=False)
    ax.set_title(title)
```

```{python}
#| echo: true
X = dense.transform(D)
dis = np.dot(X, X.T)
```

```{python}
#| code-fold: true
#| fig-cap: Histograma de la similitud usando bolsa de palabras densas
#| label: fig-text-repr-similitud-dense
sns.displot(dis.flatten())
```