# Representación de Texto

El **objetivo** de la unidad es 

## Paquetes usados {.unnumbered}

```{python}
#| echo: true
from EvoMSA import BoW,\
                   DenseBoW
from microtc.utils import tweet_iterator
from wordcloud import WordCloud                            
import numpy as np
import pandas as pd
from matplotlib import pylab as plt
import seaborn as sns
```

```{python}
#| echo: false
from IPython.display import Markdown
sns.set_style('whitegrid')
```

```{python}
#| echo: false
#| output: false
from os.path import isfile, isdir
from EvoMSA.utils import Download
from EvoMSA import utils
utils.USE_TQDM = False
if not isfile('delitos.zip'):
    Download('https://github.com/INGEOTEC/Delitos/releases/download/Datos/delitos.zip',
             'delitos.zip')
if not isdir('delitos'):
    !unzip -Pingeotec delitos.zip
```


::: {.content-visible when-format="html"}
---

**Video explicando la unidad**

---
:::

## Introducción 

## Bolsa de Palabras Dispersa 

La idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (@sec-manejando-texto), cada token $t$ sea asociado a un vector único $\mathbf{v_t} \in \mathbb R^d$ donde la $i$-ésima componente, i.e., $\mathbf{v_t}_i$, es diferente de cero y $\forall_{j \neq i} \mathbf{v_t}_j=0$. Es decir la $i$-ésima componente está asociada al token $t$, se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token $t$ está en la posición $i$. Por otro lado el valor que contiene la componente se usa para representar alguna característica del token. 

El conjunto de vectores $\mathbf v$ corresponde al vocabulario, teniendo $d$ diferentes token en el mismo y por definición $\forall_{i \neq j} \mathbf{v_i} \cdot \mathbf{v_j} = 0$, donde $\mathbf{v_i} \in \mathbb R^d$, $\mathbf{v_j} \in \mathbb R^d$, y $(\cdot)$ es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado. 

Usando esta notación, un texto $x$ está representado por una secuencia de términos, i.e., $(t_1, t_2, \ldots)$; la secuencia puede tener repeticiones es decir, $t_j = t_k$. Utilizando la característica de que cada token está asociado a un vector $\mathbf v$, se transforma la secuencia de términos a una secuencia de vectores (manteniendo las repeticiones), i.e., $(\mathbf{v_{t_1}}, \mathbf{v_{t_2}}, \ldots)$. Finalmente, el texto $x$ se representa como:

$$
\mathbf x = \frac{\sum_t \mathbf{v_t}}{\lVert \sum_t \mathbf{v_t} \rVert},
$$ {#eq-bolsa-palabras}

donde la suma se hace para todos los elementos de la secuencia, $\mathbf x \in \mathbb R^d$, y $\lVert \mathbf w \rVert$ es la norma Euclideana del vector $\mathbf w.$

```{mermaid}
%%| echo: false
%%| fig-cap: Diagrama Bolsa de Palabras Dispersa
%%| label: fig-repr-texto-bolsa-dispersa

flowchart LR
    Terminos([Texto\n Segmentado]) -- Pre-entrenados -->  A[Asociación]
    Terminos --> Entrenamiento[Estimación\n de Pesos]
    Corpus([Corpus]) -.-> Entrenamiento
    Entrenamiento --> A
    A --> Repr([Representación])
```

Antes de iniciar la descripción detallada del proceso de representación utilizando una bolsa de palabras dispersas, es conveniente ilustrar este proceso mediante la @fig-repr-texto-bolsa-dispersa. El **texto segmentado** es el resultado del proceso ilustrado en @fig-pre-procesamiento. El texto segmentado puede seguir dos caminos, en la parte superior se encuentra el caso cuando los pesos han sido identificados previamente y en la parte inferior es el procedimiento cuando los pesos se estiman mediante un corpus específico que normalmente es un conjunto de entrenamiento. 

### Pesado de Términos 

Como se había mencionado el valor que tiene la componente $i$-ésima del vector $\mathbf{v_t}_i$ corresponde a una característica del término asociado, este procedimiento se le conoce como el **esquema de pesado**. Por ejemplo, si el valor es $1$ (i.e., $\mathbf{v_{t_i}} = 1$) entonces el valor está indicando solo la presencia del término, este es el caso más simple. Considerando la @eq-bolsa-palabras se observa que el resultado, $\mathbf x$, cuenta las repeticiones de cada término, por esta característica a este esquema se le conoce como **frecuencia de términos** (*term frequency (TF)*). 

Una medida que complementa la información que tiene la frecuencia de términos es el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)*) en la colección, esta medida propuesta por @Jones1972 se usa en un método de pesado descrito por @Salton1973 el cual es conocido como **TFIDF**. Este método de pesado propone el considerar el producto de la frecuencia del término y el inverso de la frecuencia del término (*Inverse Document Frequency (IDF)* ) en la colección como el peso del término.


### Ejemplos 

```{python} 
#| echo: false
tm = BoW(lang='es').bow
vec = tm['Buen día']
key_f = Markdown(f'*{tm.id2token[vec[0][0]]}*')
idf_f = Markdown(f'${vec[0][1]:0.4f}$')
```

```{python} 
#| echo: true
tm = BoW(lang='es').bow
vec = tm['Buen día']
vec[:3]
```

```{python} 
#| echo: true
[(tm.id2token[k], v)
 for k, v in vec[:3]]
```


```{python} 
#| echo: true
txt = 'Buen día colegas'
[(tm.id2token[k], v)
 for k, v in tm[txt][:4]]
```

```{python} 
#| echo: false
txt = 'Buen día colegas'
vec2 = tm[txt]
idf2_f = Markdown(f'${vec2[0][1]:0.4f}$')
```

Se puede observar como los valores de IDF de los términos comunes cambiaron, por ejemplo para el caso de `{python} key_f` cambio de `{python} idf_f` a `{python} idf2_f`. Este es el resultado de que los valores están normalizados tal como se muestra en la @eq-bolsa-palabras.

La @fig-repr-texto-nube muestra la nube de palabras generada con los términos y sus respectivos valores IDF del texto *Es un placer estar platicando con ustedes.*

```{python}
#| code-fold: true
#| fig-cap: Nube de términos
#| label: fig-repr-texto-nube
txt = 'Es un placer estar platicando con ustedes.'
tokens = {tm.id2token[id]: v for id, v in tm[txt]}
word_cloud = WordCloud().generate_from_frequencies(tokens)
plt.imshow(word_cloud, interpolation='bilinear')
plt.grid(False)
plt.tick_params(left=False, right=False, labelleft=False,
                labelbottom=False, bottom=False)
```

```{python} 
#| echo: true
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'La lluvia genera un caos en la ciudad'
vec1 = tm[txt1]
vec2 = tm[txt2]
f = {k: v for k, v in vec1}
np.sum([f[k] * v for k, v in vec2 if k in f])
```


```{python} 
#| echo: true
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'Estoy dando una platica en Morelia.'
vec1 = tm[txt1]
vec2 = tm[txt2]
f = {k: v for k, v in vec1}
np.sum([f[k] * v for k, v in vec2 if k in f])
```


```{python}
#| echo: true
fname = 'delitos/delitos_ingeotec_Es_train.json'
D = list(tweet_iterator(fname))
```

```{python}
#| echo: true
X = tm.transform(D)
dis = np.dot(X, X.T)
```

```{python}
#| code-fold: true
#| fig-cap: Histograma de la similitud
#| label: fig-text-repr-similitud-bow
sns.displot(dis.data)
```

## Bolsa de Palabras Densa 

```{mermaid}
%%| echo: false
%%| fig-cap: Diagrama Bolsa de Palabras Densa
%%| label: fig-repr-texto-bolsa-densa

flowchart LR
    Terminos([Texto\n Segmentado]) -- Pre-entrenados -->  A[Asociación]
    A --> Repr([Representación])
```



$$
\mathbf u = \sum_t \mathbf{u_t} + \mathbf{w_0},
$$ {#eq-bolsa-densas}

donde $\mathbf{w_0} \in \mathbb R^{M}$,  

$$
\mathbf{u_t} = \frac{\mathbf W \mathbf {v_t}}{\lVert \sum_t \mathbf{v_t} \rVert}
$$

y $\mathbf W \in \mathbb R^{M \times d}$

## Ejemplos 

```{python} 
#| echo: true
dense = DenseBoW(lang='es',
                 voc_size_exponent=15,
                 emoji=False, keyword=True,
                 distance_hyperplane=True,
                 dataset=False)
txt1 = 'Es un placer estar platicando con ustedes.'
txt2 = 'La lluvia genera un caos en la ciudad.'
txt3 = 'Estoy dando una platica en Morelia.'
X = dense.transform([txt1, txt2, txt3])
np.dot(X[0], X[1]), np.dot(X[0], X[2])
```


La @fig-repr-texto-nube-densa muestra la nube de palabras generada con las características y sus respectivos valores del texto *Es un placer estar platicando con ustedes.* y *La lluvia genera un caos en la ciudad.*

```{python}
#| code-fold: true
#| fig-cap: Nube de características positivas y negativas
#| label: fig-repr-texto-nube-densa
values = dense.transform([txt1, txt2])
names = dense.names
tokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v > 0}
tokens_neg = {names[id]: v for id, v in enumerate(values[1]) if v > 0}

word_pos = WordCloud().generate_from_frequencies(tokens_pos)
word_neg = WordCloud().generate_from_frequencies(tokens_neg)

fig, (ax1, ax2) = plt.subplots(1, 2)

for cloud, ax, title in zip([word_pos, word_neg],
                     [ax1, ax2],
                     ['Es un ... ustedes.', 
                      'La lluvia ... ciudad.']):
    ax.imshow(cloud, interpolation='bilinear')
    ax.grid(False)
    ax.tick_params(left=False, right=False, labelleft=False,
                   labelbottom=False, bottom=False)
    ax.set_title(title)
```

```{python}
#| echo: true
X = dense.transform(D)
unit = np.linalg.norm(X, axis=1)
X = X / np.atleast_2d(unit).T
dis = np.dot(X, X.T)
```

```{python}
#| code-fold: true
#| fig-cap: Histograma de la similitud usando bolsa de palabras densas
#| label: fig-text-repr-similitud-dense
sns.displot(dis.flatten())
```