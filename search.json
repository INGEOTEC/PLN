[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Procesamiento de Lenguaje Natural",
    "section": "",
    "text": "Prefacio\nEl curso trata de ser auto-contenido, es decir, no debería de ser necesario leer otras fuentes para poder entenderlo y realizar las actividades. De cualquier manera es importante comentar que el curso está basado en los siguientes libros de texto:\nLa Tabla 1.1 muestra la notación que se seguirá en este documento.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Procesamiento de Lenguaje Natural",
    "section": "Licencia",
    "text": "Licencia\n\nEsta obra está bajo una Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "capitulos/01Introduccion.html",
    "href": "capitulos/01Introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introducción</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "href": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "title": "2  Manejando Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.params import OPTION_GROUP, OPTION_DELETE,\\\n                           OPTION_NONE\nfrom microtc.textmodel import SKIP_SYMBOLS\nfrom b4msa.textmodel import TextModel\nfrom b4msa.lang_dependency import LangDependency\nfrom nltk.stem.snowball import SnowballStemmer\nimport unicodedata\nimport re\n\n\nVideo explicando la unidad",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#introducción",
    "href": "capitulos/02ManejandoTexto.html#introducción",
    "title": "2  Manejando Texto",
    "section": "2.1 Introducción",
    "text": "2.1 Introducción\nSe podría suponer que el texto se que se analizará está bien escrito y tiene un formato adecuado para su procesamiento. Desafortunadamente, la realidad es que en la mayoría de aplicaciones el texto que se analiza tiene errores de ortográficos, errores de formato y además no es trivial identificar la unidad mínima de procesamiento que podría ser de manera natural, en el español, las palabras. Por este motivo, esta unidad trata técnicas comunes que se utilizan para normalizar el texto, esta normalización es un proceso previo al desarrollo de los algoritmos de PLN.\nLa Figura 2.1 esquematiza el procedimiento que se presenta en esta unidad, la idea es que se un texto pasa primeramente a un proceso de normalización (Sección 2.2 y Sección 2.3), para después ser segmentado (ver Sección 2.4) y el resultado es lo que se utiliza para modelar el lenguaje.\n\n\n\n\n\n\nflowchart LR\n    Entrada([Texto]) --&gt;  Norm[Normalización de Texto]\n    Norm --&gt; Seg[Segmentación]\n    Seg --&gt; Terminos(...)\n\n\n\n\nFigura 2.1: Diagrama de Pre-procesamiento\n\n\n\n\n\nLas normalizaciones y segmentaciones descritas en esta unidad se basan principalmente en las utilizadas en los siguientes artículos científicos.\n\nAn automated text categorization framework based on hyperparameter optimization (Tellez et al. (2018))\nA simple approach to multilingual polarity classification in Twitter (Tellez, Miranda-Jiménez, Graff, Moctezuma, Suárez, et al. (2017))\nA case study of Spanish text transformations for twitter sentiment analysis (Tellez, Miranda-Jiménez, Graff, Moctezuma, Siordia, et al. (2017))",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-normalizacion",
    "href": "capitulos/02ManejandoTexto.html#sec-normalizacion",
    "title": "2  Manejando Texto",
    "section": "2.2 Normalización de Texto Sintáctica",
    "text": "2.2 Normalización de Texto Sintáctica\nLa descripción de las normalizaciones empieza presentando las que se puede aplicar a nivel de caracteres, sin la necesidad de conocer el significado de las palabras. También se agrupan en este conjunto aquellas transformaciones que se realizan mediante expresiones regulares o su búsqueda en una lista de palabras previamente definidas.\n\n2.2.1 Entidades\nLa descripción de diferentes técnicas de normalización empieza con el manejo de entidades en el texto. Algunas entidades que se tratarán serán los nombres de usuario, números o URLs mencionados en un texto. Por otro lado están las acciones que se realizarán a las entidades encontradas, estas acciones corresponden a su borrado o remplazo por algún otro toquen.\n\n2.2.1.1 Usuarios\nEn esta sección se trabajará con los nombres de usuarios que siguen el formato usado por Twitter. En un tuit, los nombres de usuarios son aquellas palabras que inician con el caracter @ y terminan con un espacio o caracter terminal. Las acciones que se realizarán con los nombres de usuario encontrados serán su borrado o reemplazo por una etiqueta en particular.\nEl procedimiento para encontrar los nombres de usuarios es mediante expresiones regulares, en particular se usa la expresión @\\S+, tal y como se muestra en el siguiente ejemplo.\n\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"\", text)\n\n'Hola   te está buscando'\n\n\nLa segunda acción es reemplazar cada nombre de usuario por una etiqueta particular, en el siguiente ejemplo se reemplaza por la etiqueta _usr.\n\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"_usr\", text)\n\n'Hola _usr _usr te está buscando'\n\n\n\n\n2.2.1.2 URL\nLos ejemplos anteriores se pueden adaptar para manejar URL; solamente es necesario adecuar la expresión regular que identifica una URL. En el siguiente ejemplo se muestra como se pueden borrar las URLs que aparecen en un texto.\n\ntext = \"puedes verificar que http://google.com esté funcionando\"\nre.sub(r\"https?://\\S+\", \"\", text)\n\n'puedes verificar que  esté funcionando'\n\n\n\n\n2.2.1.3 Números\nThe previous code can be modified to deal with numbers and replace the number found with a shared label such as _num.\n\ntext = \"acabamos de ganar 10 M\"\nre.sub(r\"\\d\\d*\\.?\\d*|\\d*\\.\\d\\d*\", \"_num\", text)\n\n'acabamos de ganar _num M'\n\n\n\n\n\n2.2.2 Ortografía\nEl siguiente bloque de normalizaciones agrupa aquellas modificaciones que se realizan a algún componente de tal manera que aunque impacta en su ortografía puede ser utilizado para reducir la dimensión y se ve reflejado en la complejidad del algoritmo.\n\n2.2.2.1 Mayúsculas y Minúsculas\nLa primera de estas transformaciones es convertir todas los caracteres a minúsculas. Como se puede observar esta transformación hace que el vocabulario se reduzca, por ejemplo, las palabras México o MÉXICO son representados por la palabra méxico. Esta operación se puede realizar con la función lower tal y cómo se muestra a continuación.\n\ntext = \"México\"\ntext.lower()\n\n'méxico'\n\n\n\n\n2.2.2.2 Signos de Puntuación\nLos signos de puntuación son necesarios para tareas como la generación de textos, pero existen otras aplicaciones donde los signos de puntuación tienen un efecto positivo en el rendimiento del algorithm, este es el caso de tareas de categorización de texto. El efecto que tiene el quitar los signos de puntuación es que el vocabulario se reduce. Los símbolos de puntuación se pueden remover teniendo una lista de los mismos, esta lista de signos de puntuación se encuentra en la variable SKIP_SYMBOLS y el siguiente código muestra un procedimiento para quitarlos.\n\ntext = \"¡Hola! buenos días:\"\noutput = \"\"\nfor x in text:\n    if x in SKIP_SYMBOLS:\n        continue\n    output += x\noutput\n\n'Hola buenos días'\n\n\n\n\n2.2.2.3 Símbolos Diacríticos\nContinuando con la misma idea de reducir el vocabulario, es común eliminar los símbolos diacríticos en las palabras. Esta transformación también tiene el objetivo de normalizar aquellos textos informales donde los símbolos diacríticos son usado con una menor frecuencia, en particular los acentos en el caso del español. Por ejemplo, es común encontrar la palabra México escrita como Mexico.\nEl siguiente código muestra un procedimiento para eliminar los símbolos diacríticos.\n\ntext = 'México'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 &lt;= o and o &lt;= 0x036F:\n        continue\n    output += x\noutput\n\n'Mexico'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-normalizacion-semantica",
    "href": "capitulos/02ManejandoTexto.html#sec-normalizacion-semantica",
    "title": "2  Manejando Texto",
    "section": "2.3 Normalización Semántica",
    "text": "2.3 Normalización Semántica\nLas siguientes normalizaciones comparten el objetivo con las normalizaciones presentadas hasta este momento, el cual es la reducción del vocabulario; la diferencia es que las siguientes utilizan el significado o uso de la palabra.\n\n2.3.1 Palabras Comunes\nLas palabras comunes (stop words) son palabras utilizadas frecuentemente en el lenguaje, las cuales son necesarias para comunicación, pero no aportan información para discriminar un texto de acuerdo a su significado.\nThe stop words are the most frequent words used in the language. These words are essential to communicate but are not so much on tasks where the aim is to discriminate texts according to their meaning.\nLas palabras vacías se pueden guardar en un diccionario y el proceso de identificación consiste en buscar la existencia de la palabra en el diccionario. Una vez que la palabra analizada se encuentra en el diccionario, se procede a quitarla o cambiarla por un token particular. El proceso de borrado se muestra en el siguiente código.\n\nlang = LangDependency('spanish')\n\ntext = '¡Buenos días! El día de hoy tendremos un día cálido.'\noutput = []\nfor word in text.split():\n    if word.lower() in lang.stopwords[len(word)]:\n        continue\n    output.append(word)\noutput = \" \".join(output) \noutput\n\n'¡Buenos días! día hoy día cálido.'\n\n\n\n\n2.3.2 Lematización y Reducción a la Raíz\nLa idea de lematización y reducción a la raíz (stemming) es transformar una palabra a su raíz mediante un proceso heurístico o morfológico. Por ejemplo, las palabras jugando o jugaron se transforman a la palabra jugar.\nEl siguiente código muestra el proceso de reducción a la raíz utilizando la clase SnowballStemmer.\n\nstemmer = SnowballStemmer('spanish')\n\ntext = 'Estoy jugando futbol con mis amigos'\noutput = []\nfor word in text.split():\n    w = stemmer.stem(word)\n    output.append(w)\noutput = \" \".join(output) \noutput\n\n'estoy jug futbol con mis amig'",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-segmentacion",
    "href": "capitulos/02ManejandoTexto.html#sec-segmentacion",
    "title": "2  Manejando Texto",
    "section": "2.4 Segmentación",
    "text": "2.4 Segmentación\nUna vez que el texto ha sido normalizado es necesario segmentarlo (tokenize) a sus componentes fundamentales, e.g., palabras o gramas de caracteres (q-grams) o de palabras (n-grams). Existen diferentes métodos para segmentar un texto, probablemente una de las más sencillas es asumir que una palabra está limitada entre dos espacios o signos de puntuación. Partiendo de las palabras encontradas se empiezan a generar los gramas de palabras, e.g., bigramas, o los gramas de caracteres si se desea solo generarlos a partir de las palabras.\n\n2.4.1 Gramas de Palabras (n-grams)\nEl primer método de segmentación revisado es la creación de los gramas de palabras. El primer paso es encontrar las palabras las cuales se pueden encontrar mediante la función split; una vez que las palabras están definidas estás se pueden unir para generar los gramas de palabras del tamaño deseado, tal y como se muestra en el siguiente código.\n\ntext = 'Estoy jugando futbol con mis amigos'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n\n['Estoy~jugando~futbol',\n 'jugando~futbol~con',\n 'futbol~con~mis',\n 'con~mis~amigos']\n\n\n\n\n2.4.2 Gramas de Caracteres (q-grams)\nLa segmentación de gramas de caracteres complementa los gramas de palabras. Los gramas de caracteres están definidos como la subcadena de longitud \\(q\\). Este tipo de segmentación tiene la característica de que es agnóstica al lenguaje, es decir, se puede aplicar en cualquier idioma; contrastando, los gramas de palabras se pueden aplicar solo a los lenguajes que tienen definido el concepto de palabra, por ejemplo en el idioma chino las palabras no se pueden identificar como se pueden identificar en el español o inglés. La segunda característica importante es que ayuda en el problema de errores ortográficos, siguiendo una perspectiva de similitud aproximada.\nEl código para realizar los gramas de caracteres es similar a la presentada anteriormente, siendo la diferencia que el ciclo está por los caracteres en lugar de la palabras como se había realizado. El siguiente código muestra una implementación para realizar gramas de caracteres.\n\ntext = 'Estoy jugando'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n\n['Esto',\n 'stoy',\n 'toy ',\n 'oy j',\n 'y ju',\n ' jug',\n 'juga',\n 'ugan',\n 'gand',\n 'ando']",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#textmodel",
    "href": "capitulos/02ManejandoTexto.html#textmodel",
    "title": "2  Manejando Texto",
    "section": "2.5 TextModel",
    "text": "2.5 TextModel\nHabiendo descrito diferentes tipos de normalización (sintáctica y semántica) y el proceso de segmentación es momento para describir la librería B4MSA (Tellez, Miranda-Jiménez, Graff, Moctezuma, Suárez, et al. (2017)) que implementa estos procedimientos; específicamente, el punto de acceso de estos procedimientos corresponde a la clase TextModel. El método TextModel.text_transformations es el que realiza todos los métodos de normalización (Sección 2.2 y Sección 2.3) y el método TextModel.tokenize es el encargado de realizar la segmentación (Sección 2.4) siguiendo el flujo mostrado en la Figura 2.1.\n\n2.5.1 Normalizaciones\nEl primer conjunto de parámetros que se describen son los que corresponden a las entidades (Sección 2.2.1). Estos parámetros tiene tres opciones, borrar (OPTION_DELETE), remplazar (OPTION_GROUP) o ignorar. Los nombres de los parámetros son:\n\nusr_option\nurl_option\nnum_option\n\nque corresponden al procesamiento de usuarios, URL y números respectivamente. Adicionalmente, TextModel trata los emojis, hashtags y nombres, mediante los siguientes parámetros:\n\nemo_option\nhashtag_option\nent_option\n\nPor ejemplo, el siguiente código muestra como se borra el usuario y se reemplaza un hashtag; se puede observar que en la respuesta se cambian todos los espacios por el caracter ~ y se incluye ese mismo al inicio y final del texto.\n\ntm = TextModel(hashtag_option=OPTION_GROUP,\n               usr_option=OPTION_DELETE)\ntexto = 'mira @xyz estoy triste. #UnDiaLluvioso'\ntm.text_transformations(texto)\n\n'~mira~estoy~triste.~_htag~'\n\n\nSiguiendo con las transformaciones sintácticas, toca el tiempo a describir aquellas que relacionadas a la ortografía (Sección 2.2.2) las cuales corresponden a la conversión a minúsculas, borrado de signos de puntuación y símbolos diacríticos. Estas normalizaciones se activan con los siguiente parámetros.\n\nlc\ndel_punc\ndel_diac\n\nEn el siguiente ejemplo se transforman el texto a minúscula y se remueven los signos de puntuación.\n\ntm = TextModel(lc=True,\n               del_punc=True,\n               del_diac=False)\ntexto = 'Hoy está despejado.'\ntm.text_transformations(texto)\n\n'~hoy~está~despejado~'\n\n\nLas normalizaciones semánticas (Sección 2.3) que se tienen implementadas en la librería corresponden al borrado de palabras comunes y reducción a la raíz; estás se pueden activar con los siguientes parámetros.\n\nstopwords\nstemming\n\nPor ejemplo, las siguientes instrucciones quitan las palabras comunes y realizan una reducción a la raíz.\n\ntm = TextModel(lang='es',\n               stopwords=OPTION_DELETE,\n               stemming=True)\ntexto = 'el clima es perfecto'\ntm.text_transformations(texto)\n\n'~clim~perfect~'\n\n\n\n\n2.5.2 Segmentación\nEl paso final es describir el uso de la segmentación. La librería utiliza el parámetro token_list para indicar el tipo de segmentación que se desea realizar. El formato es una lista de número, donde el valor indica el tipo de segmentación. El número \\(1\\) indica que se realizará una segmentación por palabras, los número positivo corresponden a los gramas de caracteres y los números negativos a los gramas de palabras.\nPor ejemplo, utilizando las normalizaciones que se tienen por defecto, el siguiente código segmenta utilizando gramas de caracteres de tamañan \\(4.\\)\n\ntm = TextModel(token_list=[4])\ntm.tokenize('buenos días')\n\n['q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n\n\npara poder identificar cuando se trata de un segmento que corresponde a una palabra o un grama de caracteres, a los últimos se les agrega el prefijo q:. Cabe mencionar que por defecto se remueven los símbolos diacríticos.\nEl ejemplo anterior, se utiliza para generar un grama de palabras de tamaño \\(2.\\) Como se ha mencionado los gramas de palabras se especifican con números negativos siendo el valor absoluto el tamaño del grama.\n\ntm = TextModel(token_list=[-2])\ntm.tokenize('buenos días')\n\n['buenos~dias']\n\n\nPara completar la explicación, se combinan la segmentación de gramas de caracteres y palabras además de incluir las palabras en la segmentación.\n\ntm = TextModel(token_list=[4, -2, -1])\ntm.tokenize('buenos días')\n\n['buenos~dias',\n 'buenos',\n 'dias',\n 'q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n\n\n\n\n\n\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Oscar S. Siordia, y Elio A. Villaseñor. 2017. «A case study of Spanish text transformations for twitter sentiment analysis». Expert Systems with Applications 81: 457-71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Ranyart R. Suárez, y Oscar S. Siordia. 2017. «A Simple Approach to Multilingual Polarity Classification in Twitter». Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, y Mario Graff. 2018. «An automated text categorization framework based on hyperparameter optimization». Knowledge-Based Systems 149: 110-23. https://doi.org/10.1016/j.knosys.2018.03.003.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Manejando Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/03ModeladoLenguaje.html",
    "href": "capitulos/03ModeladoLenguaje.html",
    "title": "3  Modelado de Lenguaje",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Modelado de Lenguaje</span>"
    ]
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "href": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.utils import tweet_iterator, load_model, save_model\nfrom b4msa.textmodel import TextModel\nfrom EvoMSA.tests.test_base import TWEETS\nfrom EvoMSA.utils import bootstrap_confidence_interval\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.stats import norm, multinomial, multivariate_normal\nfrom scipy.special import logsumexp\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nfrom os.path import join\nimport numpy as np\n\n\nVideo explicando la unidad",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentos de Clasificación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#introducción",
    "href": "capitulos/04ClasificacionTexto.html#introducción",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nText Categorization is an NLP task that deals with creating algorithms capable of identifying the category of a text from a set of predefined categories. For example, sentiment analysis belongs to this task, and the aim is to detect the polarity (e.g., positive, neutral, or negative) of a text. Furthermore, different NLP tasks that initially seem unrelated to this problem can be formulated as a classification one such as question answering and sentence entailment, to mention a few.\nText Categorization can be tackled from different perspectives; the one followed here is to treat it as a supervised learning problem. As in any supervised learning problem, the starting point is a set of pairs, where the first element of the pair is the input and the second one corresponds to the output. Let \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentos de Clasificación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "href": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.2 Modelado Probabilistico (Distribución Categórica)",
    "text": "4.2 Modelado Probabilistico (Distribución Categórica)\n\n4.2.1 Problema Sintético\nThe description of Bayes’ theorem continues with an example of a Categorical distribution. A Categorical distribution can simulate the drawn of \\(K\\) events that can be encoded as characters, and \\(\\ell\\) repetitions can be represented as a sequence of characters. Consequently, the distribution can illustrate the generation sequences associated with different classes, e.g., positive or negative.\nThe first step is to create the dataset. As done previously, two distributions are defined, one for each class; it can be observed that each distribution has different parameters. The second step is to sample these distributions; the distributions are sampled 1000 times with the following procedure. Each time, a random variable representing the number of outcomes taken from each distribution is drawn from a Normal \\(\\mathcal N(15, 3)\\) and stored in the variable length. The random variable indicates the number of outcomes for each Categorical distribution; the results are transformed into a sequence, associated to the label corresponding to the positive and negative class, and stored in the list D.\npos = multinomial(1, [0.20, 0.20, 0.35, 0.25])\nneg = multinomial(1, [0.35, 0.20, 0.25, 0.20])\nlength = norm(loc=15, scale=3)\nD = []\nm = {k: chr(122 - k) for k in range(4)}\nid2w = lambda x: \" \".join([m[_] for _ in x.argmax(axis=1)])\nfor l in length.rvs(size=1000):\n    D.append((id2w(pos.rvs(round(l))), 1))\n    D.append((id2w(neg.rvs(round(l))), 0))\nThe following table shows four examples of this process; the first column contains the sequence, and the second the associated label.\n\n\n\nText\nLabel\n\n\n\n\nx w x x z w y\npositive\n\n\ny w z z z x w\nnegative\n\n\nz x x x z x z w x w\npositive\n\n\nx w z w y z z z z w\nnegative\n\n\n\nAs done previously, the first step is to compute the likelihood given that dataset; considering that the data comes from a Categorical distribution, the procedure to estimate the parameters is similar to the ones used to estimate the prior. The following code estimates the data parameters corresponding to the positive class. It can be observed that the parameters estimated are similar to the ones used to generate the dataset.\nD_pos = []\n[D_pos.extend(data.split()) for data, k in D if k == 1]\nwords, l_pos = np.unique(D_pos, return_counts=True)\nw2id = {v: k for k, v in enumerate(words)}\nl_pos = l_pos / l_pos.sum()\nl_pos\narray([0.25489421, 0.33854064, 0.20773186, 0.1988333 ])\nAn equivalent procedure is performed to calculate the likelihood of the negative class.\nD_neg = []\n[D_neg.extend(data.split()) for data, k in D if k == 0]\n_, l_neg = np.unique(D_neg, return_counts=True)\nl_neg = l_neg / l_neg.sum()\nThe prior is estimated with the following code, equivalent to the one used on all the examples seen so far.\n_, priors = np.unique([k for _, k in D], return_counts=True)\nN = priors.sum()\nprior_pos = priors[1] / N\nprior_neg = priors[0] / N\nOnce the parameters have been identified, these can be used to predict the class of a given sequence. The first step is to compute the likelihood, e.g., \\(\\mathbb P(\\)w w x z\\(\\mid \\mathcal Y)\\). It can be observed that the sequence needs to be transformed into tokens which can be done with the split method. Then, the token is converted into an index using the mapping w2id; once the index is retrieved, it can be used to obtain the parameter associated with the word. The likelihood is the product of all the probabilities; however, this product is computed in log space.\ndef likelihood(params, txt):\n    params = np.log(params)\n    _ = [params[w2id[x]] for x in txt.split()]\n    tot = sum(_)\n    return np.exp(tot)\nThe likelihood combined with the prior for all the classes produces the evidence, which subsequently is used to calculate the posterior distribution. The posterior is then used to predict the class for all the sequences in \\(\\mathcal D\\). The predictions are stored in the variable hy.\npost_pos = [likelihood(l_pos, x) * prior_pos for x, _ in D]\npost_neg = [likelihood(l_neg, x) * prior_neg for x, _ in D]\nevidence = np.vstack([post_pos, post_neg]).sum(axis=0)\npost_pos /= evidence\npost_neg /= evidence\nhy = np.where(post_pos &gt; post_neg, 1, 0)\n\n\n4.2.2 Clasificador de Texto\nThe approach followed on text categorization is to treat it as supervised learning problem where the starting point is a dataset \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text. For example, the next code uses a toy sentiment analysis dataset with four classes: negative (N), neutral (NEU), absence of polarity (NONE), and positive (P).\nD = [(x['text'], x['klass']) for x in tweet_iterator(TWEETS)]\nAs can be observed, \\(\\mathcal D\\) is equivalent to the one used in the Categorical Distribution example. The difference is that sequence of letters is changed with a sentence. Nonetheless, a feasible approach is to obtain the tokens using the split method. Another approach is to retrieve the tokens using a Tokenizer, as covered in the Text Normalization Section.\nThe following code uses the TextModel class to tokenize the text using words as the tokenizer; the tokenized text is stored in the variable D.\ntm = TextModel(token_list=[-1])\ntok = tm.tokenize\nD = [(tok(x), y) for x, y in D]\nBefore estimating the likelihood parameters, it is needed to encode the tokens using an index; by doing it, it is possible to store the parameters in an array and compute everything numpy operations. The following code encodes each token with a unique index; the mapping is in the dictionary w2id.\nwords = set()\n[words.update(x) for x, y in D]\nw2id = {v: k for k, v in enumerate(words)}\nPreviously, the classes have been represented using natural numbers. The positive class has been associated with the number \\(1\\), whereas the negative class with \\(0\\). However, in this dataset, the classes are strings. It was decided to encode them as numbers to facilitate subsequent operations. The encoding process can be performed simultaneously with the estimation of the prior of each class. Please note that the priors are stored using the logarithm in the variable priors.\nuniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\npriors = np.log(priors / priors.sum())\nuniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\nIt is time to estimate the likelihood parameters for each of the classes. It is assumed that the data comes from a Categorical distribution and that each token is independent. The likelihood parameters can be stored in a matrix (variable l_tokens) with \\(K\\) rows, each row contains the parameters of the class, and the number of columns corresponds to the vocabulary’s size. The first step is to calculate the frequency of each token per class which can be done with the following code.\nl_tokens = np.zeros((len(uniq_labels), len(w2id)))\nfor x, y in D:\n    w = l_tokens[uniq_labels[y]]\n    cnt = Counter(x)\n    for i, v in cnt.items():\n        w[w2id[i]] += v\nThe next step is to normalize the frequency. However, before normalizing it, it is being used a Laplace smoothing with a value \\(0.1\\). Therefore, the constant \\(0.1\\) is added to all the matrix elements. The next step is to normalize (second line), and finally, the parameters are stored using the logarithm.\nl_tokens += 0.1\nl_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\nl_tokens = np.log(l_tokens)\n\n4.2.2.1 Prediction\nOnce all the parameters have been estimated, it is time to use the model to classify any text. The following function computes the posterior distribution. The first step is to tokenize the text (second line) and compute the frequency of each token in the text. The frequency stored in the dictionary cnt is converted into the vector x using the mapping function w2id. The final step is to compute the product of the likelihood and the prior. The product is computed in log-space; thus, this is done using the likelihood and the prior sum. The last step is to compute the evidence and normalize the result; the evidence is computed with the function logsumexp.\ndef posterior(txt):\n    x = np.zeros(len(w2id))\n    cnt = Counter(tm.tokenize(txt))\n    for i, v in cnt.items():\n        try:\n            x[w2id[i]] += v\n        except KeyError:\n            continue\n    _ = (x * l_tokens).sum(axis=1) + priors\n    l = np.exp(_ - logsumexp(_))\n    return l\nThe posterior function can predict all the text in \\(\\mathcal D\\); the predictions are used to compute the model’s accuracy. In order to compute the accuracy, the classes in \\(\\mathcal D\\) need to be transformed using the nomenclature of the likelihood matrix and priors vector; this is done with the uniq_labels dictionary (second line).\nhy = np.array([posterior(x).argmax() for x, _ in D])\ny = np.array([uniq_labels[y] for _, y in D])\n(y == hy).mean()\n0.974\n\n\n4.2.2.2 Training\nSolving supervised learning problems requires two phases; one is the training phase, and the other is the prediction. The posterior function handles the later phase, and it is missing to organize the code described in a training function. The following code describes the training function; it requires the dataset’s parameters and an instance of TextModel.\ndef training(D, tm):\n    tok = tm.tokenize\n    D =[(tok(x), y) for x, y in D]\n    words = set()\n    [words.update(x) for x, y in D]\n    w2id = {v: k for k, v in enumerate(words)}\n    uniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\n    priors = np.log(priors / priors.sum())\n    uniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n    l_tokens = np.zeros((len(uniq_labels), len(w2id)))\n    for x, y in D:\n        w = l_tokens[uniq_labels[y]]\n        cnt = Counter(x)\n        for i, v in cnt.items():\n            w[w2id[i]] += v\n    l_tokens += 0.1\n    l_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\n    l_tokens = np.log(l_tokens)\n    return w2id, uniq_labels, l_tokens, priors",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentos de Clasificación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "href": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.3 Modelado Vectorial",
    "text": "4.3 Modelado Vectorial\nxxx",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Fundamentos de Clasificación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html#paquetes-usados",
    "href": "capitulos/05RepresentacionTexto.html#paquetes-usados",
    "title": "5  Representación de Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom EvoMSA import BoW,\\\n                   DenseBoW\nfrom microtc.utils import tweet_iterator\nfrom wordcloud import WordCloud                            \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n\n\nVideo explicando la unidad",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Representación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html#introducción",
    "href": "capitulos/05RepresentacionTexto.html#introducción",
    "title": "5  Representación de Texto",
    "section": "5.1 Introducción",
    "text": "5.1 Introducción",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Representación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html#bolsa-de-palabras-dispersa",
    "href": "capitulos/05RepresentacionTexto.html#bolsa-de-palabras-dispersa",
    "title": "5  Representación de Texto",
    "section": "5.2 Bolsa de Palabras Dispersa",
    "text": "5.2 Bolsa de Palabras Dispersa\nLa idea de una bolsa de palabras discretas es que después de haber normalizado y segmentado el texto (Capítulo 2), cada token \\(t\\) sea asociado a un vector único \\(\\mathbf{v_t} \\in \\mathbb R^d\\) donde la \\(i\\)-ésima componente, i.e., \\(\\mathbf{v_t}_i\\), es diferente de cero y \\(\\forall_{j \\neq i} \\mathbf{v_t}_j=0\\). Es decir la \\(i\\)-ésima componente está asociada al token \\(t\\), se podría pensar que si el vocabulario está ordenado de alguna manera, entonces el token \\(t\\) está en la posición \\(i\\). Por otro lado el valor que contiene la componente se usa para representar alguna característica del token.\nEl conjunto de vectores \\(\\mathbf v\\) corresponde al vocabulario, teniendo \\(d\\) diferentes token en el mismo y por definición \\(\\forall_{i \\neq j} \\mathbf{v_i} \\cdot \\mathbf{v_j} = 0\\), donde \\(\\mathbf{v_i} \\in \\mathbb R^d\\), \\(\\mathbf{v_j} \\in \\mathbb R^d\\), y \\((\\cdot)\\) es el producto punto. Cabe mencionar que cualquier token fuera del vocabulario es descartado.\nUsando esta notación, un texto \\(x\\) está representado por una secuencia de términos, i.e., \\((t_1, t_2, \\ldots)\\); la secuencia puede tener repeticiones es decir, \\(t_j = t_k\\). Utilizando la característica de que cada token está asociado a un vector \\(\\mathbf v\\), se transforma la secuencia de términos a una secuencia de vectores (manteniendo las repeticiones), i.e., \\((\\mathbf{v_{t_1}}, \\mathbf{v_{t_2}}, \\ldots)\\). Finalmente, el texto \\(x\\) se representa como:\n\\[\n\\mathbf x = \\frac{\\sum_t \\mathbf{v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n\\tag{5.1}\\]\ndonde la suma se hace para todos los elementos de la secuencia, \\(\\mathbf x \\in \\mathbb R^d\\), y \\(\\lVert \\mathbf w \\rVert\\) es la norma Euclideana del vector \\(\\mathbf w.\\)\n\n\n\n\n\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados --&gt;  A[Asociación]\n    Terminos --&gt; Entrenamiento[Estimación\\n de Pesos]\n    Corpus([Corpus]) -.-&gt; Entrenamiento\n    Entrenamiento --&gt; A\n    A --&gt; Repr([Representación])\n\n\n\n\nFigura 5.1: Diagrama Bolsa de Palabras Dispersa\n\n\n\n\n\nAntes de iniciar la descripción detallada del proceso de representación utilizando una bolsa de palabras dispersas, es conveniente ilustrar este proceso mediante la Figura 5.1. El texto segmentado es el resultado del proceso ilustrado en Figura 2.1. El texto segmentado puede seguir dos caminos, en la parte superior se encuentra el caso cuando los pesos han sido identificados previamente y en la parte inferior es el procedimiento cuando los pesos se estiman mediante un corpus específico que normalmente es un conjunto de entrenamiento.\n\n5.2.1 Pesado de Términos\nComo se había mencionado el valor que tiene la componente \\(i\\)-ésima del vector \\(\\mathbf{v_t}_i\\) corresponde a una característica del término asociado, este procedimiento se le conoce como el esquema de pesado. Por ejemplo, si el valor es \\(1\\) (i.e., \\(\\mathbf{v_{t_i}} = 1\\)) entonces el valor está indicando solo la presencia del término, este es el caso más simple. Considerando la Ecuación 5.1 se observa que el resultado, \\(\\mathbf x\\), cuenta las repeticiones de cada término, por esta característica a este esquema se le conoce como frecuencia de términos (term frequency (TF)).\nUna medida que complementa la información que tiene la frecuencia de términos es el inverso de la frecuencia del término (Inverse Document Frequency (IDF)) en la colección, esta medida propuesta por Sparck Jones (1972) se usa en un método de pesado descrito por Salton y Yang (1973) el cual es conocido como TFIDF. Este método de pesado propone el considerar el producto de la frecuencia del término y el inverso de la frecuencia del término (Inverse Document Frequency (IDF) ) en la colección como el peso del término.\n\n\n5.2.2 Ejemplos\n\ntm = BoW(lang='es').bow\nvec = tm['Buen día']\nvec[:3]\n\n[(11219, 0.3984336285263178),\n (11018, 0.3245843730253675),\n (24409, 0.2377856890280623)]\n\n\n\n[(tm.id2token[k], v)\n for k, v in vec[:3]]\n\n[('buen~dia', 0.3984336285263178),\n ('buen', 0.3245843730253675),\n ('dia', 0.2377856890280623)]\n\n\n\ntxt = 'Buen día colegas'\n[(tm.id2token[k], v)\n for k, v in tm[txt][:4]]\n\n[('buen~dia', 0.24862785236357487),\n ('buen', 0.20254494048246244),\n ('dia', 0.1483814139998851),\n ('colegas', 0.3538047214393573)]\n\n\nSe puede observar como los valores de IDF de los términos comunes cambiaron, por ejemplo para el caso de buen~dia cambio de \\(0.3984\\) a \\(0.2486\\). Este es el resultado de que los valores están normalizados tal como se muestra en la Ecuación 5.1.\nLa Figura 5.2 muestra la nube de palabras generada con los términos y sus respectivos valores IDF del texto Es un placer estar platicando con ustedes.\n\n\nCódigo\ntxt = 'Es un placer estar platicando con ustedes.'\ntokens = {tm.id2token[id]: v for id, v in tm[txt]}\nword_cloud = WordCloud().generate_from_frequencies(tokens)\nplt.imshow(word_cloud, interpolation='bilinear')\nplt.grid(False)\nplt.tick_params(left=False, right=False, labelleft=False,\n                labelbottom=False, bottom=False)\n\n\n\n\n\n\n\n\nFigura 5.2: Nube de términos\n\n\n\n\n\n\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n\n0.01645519294478695\n\n\n\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'Estoy dando una platica en Morelia.'\nvec1 = tm[txt1]\nvec2 = tm[txt2]\nf = {k: v for k, v in vec1}\nnp.sum([f[k] * v for k, v in vec2 if k in f])\n\n0.2035427118119315\n\n\n\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nD = list(tweet_iterator(fname))\n\n\nX = tm.transform(D)\ndis = np.dot(X, X.T)\n\n\n\nCódigo\nsns.displot(dis.data)\n\n\n\n\n\n\n\n\nFigura 5.3: Histograma de la similitud",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Representación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html#bolsa-de-palabras-densa",
    "href": "capitulos/05RepresentacionTexto.html#bolsa-de-palabras-densa",
    "title": "5  Representación de Texto",
    "section": "5.3 Bolsa de Palabras Densa",
    "text": "5.3 Bolsa de Palabras Densa\nLa Figura 5.4 muestra el procedimiento que se sigue para representar un texto en una bolsa de palabras dispersa. En primer lugar la bolsa de palabras densa considera que los vectores asociados a los términos se encuentra pre-entrenados y en general no es factible entrenarlos en el momento, esto por el tiempo que lleva estimar estos vectores.\n\n\n\n\n\n\nflowchart LR\n    Terminos([Texto\\n Segmentado]) -- Pre-entrenados --&gt;  A[Asociación]\n    A --&gt; Repr([Representación])\n\n\n\n\nFigura 5.4: Diagrama Bolsa de Palabras Densa\n\n\n\n\n\nEl texto se representa como el vector \\(\\mathbf u\\) que se calcula usando la Ecuación 5.2 donde se observa que es la suma de los vectores asociados a cada término más un coeficiente \\(\\mathbf{w_0}\\). En particular el coeficiente \\(\\mathbf{w_0} \\in \\mathbb R^{M}\\) no se encuentra en todas las representaciones densas, pero en la representación que se usará contiene este vector, \\(M\\) es la dimensión de la representación densa.\n\\[\n\\mathbf u = \\sum_t \\mathbf{u_t} + \\mathbf{w_0}.\n\\tag{5.2}\\]\nEn vector \\(\\mathbf {u_t}\\) está asociado al término \\(t\\), en particular este vector en la representación densa que se describirá está definido en términos de una bolsa de palabras dispersa (Ecuación 5.1) como se puede observar en la Ecuación 5.3\n\\[\n\\mathbf{u_t} = \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert},\n\\tag{5.3}\\]\ndonde \\(\\mathbf W \\in \\mathbb R^{M \\times d}\\) es la matriz que hace la proyección de la representación dispersa a la representación densa, se puede observar esa operación está normalizada con la norma Euclideana de la representación dispersa.\nCombinando las Ecuación 5.2 y Ecuación 5.3 queda la\n\\[\n\\begin{split}\n\\mathbf{u_t} &= \\sum_t \\frac{\\mathbf W \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0} \\\\\n&= \\mathbf W \\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert} + \\mathbf{w_0},\n\\end{split}\n\\]\ndonde se puede observar la representación dispersa (Ecuación 5.1), i.e., \\(\\frac{\\sum_t \\mathbf {v_t}}{\\lVert \\sum_t \\mathbf{v_t} \\rVert}\\) lo cual resulta en la Ecuación 5.4\n\\[\n\\mathbf u = \\mathbf W \\mathbf x + \\mathbf{w_0},\n\\tag{5.4}\\]\nque representa un texto en el vector \\(\\mathbf u \\in \\mathbb R^M.\\)\n\n5.3.1 Ejemplos\n\ndense = DenseBoW(lang='es',\n                 voc_size_exponent=15,\n                 emoji=False, keyword=True,\n                 distance_hyperplane=True,\n                 dataset=False)\ntxt1 = 'Es un placer estar platicando con ustedes.'\ntxt2 = 'La lluvia genera un caos en la ciudad.'\ntxt3 = 'Estoy dando una platica en Morelia.'\nX = dense.transform([txt1, txt2, txt3])\nnp.dot(X[0], X[1]), np.dot(X[0], X[2])\n\n(0.7728943423183761, 0.8721107462230386)\n\n\nLa Figura 5.5 muestra la nube de palabras generada con las características y sus respectivos valores del texto Es un placer estar platicando con ustedes. y La lluvia genera un caos en la ciudad.\n\n\nCódigo\nvalues = dense.transform([txt1, txt2])\nnames = dense.names\ntokens_pos = {names[id]: v for id, v in enumerate(values[0]) if v &gt; 0}\ntokens_neg = {names[id]: v for id, v in enumerate(values[1]) if v &gt; 0}\n\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_pos, word_neg],\n                     [ax1, ax2],\n                     ['Es un ... ustedes.', \n                      'La lluvia ... ciudad.']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 5.5: Nube de características positivas y negativas\n\n\n\n\n\n\nX = dense.transform(D)\nunit = np.linalg.norm(X, axis=1)\nX = X / np.atleast_2d(unit).T\ndis = np.dot(X, X.T)\n\n\n\nCódigo\nsns.displot(dis.flatten())\n\n\n\n\n\n\n\n\nFigura 5.6: Histograma de la similitud usando bolsa de palabras densas\n\n\n\n\n\n\n\n\n\n\n\nSalton, Gerard, y Chungshu S. Yang. 1973. «On the specification of term values in automatic indexing». Journal of Documentation 29 (abril): 351-72. https://doi.org/10.1108/EB026562.\n\n\nSparck Jones, Karen. 1972. «A statistical interpretation of term specificity and its application in retrieval». Journal of Documentation 28: 11-21. https://doi.org/10.1108/EB026526.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Representación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#paquetes-usados",
    "href": "capitulos/06MezclaModelos.html#paquetes-usados",
    "title": "6  Mezcla de Modelos",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom EvoMSA import BoW,\\\n                   DenseBoW,\\\n                   StackGeneralization\nfrom microtc.utils import tweet_iterator\nfrom IngeoML import CI, SelectFromModelCV\nfrom sklearn.metrics import f1_score,\\\n                            recall_score,\\\n                            precision_score\nfrom wordcloud import WordCloud                            \nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pylab as plt\nimport seaborn as sns\n\n\nVideo explicando la unidad",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#introducción",
    "href": "capitulos/06MezclaModelos.html#introducción",
    "title": "6  Mezcla de Modelos",
    "section": "6.1 Introducción",
    "text": "6.1 Introducción\n\n\n\n\n\n\nflowchart LR\n    Repr([Representación]) -- Clasificación --&gt; Clasificador[Clasificador]\n    Repr -- Regresión --&gt; Regresor[Regresor]\n    Clasificador --&gt; Prediccion([Predicción])\n    Regresor --&gt; Prediccion\n\n\n\n\nFigura 6.1: Diagrama de predicción\n\n\n\n\n\nEl conjunto de datos se puede conseguir en la página de Delitos aunque en esta dirección es necesario poblar los textos dado que solamente se encuentra el identificador del Tweet.\nPara leer los datos del conjunto de entrenamiento y prueba se utilizan las siguientes instrucciones. En la variable D se tiene los datos que se utilizarán para entrenar el clasificador basado en la bolsa de palabras y en Dtest los datos del conjunto de prueba, que son usados para medir el rendimiento del clasificador.\n\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nfname_test = 'delitos/delitos_ingeotec_Es_test.json'\nD = list(tweet_iterator(fname))\nDtest = list(tweet_iterator(fname_test))\n\nEn la siguiente instrucción se observa el primer elemento del conjunto de entrenamiento. Se puede observar que en el campo text se encuentra el texto, el campo klass representa la etiqueta o clase, donde \\(0\\) representa la clase negativa y \\(1\\) la clase positiva, es decir, la presencia de un delito. El campo id es el identificador del Tweet y annotations son las clases dadas por los etiquetadores a ese ejemplo.\n\nD[81]\n\n{'annotations': [0, 0, 0],\n 'id': 1107040319986696195,\n 'klass': 0,\n 'text': 'To loco'}",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#bolsa-de-palabras-dispersa",
    "href": "capitulos/06MezclaModelos.html#bolsa-de-palabras-dispersa",
    "title": "6  Mezcla de Modelos",
    "section": "6.2 Bolsa de Palabras Dispersa",
    "text": "6.2 Bolsa de Palabras Dispersa\nSe inicia con la creación de un clasificador basado en una bolsa de palabras dispersa, el clasificador es una máquina de soporte vectorial lineal (LinearSVC). La siguiente instrucción usa la clase BoW para crear este clasificador de texto. El primer paso es seleccionar el lenguaje, en este caso español (es) y después se entrena usando el método fit.\n\nbow = BoW(lang='es').fit(D)\n\nHabiendo entrenado el clasificador de texto es momento de utilizarlo para predecir, las siguientes dos instrucciones muestra el uso de la instancia bow para predecir clase del texto me golpearon y robaron la bicicleta en la noche. Se puede observar que la clase es \\(1\\), lo cual indica que el texto menciona la ejecución de un delito.\n\ntxt = 'me golpearon y robaron la bicicleta en la noche'\nbow.predict([txt])\n\narray([1])\n\n\nEl método predict recibe una lista de textos a predecir, en la siguiente instrucción se predicen todas las clases del conjunto de prueba (Dtest), la predicciones se guardar en la variable hy_bow.\n\nhy_bow = bow.predict(Dtest)\n\nHabiendo realizado la predicciones en el conjunto de prueba (\\(\\mathcal D\\)), es momento de utilizar estas para medir el rendimiento, en esta ocasión se mide el valor \\(f_1\\) para cada clase. El primer valor (\\(0.9461\\)) corresponde a la medida \\(f_1\\) en la clase negativa y el segundo (\\(0.7460\\)) corresponde al valor en la clase positiva.\n\ny = np.r_[[x['klass'] for x in Dtest]]\nf1_score(y, hy_bow, average=None)\n\narray([0.94612795, 0.74603175])\n\n\nCon el objetivo de conocer la variabilidad del rendimiento del clasificador en este conjunto de datos, la siguientes instrucciones calcula el intervalo de confianza; para realizarlo se utiliza la clase CI la cual recibe la estadística a calcular, en este caso medida \\(f_1\\). El siguiente paso es llamar a la clase con las entradas para calcular el intervalo, estas corresponden a las mediciones y predicciones del conjunto de prueba.\n\nci = CI(statistic=lambda y, hy: f1_score(y, hy, \n                                         average=None))\nci_izq, ci_der = ci(y, hy_bow)\n\nEl intervalo izquierdo es \\([0.9233, 0.6502]\\) y el derecho tiene los valores \\([0.9632, 0.8312]\\). Para complementar la información del intervalo de confianza, la Figura 6.2 muestra el histograma y la densidad estimada para calcular el intervalo de confianza. Se ve que la varianza en la clase negativa es menor además de que tiene un rendimiento mejor que en la clase positiva.\n\n\nCódigo\ndf_bow = pd.DataFrame(ci.statistic_samples, columns=['f1-neg', 'f1-pos'])\ndf_bow['Tipo'] = 'BoW'\nsns.set_style('whitegrid')\nsns.displot(df_bow, kde=True)\n\n\n\n\n\n\n\n\nFigura 6.2: Histograma de f1 por clase\n\n\n\n\n\nUna manera de poder analizar el comportamiento del clasificador de texto implementado es visualizar en una nube de palabras las características que tienen el mayor peso en la decisión. Esto se realiza en las siguientes instrucciones siendo el primer paso obtener los coeficientes de la máquina de soporte vectorial lineal, los cuales se guardan en la variable ws. El segundo componente es el valor de IDF que tiene cada uno de los términos, esto se encuentran en el atributo BoW.weights tal y como se muestra en la segunda instrucción del siguiente código.\n\nws = bow.estimator_instance.coef_[0]\nidfs = bow.weights\n\nTeniendo los valores del clasificador y del IDF, solamente es necesario obtener su producto y separar los términos positivos de los negativos, tal y como se muestra en el siguiente código. La Figura 6.3 muestra las nubes de palabras generadas, cabe mencionar que aquellos términos que tienen como prefijo q: corresponden a q-gramas de caracteres y los términos que tienen el caracter ~ corresponden a bigramas de palabras.\n\ntokens_pos = {name: w * idf\n              for name, idf, w in zip(bow.names,\n                                      idfs, ws)\n              if w &gt; 0}\ntokens_neg = {name: w * idf * -1\n              for name, idf, w in zip(bow.names,\n                                      idfs, ws)\n              if w &lt; 0}\n\nEn la figura se puede observar que las características más importantes corresponden a la presencia de asesinan, asesinan a, tiroteo entre otras y por el lado de palabras relacionadas a la clase negativa se observan secuestrado, asesina, un asesino, entre otras. En Sección 6.4 se muestran ejemplos que ayudan a comprender el hecho de que la palabra asesinan sea considerada como positiva y por otro lado asesina se en la clase negativa.\n\n\nCódigo\nword_pos = WordCloud().generate_from_frequencies(tokens_pos)\nword_neg = WordCloud().generate_from_frequencies(tokens_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_neg, word_pos],\n                     [ax1, ax2],\n                     ['Negativas', 'Positivas']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 6.3: Nubes de términos positivos y negativos",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#bolsa-de-palabras-densas",
    "href": "capitulos/06MezclaModelos.html#bolsa-de-palabras-densas",
    "title": "6  Mezcla de Modelos",
    "section": "6.3 Bolsa de Palabras Densas",
    "text": "6.3 Bolsa de Palabras Densas\nEs ahora el turno de hacer el análisis del clasificador de texto que se basado en bolsa de palabras densas. La siguiente instrucción muestra el uso de este clasificador donde se usa una bolsa de palabras con un vocabulario de \\(2^{15}\\) (parámetro voc_size_exponent=15) y además se seleccionan como representaciones aquellos entrenados en los conjuntos de emojis (emoji=True) y las palabras claves (keyword=True). En esta caso, los parámetros del clasificador no son estimados, es decir, no se llama al método fit. Esto es porque en este ejemplo se van a seleccionar aquellas representaciones que mejor representan al problema de Delitos utilizando una máquina de soporte vectorial lineal.\n\ndense = DenseBoW(lang='es',\n                 voc_size_exponent=15,\n                 emoji=True, keyword=True,\n                 dataset=False)\n\nPara seleccionar las características que mejor representan al problema de delitos se utiliza la clase SelectFromModelCV la cual usa los coeficientes de la máquina de soporte vectorial para seleccionar las características más representativas, estas corresponden aquellas que tienen los coeficientes más grandes tomando su valor absoluto. La selección se realiza llamando al método DenseBoW.select con los parámetros que se observan en las siguientes instrucciones. En particular SelectFromModelCV es un método supervisado entonces se utilizarán las clase del conjunto de entrenamiento, y para poder medir el rendimiento de cada conjunto de características seleccionadas se usa una validación cruzada. La última instrucción estima los valores del clasificador con las características seleccionadas.\n\nmacro_f1 = lambda y, hy: f1_score(y, hy, average='macro')\nkwargs = dense.estimator_kwargs\nestimator = dense.estimator_class(**kwargs)\nkwargs = dict(estimator=estimator,\n              scoring=macro_f1)\ndense.select(D=D,\n             feature_selection=SelectFromModelCV,\n             feature_selection_kwargs=kwargs)\ndense.fit(D)\n\nComo se mencionó la clase SelectFromModelCV selecciona aquellas características que mejor rendimiento dan, la clase mantiene los valores estimados en cada selección, las siguientes instrucciones ejemplifican como obtener los valores de rendimiento en las selecciones. La variable perf es una diccionario donde la llave es el número de características y el valor es el rendimiento correspondiente. La Figura 6.4 muestra es rendimiento se puede observar la dinámica donde con un poco menos de \\(1000\\) características se tiene un valor de rendimiento cercano a \\(0.9\\).\n\nselect = dense.feature_selection\nperf = select.cv_results_\n\n\n\nCódigo\n_ = [{'d': k, 'macro-f1': v} for k, v in perf.items()]\ndf = pd.DataFrame(_)\nax = sns.lineplot(df, x='d', y='macro-f1')\nsns.set_style('whitegrid')\n\n\n\n\n\n\n\n\nFigura 6.4: Rendimiento Variando el Número de Características\n\n\n\n\n\nDespués de haber seleccionado el número de características, se utiliza un código equivalente al usado en BoW para predecir las clases del conjunto de prueba (\\(\\mathcal G\\)), tal y como se muestra en la siguiente instrucción.\n\nhy_dense = dense.predict(Dtest)\n\nEl rendimiento en \\(f_1\\) del clasificador basado en una bolsa se muestra con el siguiente código. Este valor puntual se complementa con la Figura 6.5 donde se muestra la distribución de esta medición y se compara con la obtenida con el clasificador de bolsa de palabras dispersa (i.e., bow).\n\nf1_score(y, hy_dense, average=None)\n\narray([0.94158076, 0.75362319])\n\n\n\n\nCódigo\nci(y, hy_dense)\ndf_dense = pd.DataFrame(ci.statistic_samples, columns=['f1-neg', 'f1-pos'])\ndf_dense['Tipo'] = 'Dense'\n\n_ = df_bow.melt(id_vars=['Tipo'], value_name='value', var_name='f1')\n_2 = df_dense.melt(id_vars=['Tipo'], value_name='value', var_name='f1')\n_ = pd.concat((_, _2))\nsns.set_style(\"whitegrid\")\nfig = sns.displot(_, x='value', hue='f1', kde=True, col='Tipo')\n# plt.grid()\n\n\n\n\n\n\n\n\nFigura 6.5: Histogramas de f1 por clase\n\n\n\n\n\nEn un clasificador basado en palabras densas también se puede comprender su comportamiento mostrando aquellas características que tiene un mayor peso al momento de decidir la clase. En las siguientes instrucciones se agrupan las características positivas y las negativas, utilizando el valor estimado por la máquina de soporte vectorial lineal (w). Considerando que cada característica está asociada a una palabra o emoji, entonces se pueden visualizar mediante una nube de palabras.\n\nw = dense.estimator_instance.coef_[0]\nnames = np.array(dense.names)\ncarac_pos = {k: v for k, v in zip(names, w) if v &gt; 0}\ncarac_neg = {k: v * -1 for k, v in zip(names, w) if v &lt; 0}\n\nLa Figura 6.6 muestra las nubes de palabras de las características positivas y negativas, se puede observar que una característica significativa de la clase positiva corresponde al modelo robo, muere, entre otros y de la clase negativa se observa comentando, **ocurrir*, entre otras.\n\n\nCódigo\nword_pos = WordCloud().generate_from_frequencies(carac_pos)\nword_neg = WordCloud().generate_from_frequencies(carac_neg)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\n\nfor cloud, ax, title in zip([word_neg, word_pos],\n                     [ax1, ax2],\n                     ['Negativas', 'Positivas']):\n    ax.imshow(cloud, interpolation='bilinear')\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 6.6: Nube de características positivas y negativas",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#sec-analisis-ejemplo",
    "href": "capitulos/06MezclaModelos.html#sec-analisis-ejemplo",
    "title": "6  Mezcla de Modelos",
    "section": "6.4 Análisis Mediante Ejemplos",
    "text": "6.4 Análisis Mediante Ejemplos\nHasta el momento se ha presentado un análisis global de los clasificadores dispersos (bow) y densos (dense), en esta sección se especializa el análisis al nivel de ejemplos. Lo primero que se realiza es ver el valor de la función de decisión, el signo de este valor indica la clase, un valor positivo indica clase positiva y el signo negativo corresponde a la clase negativa. El valor absoluto de la función indica de manera proporcional la distancia que existe al hiperplano que define las clases. Dado que se está utilizando este valor para contrastar el comportamiento de los algoritmos entonces la distancia entre el ejemplo al hiperplano está dado por la función de decisión dividida entre la norma de los coeficientes. La primera línea calcula la norma de los coeficientes estimados tanto para el clasificador disperso (bow_norm) y el denso (dense_norm)\n\nbow_norm = np.linalg.norm(bow.estimator_instance.coef_[0])\ndense_norm = np.linalg.norm(dense.estimator_instance.coef_[0])\n\nCon las normas se procederá a calcular la función de decisión para el ejemplo Asesinan a persona en Jalisco, este ejemplo es positivo dado que menciona la ocurrencia de un delito. En las siguientes instrucciones se calcula la distancia al hiperplano la cual se puede observar que es positiva indicando que el texto es positivo.\n\ntxt = 'Asesinan a persona en Jalisco.'\nbow.decision_function([txt]) / bow_norm\n\narray([[0.03104452]])\n\n\nComplementando la distancia del clasificador disperso se presenta la distancia del clasificador denso en el siguiente código. También se puede observar que su valor es positivo, pero este se encuentre más cercano al hiperplano de decisión, lo cual indica que existe una mayor incertidumbre en su clase.\n\ndense.decision_function([txt]) / dense_norm\n\narray([[0.00906055]])\n\n\nRealizando el mismo procedimiento pero para texto La asesina vivía en Jalisco. Lo primero que se debe de notar es que el texto es negativo dado que se menciona que existe una asesina, pero el texto no indica que se haya cometido algún delito, esta fue una de las reglas que se siguió para etiquetar los textos tanto del conjunto de entrenamiento (\\(\\mathcal T\\)) como del conjunto de prueba (\\(\\mathcal G\\)). Pero es evidente que el texto anterior y el actual son sintácticamente muy similares, pero con una semántica diferente.\nEl siguiente código predice la función de decisión del clasificador disperso, la distancia es el valor absoluto del número presentado y el signo indica el lado del hiperplano, se observa que es negativo, entonces el clasificador indica que pertenece a la clase negativa.\n\ntxt = 'La asesina vivía en Jalisco.'\nbow.decision_function([txt]) / bow_norm\n\narray([[-0.03643002]])\n\n\nEl mismo procedimiento se realiza para el clasificador denso como se indica a continuación, obteniendo también un valor negativo y con una magnitud similar al encontrado por el clasificador disperso.\n\ndense.decision_function([txt]) / dense_norm\n\narray([[-0.03598119]])\n\n\nContinuando con el análisis, se puede visualizar los coeficientes más significativos para realizar la predicción. Por ejemplo, las siguientes instrucciones muestran los 5 coeficientes más significativos para predecir el texto Asesinan a persona en Jalisco.\n\ntxt = 'Asesinan a persona en Jalisco.'\nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\nsorted([(bow.names[k], w[k] * v) for k, v in vec],\n       key=lambda x: np.fabs(x[1]), reverse=True)[:5]\n\n[('asesinan', 0.21959409949185787),\n ('asesinan~a', 0.20828280738121605),\n ('q:sina', 0.14511813536902943),\n ('q:n~a~', 0.08388029475644186),\n ('q:an~a', 0.0734437622092712)]\n\n\nUn procedimiento equivalente se realiza para el clasificador denso, tal y como se muestra en el siguiente código.\n\nw = dense.estimator_instance.coef_[0]\nvec = dense.transform([txt])[0] * w\nsorted([(dense.names[k], v) for k, v in enumerate(vec)],\n       key=lambda x: np.fabs(x[1]), reverse=True)[:5]\n\n[('ocurrir', 0.06683457750173856),\n ('muere', 0.053880044741064774),\n ('consiguio', -0.05168798790090579),\n ('critican', -0.04459207389659752),\n ('hubieses', -0.04426955144485894)]\n\n\nLa Figura 6.7 muestra la nube de palabras de los términos y características más significativas para la predicción del ejemplo positivo (Asesinan a persona en Jalisco). La nube de palabras para el ejemplo negativo (La asesina vivía en Jalisco) se muestra en la Figura 6.8. La nube de palabras está codificada de la siguiente manera, las palabras que corresponden a la clase positiva está en mayúsculas y las de la clase negativa en minúsculas. Por ejemplo, en Figura 6.7 se observa que la palabra asesinan es relevante para la clasificación del ejemplo así como la característica ocurrir.\n\n\nCódigo\ndef codifica(names, vec):\n    carac_pos = dict()\n    for k, v in zip(names, vec):\n        if v &gt; 0:\n            key = f'{k.upper()}'\n        else:\n            key = k\n        carac_pos[key] = np.fabs(v)\n    return carac_pos\ntxt = 'Asesinan a persona en Jalisco.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] &gt; 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 6.7: Nube de características ejemplo positivo\n\n\n\n\n\nEn el caso del ejemplo negativo, la Figura 6.8 muestra q-gramas de caracteres asociados a la clase positiva y también es evidente la palabra asesina. En el caso del clasificador denso también se observan características positivas como ocurrir y características negativas como critican y empleados.\n\n\nCódigo\ntxt = 'La asesina vivía en Jalisco.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] &gt; 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 6.8: Nube de características ejemplo negativo\n\n\n\n\n\nComplementando los ejemplos anteriores, la Figura 6.9 muestra la nube de palabras obtenidas al calcular la función de decisión del texto Le acaban de robar la bicicleta a mi hijo. Se observa que este texto corresponde a la clase positiva y la función de decisión normalizada del clasificador disperso es \\(-0.0335\\) y del clasificador denso corresponde a \\(-0.0798\\). Ambas funciones de decisión indican que la clase es negativa, lo cual es un error. La figura muestra que los q-gramas de caracteres y las características positivas dominan las nubes, pero estas no tienen el peso suficiente para realizar una predicción correcta.\n\n\nCódigo\ntxt = 'Le acaban de robar la bicicleta a mi hijo.'\n_ = dense.transform([txt])[0] * dense.estimator_instance.coef_[0]\nword_cloud_dense = WordCloud().generate_from_frequencies(codifica(dense.names, _))\n \nw = bow.estimator_instance.coef_[0]\nvec = bow.bow[txt]\ncarac_pos = dict()\nfor k, v in vec:\n    if w[k] &gt; 0:\n        key = f'{bow.names[k].upper()}'\n    else:\n        key = bow.names[k]\n    carac_pos[key] = np.fabs(v * w[k])\n\nword_cloud = WordCloud().generate_from_frequencies(carac_pos)\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.imshow(word_cloud, interpolation='bilinear')\nax2.imshow(word_cloud_dense, interpolation='bilinear')\nfor ax, title in zip([ax1, ax2], ['BoW', 'DenseBoW']):\n    ax.grid(False)\n    ax.tick_params(left=False, right=False, labelleft=False,\n                   labelbottom=False, bottom=False)\n    ax.set_title(title)\n\n\n\n\n\n\n\n\nFigura 6.9: Nube de características en un ejemplo positivo con predicción negativa",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#combinando-modelos",
    "href": "capitulos/06MezclaModelos.html#combinando-modelos",
    "title": "6  Mezcla de Modelos",
    "section": "6.5 Combinando Modelos",
    "text": "6.5 Combinando Modelos\n\n\n\n\n\n\nflowchart TB\n    Conc[Concatenación]\n    Representación --&gt; Estimadores --&gt; Conc\n    Conc --&gt; Predicción\n    subgraph Representación [Representaciones]\n        direction TB\n        Repr1([Representación 1])\n        Repr2([Representación 2])\n        ReprI([...])\n        ReprM([Representación M])\n    end\n    subgraph Estimadores\n        direction TB    \n        Est1[Estimador 1] \n        Est2[Estimador 2]\n        EstI[...]\n        EstM[Estimador M]\n    end\n    subgraph Predicción [Etapa de Predicción]\n        direction TB\n        Entrada\n        Entrada -- Clasificación --&gt; Clasificador[Clasificador]\n        Entrada -- Regresión --&gt; Regresor[Regresor]\n        Clasificador --&gt; Prediccion([Predicción])\n        Regresor --&gt; Prediccion\n    end\n\n\n\n\nFigura 6.10: Diagrama de predicción en mezcla de modelos\n\n\n\n\n\nLa siguiente pregunta es conocer si los modelos anteriores se pueden combinar para realizar una mejor predicción. En esta sección se utiliza la técnica de Stack Generalization (Wolpert (1992), Graff et al. (2020)) para combinar los dos modelos. La siguiente linea entrena el clasificador, el cual recibe como parámetros los clasificador a juntar.\n\nstack = StackGeneralization([bow, dense]).fit(D)\n\nSiguiendo el procedimiento de los clasificadores dispersos y densos, la siguiente linea predice la clase de los ejemplos del conjunto de prueba y calcula su rendimiento en términos de la medida \\(f_1\\).\n\nhy_stack = stack.predict(Dtest)\nf1_score(y, hy_stack, average=None)\n\narray([0.94791667, 0.79166667])\n\n\nPara poder comparar el rendimiento de los tres clasificadores desarrollados, la Tabla 6.1 presenta el rendimiento con las medidas recall y precision en las dos clases. Se puede observar que el bow tiene el mejor recall en la clase negativa y mejor precision en la clase positiva. Por otro lado el mejor recall en la clase positiva y precision en la clase negativa lo tiene stack.\n\n\n\nTabla 6.1: Rendimiento\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRecall neg\n Recall pos\nPrecision neg\n Precision pos\n\n\n\n\nbow\n\\(0.9894\\)\n\\(0.6184\\)\n\\(0.9065\\)\n\\(0.9400\\)\n\n\ndense\n\\(0.9648\\)\n\\(0.6842\\)\n\\(0.9195\\)\n\\(0.8387\\)\n\n\nstack\n\\(0.9613\\)\n\\(0.7500\\)\n\\(0.9349\\)\n\\(0.8382\\)\n\n\n\n\n\n\n\nCon respecto al rendimiento en términos de \\(f_1\\), la Tabla 6.2 presenta la información con respecto a cada clase y la última columna contiene el macro-\\(f_1\\). Los valores indican que en la clase positiva el mejor valor corresponde a stack lo cual se ve reflejado en el macro-\\(f_1\\). El algoritmo de Stack Generalization nos indica que se hizo una mejora en la predicción de la clase positiva y la clase negativa se mantuvo constante al menos con respecto de la medida \\(f_1\\).\n\n\n\nTabla 6.2: Rendimiento\n\n\n\n\n\n\n\nf1 neg\n f1 pos\nmacro-f1\n\n\n\n\nbow\n\\(0.9461\\)\n\\(0.7460\\)\n\\(0.8461\\)\n\n\ndense\n\\(0.9416\\)\n\\(0.7536\\)\n\\(0.8476\\)\n\n\nstack\n\\(0.9479\\)\n\\(0.7917\\)\n\\(0.8698\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraff, Mario, Sabino Miranda-Jiménez, Eric S. Tellez, y Daniela Moctezuma. 2020. «EvoMSA: A Multilingual Evolutionary Approach for Sentiment Analysis». Computational Intelligence Magazine 15: 76-88. https://ieeexplore.ieee.org/document/8956106.\n\n\nWolpert, David H. 1992. «Stacked generalization». Neural Networks 5 (2): 241-59. https://doi.org/10.1016/S0893-6080(05)80023-1.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Mezcla de Modelos</span>"
    ]
  },
  {
    "objectID": "capitulos/07TareasClasificacion.html",
    "href": "capitulos/07TareasClasificacion.html",
    "title": "7  Tareas de Clasificación de Texto",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Tareas de Clasificación de Texto</span>"
    ]
  },
  {
    "objectID": "capitulos/08BasesConocimiento.html",
    "href": "capitulos/08BasesConocimiento.html",
    "title": "8  Bases de Conocimiento",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Bases de Conocimiento</span>"
    ]
  },
  {
    "objectID": "capitulos/09Visualizacion.html",
    "href": "capitulos/09Visualizacion.html",
    "title": "9  Visualización",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Visualización</span>"
    ]
  },
  {
    "objectID": "capitulos/10Conclusiones.html",
    "href": "capitulos/10Conclusiones.html",
    "title": "10  Conclusiones",
    "section": "",
    "text": "El objetivo de la unidad es",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Conclusiones</span>"
    ]
  },
  {
    "objectID": "capitulos/11Referencias.html",
    "href": "capitulos/11Referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Graff, Mario, Sabino Miranda-Jiménez, Eric S. Tellez, and Daniela\nMoctezuma. 2020. “EvoMSA: A Multilingual Evolutionary\nApproach for Sentiment Analysis.” Computational Intelligence\nMagazine 15: 76–88. https://ieeexplore.ieee.org/document/8956106.\n\n\nSalton, Gerard, and Chungshu S. Yang. 1973. “On the Specification\nof Term Values in Automatic Indexing.” Journal of\nDocumentation 29 (April): 351–72. https://doi.org/10.1108/EB026562.\n\n\nSparck Jones, Karen. 1972. “A Statistical Interpretation of Term\nSpecificity and Its Application in Retrieval.” Journal of\nDocumentation 28: 11–21. https://doi.org/10.1108/EB026526.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nOscar S. Siordia, and Elio A. Villaseñor. 2017. “A Case Study of\nSpanish Text Transformations for Twitter Sentiment Analysis.”\nExpert Systems with Applications 81: 457–71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nRanyart R. Suárez, and Oscar S. Siordia. 2017. “A\nSimple Approach to Multilingual\nPolarity Classification in\nTwitter.” Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, and Mario\nGraff. 2018. “An Automated Text Categorization Framework Based on\nHyperparameter Optimization.” Knowledge-Based Systems\n149: 110–23. https://doi.org/10.1016/j.knosys.2018.03.003.\n\n\nWolpert, David H. 1992. “Stacked\ngeneralization.” Neural Networks 5 (2): 241–59.\nhttps://doi.org/10.1016/S0893-6080(05)80023-1.",
    "crumbs": [
      "Referencias"
    ]
  }
]