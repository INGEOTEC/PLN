[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Procesamiento de Lenguaje Natural",
    "section": "",
    "text": "Prefacio\nEl curso trata de ser auto-contenido, es decir, no debería de ser necesario leer otras fuentes para poder entenderlo y realizar las actividades. De cualquier manera es importante comentar que el curso está basado en los siguientes libros de texto:\n\nSpeech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Third Edition draft. Daniel Jurafsky and James H. Martin. pdf\nIntroduction to machine learning, Third Edition. Ethem Alpaydin. MIT Press.\nAn Introduction to Statistical Learning with Applications in R. Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani. Springer Texts in Statistics.\nAll of Statistics. A Concise Course in Statistical Inference. Larry Wasserman. MIT Press.\nAn Introduction to the Bootstrap. Bradley Efron and Robert J. Tibshirani. Monographs on Statistics and Applied Probability 57. Springer-Science+Business Media.\nUnderstanding Machine Learning: From Theory to Algorithms. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press.\n\n\n\nNotación\nLa Tabla 1.1 muestra la notación que se seguirá en este documento.\n\n\nTabla 1.1: Notación\n\n\n\n\n\n\nSímbolo\nSignificado\n\n\n\n\n\\(x\\)\nVariable usada comunmente como entrada\n\n\n\\(y\\)\nVariable usada comunmente como salida\n\n\n\\(\\mathbb R\\)\nNúmeros reales\n\n\n\\(\\mathbf x\\)\nVector Columna \\(\\mathbf x \\in \\mathbb R^d\\)\n\n\n\\(d\\)\nDimensión\n\n\n\\(\\mathbf w \\cdot \\mathbf x\\)\nProducto punto donde \\(\\mathbf w\\) y \\(\\mathbf x \\in \\mathbb R^d\\)\n\n\n\\(\\mathcal D\\)\nConjunto de datos\n\n\n\\(\\mathcal T\\)\nConjunto de entrenamiento\n\n\n\\(\\mathcal V\\)\nConjunto de validación\n\n\n\\(\\mathcal G\\)\nConjunto de prueba\n\n\n\\(N\\)\nNúmero de ejemplos\n\n\n\\(K\\)\nNúmero de clases\n\n\n\\(\\mathbb P(\\cdot)\\)\nProbabilidad\n\n\n\\(\\mathcal X, \\mathcal Y\\)\nVariables aleatorías\n\n\n\\(\\mathcal N(\\mu, \\sigma^2)\\)\nDistribución Normal con parámetros \\(\\mu\\) y \\(\\sigma^2\\)\n\n\n\\(f_{\\mathcal X}\\)\nFunción de densidad de probabilidad de \\(\\mathcal X\\)\n\n\n\\(\\mathbb 1(e)\\)\nFunción para indicar; \\(1\\) only if \\(e\\) is true\n\n\n\\(\\Omega\\)\nEspacio de búsqueda\n\n\n\\(\\mathbb V\\)\nVarianza\n\n\n\\(\\mathbb E\\)\nEsperanza"
  },
  {
    "objectID": "capitulos/01Introduccion.html",
    "href": "capitulos/01Introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "href": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "title": "2  Manejando Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.params import OPTION_GROUP, OPTION_DELETE, OPTION_NONE\nfrom microtc.textmodel import SKIP_SYMBOLS\nfrom b4msa.textmodel import TextModel\nfrom b4msa.lang_dependency import LangDependency\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud as WC\nfrom matplotlib import pylab as plt\nimport numpy as np\nimport unicodedata\nimport re\n\n\nVideo explicando la unidad"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#normalización-de-texto",
    "href": "capitulos/02ManejandoTexto.html#normalización-de-texto",
    "title": "2  Manejando Texto",
    "section": "2.1 Normalización de Texto",
    "text": "2.1 Normalización de Texto\nIn all the topics covered, the assumption is that the text is well-formatted and spaces nicely surround the words (tokens). However, this is not the general case, and the spelling errors and the procedure used to define the tokens strongly impact the algorithm’s performance. Consequently, this part of the course is devoted to presenting standard techniques used to normalize the text and to transform the text into tokens.\nThe text normalization described are mainly the ones used in the following research words:\n\nAn automated text categorization framework based on hyperparameter optimization (Tellez et al. (2018))\nA simple approach to multilingual polarity classification in Twitter (Tellez, Miranda-Jiménez, Graff, Moctezuma, Suárez, et al. (2017))\nA case study of Spanish text transformations for twitter sentiment analysis (Tellez, Miranda-Jiménez, Graff, Moctezuma, Siordia, et al. (2017))"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#entity",
    "href": "capitulos/02ManejandoTexto.html#entity",
    "title": "2  Manejando Texto",
    "section": "2.2 Entity",
    "text": "2.2 Entity\nThe journey of text normalization starts with handling different entities within a text; the entities could be the mentioned of a user in a tweet, the numbers, or the URL, to mention a few. The actions performed to the entities found are to delete them or replace them for a particular token.\n\n2.2.1 Users\nThe first process is to deal with username following the format of Twitter. In a tweet, the mention of a user is identified with a string starting with the character @. The two actions could be to delete all the users’ mentions or change them for a common label.\nThe procedure uses regular expressions to find the entities; for example, the following code can remove the users’ mentions.\n\ntext = 'Hi @xx, @mm is talking about you.'\nre.sub(r\"@\\S+\", \"\", text)\n\n'Hi   is talking about you.'\n\n\nOn the other hand, to replace the username with a shared label can be implemented with the following code, where the label is _usr\n\ntext = 'Hi @xx, @mm is talking about you.'\nre.sub(r\"@\\S+\", \"_usr\", text)\n\n'Hi _usr _usr is talking about you.'\n\n\n\n\n2.2.2 URL\nThe previous code can be adapted to handle URL; one only needs to define the regular expression to use; see the following code that removes all the appearances of the URL.\n\ntext = \"go http://google.com, and find out\"\nre.sub(r\"https?://\\S+\", \"\", text)\n\n'go  and find out'\n\n\n\n\n2.2.3 Numbers\nThe previous code can be modified to deal with numbers and replace the number found with a shared label such as _num.\n\ntext = \"we have won 10 M\"\nre.sub(r\"\\d\\d*\\.?\\d*|\\d*\\.\\d\\d*\", \"_num\", text)\n\n'we have won _num M'"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#spelling",
    "href": "capitulos/02ManejandoTexto.html#spelling",
    "title": "2  Manejando Texto",
    "section": "2.3 Spelling",
    "text": "2.3 Spelling\nThe next block of text normalization modifies the writing of the text, removing components that, for particular applications, can be ignored to reduce the vocabulary size, which impacts the complexity of the algorithm and could be reflected in an improvement in the performance.\n\n2.3.1 Case sensitive\nThe first of these transformations is the conversion to lower case; transforming all the words to the lower case has the consequence that the vocabulary is reduced, e.g., the word Mexico and mexico would be considered the same token. This operation can be implemented with function lower as follows.\n\ntext = \"Mexico\"\ntext.lower()\n\n'mexico'\n\n\n\n\n2.3.2 Punctuation\nThe punctuation symbols are essential to natural language understanding and generation; however, for other applications, such as sentiment analysis or text categorization, its contribution is opaque by the increase in the vocabulary size. Consequently, its removal influences the vocabulary size, which sometimes has a positive result on the performance.\nThese symbols can be removed by traversing the string and skipping the punctuations.\n\ntext = \"Hi! good morning,\"\noutput = \"\"\nfor x in text:\n    if x in SKIP_SYMBOLS:\n        continue\n    output += x\noutput\n\n'Hi good morning'\n\n\n\n\n2.3.3 Diacritic\nDifferent languages use diacritic symbols, e.g., México; as expected, this has the consequence of increasing the vocabulary. On the other hand, in informal writing, the misuse of diacritic symbols is common; one particular way to handle this problem is to remove the diacritic symbols and treat them as the same word, e.g., México would be replaced by Mexico.\n\ntext = 'México'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 &lt;= o and o &lt;= 0x036F:\n        continue\n    output += x\noutput\n\n'Mexico'"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#semantic-normalizations",
    "href": "capitulos/02ManejandoTexto.html#semantic-normalizations",
    "title": "2  Manejando Texto",
    "section": "2.4 Semantic Normalizations",
    "text": "2.4 Semantic Normalizations\nThe next set of normalization techniques aims to reduce the vocabulary size using the meaning of the words to modify them or remove them from the text.\n\n2.4.1 Stop words\nThe stop words are the most frequent words used in the language. These words are essential to communicate but are not so much on tasks where the aim is to discriminate texts according to their meaning.\nThe stop words can be stored in a dictionary, and then the process of removing them consists of traversing all the tokens from a text and then removing those in the dictionary. The process is exemplified with the following code.\n\nlang = LangDependency('english')\n\ntext = 'Good morning! Today, we have a warm weather.'\noutput = []\nfor word in text.split():\n    if word.lower() in lang.stopwords[len(word)]:\n        continue\n    output.append(word)\noutput = \" \".join(output) \noutput\n\n'Good morning! Today, warm weather.'\n\n\n\n\n2.4.2 Stemmming and Lemmatization\nThe idea of stemming and lemmatization, seen as a normalization process, is to group different words based on their root; for example, the process would associate words like playing, player, plays with the token play.\nStemming treats the problem with fewer constraints than lemmatization, having as a consequence that the common word found cannot be the common root of the words; additionally, the algorithms do not consider the role of the word being processed in the sentence. On the other hand, a lemmatization algorithm obtains the root of the word considering the part of the speech of the processed word.\n\nstemmer = PorterStemmer()\n\ntext = 'I like playing football'\noutput = []\nfor word in text.split():\n    w = stemmer.stem(word)\n    output.append(w)\noutput = \" \".join(output) \noutput\n\n'i like play footbal'"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#tokenization",
    "href": "capitulos/02ManejandoTexto.html#tokenization",
    "title": "2  Manejando Texto",
    "section": "2.5 Tokenization",
    "text": "2.5 Tokenization\nOnce the text has been normalized, it is time to transform it into its fundamental elements, which could be words, bigrams, n-grams, substrings, or a combination of them; this process is known as tokenization. Different methods can be applied to tokenize a text, the one used is so far is to transform a text into a list of words where the word is surrounded by space or non-printable characters. The decision of which tokenizer to use depends on the application; for example, in order to generate text, it is crucial to learn the punctuation symbols, so these symbols are tokens. On the other hand, in the text categorization problem, where the task is to classify a text, it might be irrelevant to keep the order of the words.\n\n2.5.1 n-grams\nThe first tokenizer review corresponds to transforming the text into words, bigrams, and in general, n-grams. The case of words is straightforward using the function split; once the words have been obtained, these can be combined to form an n-gram of any size, as shown below.\n\ntext = 'I like playing football on Saturday'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n\n['I~like~playing',\n 'like~playing~football',\n 'playing~football~on',\n 'football~on~Saturday']\n\n\n\n\n2.5.2 q-grams\nThe q-gram tokenizer complements the n-grams one; it is defined as the substring of length \\(q\\). The q-grams have two relevant features; the first one is that they are language agnostic consequently can be applied to any language, and the second is that they tackle the misspelling problem from an approximate matching perspective.\nThe code is equivalent to the one used to compute n-grams, being the difference that the iteration is on characters instead of words.\n\ntext = 'I like playing'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n\n['I li',\n ' lik',\n 'like',\n 'ike ',\n 'ke p',\n 'e pl',\n ' pla',\n 'play',\n 'layi',\n 'ayin',\n 'ying']"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#textmodel",
    "href": "capitulos/02ManejandoTexto.html#textmodel",
    "title": "2  Manejando Texto",
    "section": "2.6 TextModel",
    "text": "2.6 TextModel\nThe class TextModel of the library B4MSA contains the text normalization and tokenizers described and can be used as follows.\nThe first step is to instantiate the class given the desired parameters. The Entity parameters have three options to delete (OPTION_DELETE) the entity, replace (OPTION_GROUP) it with a predefined token, or do not apply that operation (OPTION_NONE). These parameters are:\n\nusr_option\nurl_option\nnum_option\n\nThe class has three additional transformation which are:\n\nemo_option\nhashtag_option\nent_option\n\nThe Spelling transformations can be triggered with the following keywords:\n\nlc\ndel_punc\ndel_diac\n\nwhich corresponds to lower case, punctuation, and diacritic.\nThe Semantic normalizations are set up with the parameters:\n\nstopwords\nstemming\n\nFinally, the tokenizer is configured with the token_list parameter, which has the following format; negative numbers indicate \\(n\\)-grams and positive numbers \\(q\\)-grams.\nFor example, the following code invokes the text normalization algorithm; the only difference is that spaces are replaced with ~.\n\ntext = 'I like playing football with @mgraffg'\ntm = TextModel(token_list=[-1, 3], lang='english', \n               usr_option=OPTION_GROUP,\n               stemming=True)\ntm.text_transformations(text)\n\n'~i~like~play~fotbal~with~_usr~'\n\n\nOn the other hand, the tokenizer is used as follows.\n\ntext = 'I like playing football with @mgraffg'\ntm = TextModel(token_list=[-1, 5], lang='english', \n               usr_option=OPTION_GROUP,\n               stemming=True)\ntm.tokenize(text)\n\n['i',\n 'like',\n 'play',\n 'fotbal',\n 'with',\n '_usr',\n 'q:~i~li',\n 'q:i~lik',\n 'q:~like',\n 'q:like~',\n 'q:ike~p',\n 'q:ke~pl',\n 'q:e~pla',\n 'q:~play',\n 'q:play~',\n 'q:lay~f',\n 'q:ay~fo',\n 'q:y~fot',\n 'q:~fotb',\n 'q:fotba',\n 'q:otbal',\n 'q:tbal~',\n 'q:bal~w',\n 'q:al~wi',\n 'q:l~wit',\n 'q:~with',\n 'q:with~',\n 'q:ith~_',\n 'q:th~_u',\n 'q:h~_us',\n 'q:~_usr',\n 'q:_usr~']\n\n\nIt can be observed that all \\(q\\)-grams start with the prefix q:.\n\n\n\n\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Oscar S. Siordia, y Elio A. Villaseñor. 2017. «A case study of Spanish text transformations for twitter sentiment analysis». Expert Systems with Applications 81: 457-71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Ranyart R. Suárez, y Oscar S. Siordia. 2017. «A Simple Approach to Multilingual Polarity Classification in Twitter». Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, y Mario Graff. 2018. «An automated text categorization framework based on hyperparameter optimization». Knowledge-Based Systems 149: 110-23. https://doi.org/10.1016/j.knosys.2018.03.003."
  },
  {
    "objectID": "capitulos/03ModeladoLenguaje.html",
    "href": "capitulos/03ModeladoLenguaje.html",
    "title": "3  Modelado de Lenguaje",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "href": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "title": "4  Clasificación de Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.utils import tweet_iterator, load_model, save_model\nfrom b4msa.textmodel import TextModel\nfrom EvoMSA.tests.test_base import TWEETS\nfrom EvoMSA.utils import bootstrap_confidence_interval\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.stats import norm, multinomial, multivariate_normal\nfrom scipy.special import logsumexp\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nfrom os.path import join\nimport numpy as np\n\n\nVideo explicando la unidad"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#introducción",
    "href": "capitulos/04ClasificacionTexto.html#introducción",
    "title": "4  Clasificación de Texto",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nText Categorization is an NLP task that deals with creating algorithms capable of identifying the category of a text from a set of predefined categories. For example, sentiment analysis belongs to this task, and the aim is to detect the polarity (e.g., positive, neutral, or negative) of a text. Furthermore, different NLP tasks that initially seem unrelated to this problem can be formulated as a classification one such as question answering and sentence entailment, to mention a few.\nText Categorization can be tackled from different perspectives; the one followed here is to treat it as a supervised learning problem. As in any supervised learning problem, the starting point is a set of pairs, where the first element of the pair is the input and the second one corresponds to the output. Let \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text."
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "href": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "title": "4  Clasificación de Texto",
    "section": "4.2 Modelado Probabilistico (Distribución Categórica)",
    "text": "4.2 Modelado Probabilistico (Distribución Categórica)\n\n4.2.1 Problema Sintético\nThe description of Bayes’ theorem continues with an example of a Categorical distribution. A Categorical distribution can simulate the drawn of \\(K\\) events that can be encoded as characters, and \\(\\ell\\) repetitions can be represented as a sequence of characters. Consequently, the distribution can illustrate the generation sequences associated with different classes, e.g., positive or negative.\nThe first step is to create the dataset. As done previously, two distributions are defined, one for each class; it can be observed that each distribution has different parameters. The second step is to sample these distributions; the distributions are sampled 1000 times with the following procedure. Each time, a random variable representing the number of outcomes taken from each distribution is drawn from a Normal \\(\\mathcal N(15, 3)\\) and stored in the variable length. The random variable indicates the number of outcomes for each Categorical distribution; the results are transformed into a sequence, associated to the label corresponding to the positive and negative class, and stored in the list D.\npos = multinomial(1, [0.20, 0.20, 0.35, 0.25])\nneg = multinomial(1, [0.35, 0.20, 0.25, 0.20])\nlength = norm(loc=15, scale=3)\nD = []\nm = {k: chr(122 - k) for k in range(4)}\nid2w = lambda x: \" \".join([m[_] for _ in x.argmax(axis=1)])\nfor l in length.rvs(size=1000):\n    D.append((id2w(pos.rvs(round(l))), 1))\n    D.append((id2w(neg.rvs(round(l))), 0))\nThe following table shows four examples of this process; the first column contains the sequence, and the second the associated label.\n\n\n\nText\nLabel\n\n\n\n\nx w x x z w y\npositive\n\n\ny w z z z x w\nnegative\n\n\nz x x x z x z w x w\npositive\n\n\nx w z w y z z z z w\nnegative\n\n\n\nAs done previously, the first step is to compute the likelihood given that dataset; considering that the data comes from a Categorical distribution, the procedure to estimate the parameters is similar to the ones used to estimate the prior. The following code estimates the data parameters corresponding to the positive class. It can be observed that the parameters estimated are similar to the ones used to generate the dataset.\nD_pos = []\n[D_pos.extend(data.split()) for data, k in D if k == 1]\nwords, l_pos = np.unique(D_pos, return_counts=True)\nw2id = {v: k for k, v in enumerate(words)}\nl_pos = l_pos / l_pos.sum()\nl_pos\narray([0.25489421, 0.33854064, 0.20773186, 0.1988333 ])\nAn equivalent procedure is performed to calculate the likelihood of the negative class.\nD_neg = []\n[D_neg.extend(data.split()) for data, k in D if k == 0]\n_, l_neg = np.unique(D_neg, return_counts=True)\nl_neg = l_neg / l_neg.sum()\nThe prior is estimated with the following code, equivalent to the one used on all the examples seen so far.\n_, priors = np.unique([k for _, k in D], return_counts=True)\nN = priors.sum()\nprior_pos = priors[1] / N\nprior_neg = priors[0] / N\nOnce the parameters have been identified, these can be used to predict the class of a given sequence. The first step is to compute the likelihood, e.g., \\(\\mathbb P(\\)w w x z\\(\\mid \\mathcal Y)\\). It can be observed that the sequence needs to be transformed into tokens which can be done with the split method. Then, the token is converted into an index using the mapping w2id; once the index is retrieved, it can be used to obtain the parameter associated with the word. The likelihood is the product of all the probabilities; however, this product is computed in log space.\ndef likelihood(params, txt):\n    params = np.log(params)\n    _ = [params[w2id[x]] for x in txt.split()]\n    tot = sum(_)\n    return np.exp(tot)\nThe likelihood combined with the prior for all the classes produces the evidence, which subsequently is used to calculate the posterior distribution. The posterior is then used to predict the class for all the sequences in \\(\\mathcal D\\). The predictions are stored in the variable hy.\npost_pos = [likelihood(l_pos, x) * prior_pos for x, _ in D]\npost_neg = [likelihood(l_neg, x) * prior_neg for x, _ in D]\nevidence = np.vstack([post_pos, post_neg]).sum(axis=0)\npost_pos /= evidence\npost_neg /= evidence\nhy = np.where(post_pos &gt; post_neg, 1, 0)\n\n\n4.2.2 Clasificador de Texto\nThe approach followed on text categorization is to treat it as supervised learning problem where the starting point is a dataset \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text. For example, the next code uses a toy sentiment analysis dataset with four classes: negative (N), neutral (NEU), absence of polarity (NONE), and positive (P).\nD = [(x['text'], x['klass']) for x in tweet_iterator(TWEETS)]\nAs can be observed, \\(\\mathcal D\\) is equivalent to the one used in the Categorical Distribution example. The difference is that sequence of letters is changed with a sentence. Nonetheless, a feasible approach is to obtain the tokens using the split method. Another approach is to retrieve the tokens using a Tokenizer, as covered in the Text Normalization Section.\nThe following code uses the TextModel class to tokenize the text using words as the tokenizer; the tokenized text is stored in the variable D.\ntm = TextModel(token_list=[-1])\ntok = tm.tokenize\nD = [(tok(x), y) for x, y in D]\nBefore estimating the likelihood parameters, it is needed to encode the tokens using an index; by doing it, it is possible to store the parameters in an array and compute everything numpy operations. The following code encodes each token with a unique index; the mapping is in the dictionary w2id.\nwords = set()\n[words.update(x) for x, y in D]\nw2id = {v: k for k, v in enumerate(words)}\nPreviously, the classes have been represented using natural numbers. The positive class has been associated with the number \\(1\\), whereas the negative class with \\(0\\). However, in this dataset, the classes are strings. It was decided to encode them as numbers to facilitate subsequent operations. The encoding process can be performed simultaneously with the estimation of the prior of each class. Please note that the priors are stored using the logarithm in the variable priors.\nuniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\npriors = np.log(priors / priors.sum())\nuniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\nIt is time to estimate the likelihood parameters for each of the classes. It is assumed that the data comes from a Categorical distribution and that each token is independent. The likelihood parameters can be stored in a matrix (variable l_tokens) with \\(K\\) rows, each row contains the parameters of the class, and the number of columns corresponds to the vocabulary’s size. The first step is to calculate the frequency of each token per class which can be done with the following code.\nl_tokens = np.zeros((len(uniq_labels), len(w2id)))\nfor x, y in D:\n    w = l_tokens[uniq_labels[y]]\n    cnt = Counter(x)\n    for i, v in cnt.items():\n        w[w2id[i]] += v\nThe next step is to normalize the frequency. However, before normalizing it, it is being used a Laplace smoothing with a value \\(0.1\\). Therefore, the constant \\(0.1\\) is added to all the matrix elements. The next step is to normalize (second line), and finally, the parameters are stored using the logarithm.\nl_tokens += 0.1\nl_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\nl_tokens = np.log(l_tokens)\n\n4.2.2.1 Prediction\nOnce all the parameters have been estimated, it is time to use the model to classify any text. The following function computes the posterior distribution. The first step is to tokenize the text (second line) and compute the frequency of each token in the text. The frequency stored in the dictionary cnt is converted into the vector x using the mapping function w2id. The final step is to compute the product of the likelihood and the prior. The product is computed in log-space; thus, this is done using the likelihood and the prior sum. The last step is to compute the evidence and normalize the result; the evidence is computed with the function logsumexp.\ndef posterior(txt):\n    x = np.zeros(len(w2id))\n    cnt = Counter(tm.tokenize(txt))\n    for i, v in cnt.items():\n        try:\n            x[w2id[i]] += v\n        except KeyError:\n            continue\n    _ = (x * l_tokens).sum(axis=1) + priors\n    l = np.exp(_ - logsumexp(_))\n    return l\nThe posterior function can predict all the text in \\(\\mathcal D\\); the predictions are used to compute the model’s accuracy. In order to compute the accuracy, the classes in \\(\\mathcal D\\) need to be transformed using the nomenclature of the likelihood matrix and priors vector; this is done with the uniq_labels dictionary (second line).\nhy = np.array([posterior(x).argmax() for x, _ in D])\ny = np.array([uniq_labels[y] for _, y in D])\n(y == hy).mean()\n0.974\n\n\n4.2.2.2 Training\nSolving supervised learning problems requires two phases; one is the training phase, and the other is the prediction. The posterior function handles the later phase, and it is missing to organize the code described in a training function. The following code describes the training function; it requires the dataset’s parameters and an instance of TextModel.\ndef training(D, tm):\n    tok = tm.tokenize\n    D =[(tok(x), y) for x, y in D]\n    words = set()\n    [words.update(x) for x, y in D]\n    w2id = {v: k for k, v in enumerate(words)}\n    uniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\n    priors = np.log(priors / priors.sum())\n    uniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n    l_tokens = np.zeros((len(uniq_labels), len(w2id)))\n    for x, y in D:\n        w = l_tokens[uniq_labels[y]]\n        cnt = Counter(x)\n        for i, v in cnt.items():\n            w[w2id[i]] += v\n    l_tokens += 0.1\n    l_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\n    l_tokens = np.log(l_tokens)\n    return w2id, uniq_labels, l_tokens, priors"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "href": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "title": "4  Clasificación de Texto",
    "section": "4.3 Modelado Vectorial",
    "text": "4.3 Modelado Vectorial\nxxx"
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html",
    "href": "capitulos/05RepresentacionTexto.html",
    "title": "5  Representación de Texto",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/06MezclaModelos.html",
    "href": "capitulos/06MezclaModelos.html",
    "title": "6  Mezcla de Modelos",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/07TareasClasificacion.html",
    "href": "capitulos/07TareasClasificacion.html",
    "title": "7  Tareas de Clasificación de Texto",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/08BasesConocimiento.html",
    "href": "capitulos/08BasesConocimiento.html",
    "title": "8  Bases de Conocimiento",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/09Visualizacion.html",
    "href": "capitulos/09Visualizacion.html",
    "title": "9  Visualización",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/10Conclusiones.html",
    "href": "capitulos/10Conclusiones.html",
    "title": "10  Conclusiones",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/11Referencias.html",
    "href": "capitulos/11Referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Tellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nOscar S. Siordia, and Elio A. Villaseñor. 2017. “A Case Study of\nSpanish Text Transformations for Twitter Sentiment Analysis.”\nExpert Systems with Applications 81: 457–71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nRanyart R. Suárez, and Oscar S. Siordia. 2017. “A\nSimple Approach to Multilingual\nPolarity Classification in\nTwitter.” Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, and Mario\nGraff. 2018. “An Automated Text Categorization Framework Based on\nHyperparameter Optimization.” Knowledge-Based Systems\n149: 110–23. https://doi.org/10.1016/j.knosys.2018.03.003."
  }
]