[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Procesamiento de Lenguaje Natural",
    "section": "",
    "text": "Prefacio\nEl curso trata de ser auto-contenido, es decir, no debería de ser necesario leer otras fuentes para poder entenderlo y realizar las actividades. De cualquier manera es importante comentar que el curso está basado en los siguientes libros de texto:\nLa Tabla 1.1 muestra la notación que se seguirá en este documento."
  },
  {
    "objectID": "index.html#licencia",
    "href": "index.html#licencia",
    "title": "Procesamiento de Lenguaje Natural",
    "section": "Licencia",
    "text": "Licencia\n\n\n\n\n\nEsta obra está bajo una Licencia Creative Commons Atribución-CompartirIgual 4.0 Internacional"
  },
  {
    "objectID": "capitulos/01Introduccion.html",
    "href": "capitulos/01Introduccion.html",
    "title": "1  Introducción",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "href": "capitulos/02ManejandoTexto.html#paquetes-usados",
    "title": "2  Manejando Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.params import OPTION_GROUP, OPTION_DELETE,\\\n                           OPTION_NONE\nfrom microtc.textmodel import SKIP_SYMBOLS\nfrom b4msa.textmodel import TextModel\nfrom b4msa.lang_dependency import LangDependency\nfrom nltk.stem.snowball import SnowballStemmer\nimport unicodedata\nimport re\n\n\nVideo explicando la unidad"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#introducción",
    "href": "capitulos/02ManejandoTexto.html#introducción",
    "title": "2  Manejando Texto",
    "section": "2.1 Introducción",
    "text": "2.1 Introducción\nSe podría suponer que el texto se que se analizará está bien escrito y tiene un formato adecuado para su procesamiento. Desafortunadamente, la realidad es que en la mayoría de aplicaciones el texto que se analiza tiene errores de ortográficos, errores de formato y además no es trivial identificar la unidad mínima de procesamiento que podría ser de manera natural, en el español, las palabras. Por este motivo, esta unidad trata técnicas comunes que se utilizan para normalizar el texto, esta normalización es un proceso previo al desarrollo de los algoritmos de PLN.\nLa Figura 2.1 esquematiza el procedimiento que se presenta en esta unidad, la idea es que se un texto pasa primeramente a un proceso de normalización (Sección 2.2 y Sección 2.3), para después ser segmentado (ver Sección 2.4) y el resultado es lo que se utiliza para modelar el lenguaje.\n\n\n\n\n\nflowchart LR\n    I([Texto]) --&gt;  A[Normalización de Texto]\n    A --&gt; B[Segmentación]\n    B --&gt; C(...)\n\n\nFigura 2.1: Diagrama de Pre-procesamiento\n\n\n\n\nLas normalizaciones y segmentaciones descritas en esta unidad se basan principalmente en las utilizadas en los siguientes artículos científicos.\n\nAn automated text categorization framework based on hyperparameter optimization (Tellez et al. (2018))\nA simple approach to multilingual polarity classification in Twitter (Tellez, Miranda-Jiménez, Graff, Moctezuma, Suárez, et al. (2017))\nA case study of Spanish text transformations for twitter sentiment analysis (Tellez, Miranda-Jiménez, Graff, Moctezuma, Siordia, et al. (2017))"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-normalizacion",
    "href": "capitulos/02ManejandoTexto.html#sec-normalizacion",
    "title": "2  Manejando Texto",
    "section": "2.2 Normalización de Texto Sintáctica",
    "text": "2.2 Normalización de Texto Sintáctica\nLa descripción de las normalizaciones empieza presentando las que se puede aplicar a nivel de caracteres, sin la necesidad de conocer el significado de las palabras. También se agrupan en este conjunto aquellas transformaciones que se realizan mediante expresiones regulares o su búsqueda en una lista de palabras previamente definidas.\n\n2.2.1 Entidades\nLa descripción de diferentes técnicas de normalización empieza con el manejo de entidades en el texto. Algunas entidades que se tratarán serán los nombres de usuario, números o URLs mencionados en un texto. Por otro lado están las acciones que se realizarán a las entidades encontradas, estas acciones corresponden a su borrado o remplazo por algún otro toquen.\n\n2.2.1.1 Usuarios\nEn esta sección se trabajará con los nombres de usuarios que siguen el formato usado por Twitter. En un tuit, los nombres de usuarios son aquellas palabras que inician con el caracter @ y terminan con un espacio o caracter terminal. Las acciones que se realizarán con los nombres de usuario encontrados serán su borrado o reemplazo por una etiqueta en particular.\nEl procedimiento para encontrar los nombres de usuarios es mediante expresiones regulares, en particular se usa la expresión @\\S+, tal y como se muestra en el siguiente ejemplo.\n\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"\", text)\n\n'Hola   te está buscando'\n\n\nLa segunda acción es reemplazar cada nombre de usuario por una etiqueta particular, en el siguiente ejemplo se reemplaza por la etiqueta _usr.\n\ntext = 'Hola @xx, @mm te está buscando'\nre.sub(r\"@\\S+\", \"_usr\", text)\n\n'Hola _usr _usr te está buscando'\n\n\n\n\n2.2.1.2 URL\nLos ejemplos anteriores se pueden adaptar para manejar URL; solamente es necesario adecuar la expresión regular que identifica una URL. En el siguiente ejemplo se muestra como se pueden borrar las URLs que aparecen en un texto.\n\ntext = \"puedes verificar que http://google.com esté funcionando\"\nre.sub(r\"https?://\\S+\", \"\", text)\n\n'puedes verificar que  esté funcionando'\n\n\n\n\n2.2.1.3 Números\nThe previous code can be modified to deal with numbers and replace the number found with a shared label such as _num.\n\ntext = \"acabamos de ganar 10 M\"\nre.sub(r\"\\d\\d*\\.?\\d*|\\d*\\.\\d\\d*\", \"_num\", text)\n\n'acabamos de ganar _num M'\n\n\n\n\n\n2.2.2 Ortografía\nEl siguiente bloque de normalizaciones agrupa aquellas modificaciones que se realizan a algún componente de tal manera que aunque impacta en su ortografía puede ser utilizado para reducir la dimensión y se ve reflejado en la complejidad del algoritmo.\n\n2.2.2.1 Mayúsculas y Minúsculas\nLa primera de estas transformaciones es convertir todas los caracteres a minúsculas. Como se puede observar esta transformación hace que el vocabulario se reduzca, por ejemplo, las palabras México o MÉXICO son representados por la palabra méxico. Esta operación se puede realizar con la función lower tal y cómo se muestra a continuación.\n\ntext = \"México\"\ntext.lower()\n\n'méxico'\n\n\n\n\n2.2.2.2 Signos de Puntuación\nLos signos de puntuación son necesarios para tareas como la generación de textos, pero existen otras aplicaciones donde los signos de puntuación tienen un efecto positivo en el rendimiento del algorithm, este es el caso de tareas de categorización de texto. El efecto que tiene el quitar los signos de puntuación es que el vocabulario se reduce. Los símbolos de puntuación se pueden remover teniendo una lista de los mismos, esta lista de signos de puntuación se encuentra en la variable SKIP_SYMBOLS y el siguiente código muestra un procedimiento para quitarlos.\n\ntext = \"¡Hola! buenos días:\"\noutput = \"\"\nfor x in text:\n    if x in SKIP_SYMBOLS:\n        continue\n    output += x\noutput\n\n'Hola buenos días'\n\n\n\n\n2.2.2.3 Símbolos Diacríticos\nContinuando con la misma idea de reducir el vocabulario, es común eliminar los símbolos diacríticos en las palabras. Esta transformación también tiene el objetivo de normalizar aquellos textos informales donde los símbolos diacríticos son usado con una menor frecuencia, en particular los acentos en el caso del español. Por ejemplo, es común encontrar la palabra México escrita como Mexico.\nEl siguiente código muestra un procedimiento para eliminar los símbolos diacríticos.\n\ntext = 'México'\noutput = \"\"\nfor x in unicodedata.normalize('NFD', text):\n    o = ord(x)\n    if 0x300 &lt;= o and o &lt;= 0x036F:\n        continue\n    output += x\noutput\n\n'Mexico'"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-normalizacion-semantica",
    "href": "capitulos/02ManejandoTexto.html#sec-normalizacion-semantica",
    "title": "2  Manejando Texto",
    "section": "2.3 Normalización Semántica",
    "text": "2.3 Normalización Semántica\nLas siguientes normalizaciones comparten el objetivo con las normalizaciones presentadas hasta este momento, el cual es la reducción del vocabulario; la diferencia es que las siguientes utilizan el significado o uso de la palabra.\n\n2.3.1 Palabras Comunes\nLas palabras comunes (stop words) son palabras utilizadas frecuentemente en el lenguaje, las cuales son necesarias para comunicación, pero no aportan información para discriminar un texto de acuerdo a su significado.\nThe stop words are the most frequent words used in the language. These words are essential to communicate but are not so much on tasks where the aim is to discriminate texts according to their meaning.\nLas palabras vacías se pueden guardar en un diccionario y el proceso de identificación consiste en buscar la existencia de la palabra en el diccionario. Una vez que la palabra analizada se encuentra en el diccionario, se procede a quitarla o cambiarla por un token particular. El proceso de borrado se muestra en el siguiente código.\n\nlang = LangDependency('spanish')\n\ntext = '¡Buenos días! El día de hoy tendremos un día cálido.'\noutput = []\nfor word in text.split():\n    if word.lower() in lang.stopwords[len(word)]:\n        continue\n    output.append(word)\noutput = \" \".join(output) \noutput\n\n'¡Buenos días! día hoy día cálido.'\n\n\n\n\n2.3.2 Lematización y Reducción a la Raíz\nLa idea de lematización y reducción a la raíz (stemming) es transformar una palabra a su raíz mediante un proceso heurístico o morfológico. Por ejemplo, las palabras jugando o jugaron se transforman a la palabra jugar.\nEl siguiente código muestra el proceso de reducción a la raíz utilizando la clase SnowballStemmer.\n\nstemmer = SnowballStemmer('spanish')\n\ntext = 'Estoy jugando futbol con mis amigos'\noutput = []\nfor word in text.split():\n    w = stemmer.stem(word)\n    output.append(w)\noutput = \" \".join(output) \noutput\n\n'estoy jug futbol con mis amig'"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#sec-segmentacion",
    "href": "capitulos/02ManejandoTexto.html#sec-segmentacion",
    "title": "2  Manejando Texto",
    "section": "2.4 Segmentación",
    "text": "2.4 Segmentación\nUna vez que el texto ha sido normalizado es necesario segmentarlo (tokenize) a sus componentes fundamentales, e.g., palabras o gramas de caracteres (q-grams) o de palabras (n-grams). Existen diferentes métodos para segmentar un texto, probablemente una de las más sencillas es asumir que una palabra está limitada entre dos espacios o signos de puntuación. Partiendo de el encontrar la palabra se empieza a generar los gramas de palabras, e.g., bigramas, o los gramas de caracteres si se desea solo generarlos a partir de las palabras.\n\n2.4.1 Gramas de Palabras (n-grams)\nEl primer método de segmentación revisado es la creación de los gramas de palabras. El primer paso es encontrar las palabras las cuales se pueden encontrar mediante la función split; una vez que las palabras están definidas estás se pueden unir para generar los gramas de palabras del tamaño deseado, tal y como se muestra en el siguiente código.\n\ntext = 'Estoy jugando futbol con mis amigos'\nwords = text.split()\nn = 3\nn_grams = []\nfor a in zip(*[words[i:] for i in range(n)]):\n    n_grams.append(\"~\".join(a))\nn_grams\n\n['Estoy~jugando~futbol',\n 'jugando~futbol~con',\n 'futbol~con~mis',\n 'con~mis~amigos']\n\n\n\n\n2.4.2 Gramas de Caracteres (q-grams)\nLa segmentación de gramas de caracteres complementa los gramas de palabras. Los gramas de caracteres están definidos como la subcadena de longitud \\(q\\). Este tipo de segmentación tiene la característica de que es agnóstica al lenguaje, es decir, se puede aplicar en cualquier idioma; contrastando, los gramas de palabras se pueden aplicar solo a los lenguajes que tienen definido el concepto de palabra, por ejemplo en el idioma chino las palabras no se pueden identificar como se pueden identificar en el español o inglés. La segunda característica importante es que ayuda en el problema de errores ortográficos, siguiendo una perspectiva de similitud aproximada.\nEl código para realizar los gramas de caracteres es similar a la presentada anteriormente, siendo la diferencia que el ciclo está por los caracteres en lugar de la palabras como se había realizado. El siguiente código muestra una implementación para realizar gramas de caracteres.\n\ntext = 'Estoy jugando'\nq = 4\nq_grams = []\nfor a in zip(*[text[i:] for i in range(q)]):\n    q_grams.append(\"\".join(a))\nq_grams\n\n['Esto',\n 'stoy',\n 'toy ',\n 'oy j',\n 'y ju',\n ' jug',\n 'juga',\n 'ugan',\n 'gand',\n 'ando']"
  },
  {
    "objectID": "capitulos/02ManejandoTexto.html#textmodel",
    "href": "capitulos/02ManejandoTexto.html#textmodel",
    "title": "2  Manejando Texto",
    "section": "2.5 TextModel",
    "text": "2.5 TextModel\nHabiendo descrito diferentes tipos de normalización (sintáctica y semántica) y el proceso de segmentación es momento para describir la librería B4MSA (Tellez, Miranda-Jiménez, Graff, Moctezuma, Suárez, et al. (2017)) que implementa estos procedimientos; específicamente, el punto de acceso de estos procedimientos corresponde a la clase TextModel. El método TextModel.text_transformations es el que realiza todos los métodos de normalización (Sección 2.2 y Sección 2.3) y el método TextModel.tokenize es el encargado de realizar la segmentación (Sección 2.4) siguiendo el flujo mostrado en la Figura 2.1.\n\n2.5.1 Normalizaciones\nEl primer conjunto de parámetros que se describen son los que corresponden a las entidades (Sección 2.2.1). Estos parámetros tiene tres opciones, borrar (OPTION_DELETE), remplazar (OPTION_GROUP) o ignorar. Los nombres de los parámetros son:\n\nusr_option\nurl_option\nnum_option\n\nque corresponden al procesamiento de usuarios, URL y números respectivamente. Adicionalmente, TextModel trata los emojis, hashtags y nombres, mediante los siguientes parámetros:\n\nemo_option\nhashtag_option\nent_option\n\nPor ejemplo, el siguiente código muestra como se borra el usuario y se reemplaza un hashtag; se puede observar que en la respuesta se cambian todos los espacios por el caracter ~ y se incluye ese mismo al inicio y final del texto.\n\ntm = TextModel(hashtag_option=OPTION_GROUP,\n               usr_option=OPTION_DELETE)\ntexto = 'mira @xyz estoy triste. #UnDiaLluvioso'\ntm.text_transformations(texto)\n\n'~mira~estoy~triste.~_htag~'\n\n\nSiguiendo con las transformaciones sintácticas, toca el tiempo a describir aquellas que relacionadas a la ortografía (Sección 2.2.2) las cuales corresponden a la conversión a minúsculas, borrado de signos de puntuación y símbolos diacríticos. Estas normalizaciones se activan con los siguiente parámetros.\n\nlc\ndel_punc\ndel_diac\n\nEn el siguiente ejemplo se transforman el texto a minúscula y se remueven los signos de puntuación.\n\ntm = TextModel(lc=True,\n               del_punc=True,\n               del_diac=False)\ntexto = 'Hoy está despejado.'\ntm.text_transformations(texto)\n\n'~hoy~está~despejado~'\n\n\nLas normalizaciones semánticas (Sección 2.3) que se tienen implementadas en la librería corresponden al borrado de palabras comunes y reducción a la raíz; estás se pueden activar con los siguientes parámetros.\n\nstopwords\nstemming\n\nPor ejemplo, las siguientes instrucciones quitan las palabras comunes y realizan una reducción a la raíz.\n\ntm = TextModel(lang='es',\n               stopwords=OPTION_DELETE,\n               stemming=True)\ntexto = 'el clima es perfecto'\ntm.text_transformations(texto)\n\n'~clim~perfect~'\n\n\n\n\n2.5.2 Segmentación\nEl paso final es describir el uso de la segmentación. La librería utiliza el parámetro token_list para indicar el tipo de segmentación que se desea realizar. El formato es una lista de número, donde el valor indica el tipo de segmentación. El número \\(1\\) indica que se realizará una segmentación por palabras, los número positivo corresponden a los gramas de caracteres y los números negativos a los gramas de palabras.\nPor ejemplo, utilizando las normalizaciones que se tienen por defecto, el siguiente código segmenta utilizando gramas de caracteres de tamañan \\(4.\\)\n\ntm = TextModel(token_list=[4])\ntm.tokenize('buenos días')\n\n['q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n\n\npara poder identificar cuando se trata de un segmento que corresponde a una palabra o un grama de caracteres, a los últimos se les agrega el prefijo q:. Cabe mencionar que por defecto se remueven los símbolos diacríticos.\nEl ejemplo anterior, se utiliza para generar un grama de palabras de tamaño \\(2.\\) Como se ha mencionado los gramas de palabras se especifican con números negativos siendo el valor absoluto el tamaño del grama.\n\ntm = TextModel(token_list=[-2])\ntm.tokenize('buenos días')\n\n['buenos~dias']\n\n\nPara completar la explicación, se combinan la segmentación de gramas de caracteres y palabras además de incluir las palabras en la segmentación.\n\ntm = TextModel(token_list=[4, -2, -1])\ntm.tokenize('buenos días')\n\n['buenos~dias',\n 'buenos',\n 'dias',\n 'q:~bue',\n 'q:buen',\n 'q:ueno',\n 'q:enos',\n 'q:nos~',\n 'q:os~d',\n 'q:s~di',\n 'q:~dia',\n 'q:dias',\n 'q:ias~']\n\n\n\n\n\n\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Oscar S. Siordia, y Elio A. Villaseñor. 2017. «A case study of Spanish text transformations for twitter sentiment analysis». Expert Systems with Applications 81: 457-71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma, Ranyart R. Suárez, y Oscar S. Siordia. 2017. «A Simple Approach to Multilingual Polarity Classification in Twitter». Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, y Mario Graff. 2018. «An automated text categorization framework based on hyperparameter optimization». Knowledge-Based Systems 149: 110-23. https://doi.org/10.1016/j.knosys.2018.03.003."
  },
  {
    "objectID": "capitulos/03ModeladoLenguaje.html",
    "href": "capitulos/03ModeladoLenguaje.html",
    "title": "3  Modelado de Lenguaje",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "href": "capitulos/04ClasificacionTexto.html#paquetes-usados",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom microtc.utils import tweet_iterator, load_model, save_model\nfrom b4msa.textmodel import TextModel\nfrom EvoMSA.tests.test_base import TWEETS\nfrom EvoMSA.utils import bootstrap_confidence_interval\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom sklearn.naive_bayes import MultinomialNB\nfrom scipy.stats import norm, multinomial, multivariate_normal\nfrom scipy.special import logsumexp\nfrom collections import Counter\nfrom matplotlib import pylab as plt\nfrom os.path import join\nimport numpy as np\n\n\nVideo explicando la unidad"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#introducción",
    "href": "capitulos/04ClasificacionTexto.html#introducción",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.1 Introducción",
    "text": "4.1 Introducción\nText Categorization is an NLP task that deals with creating algorithms capable of identifying the category of a text from a set of predefined categories. For example, sentiment analysis belongs to this task, and the aim is to detect the polarity (e.g., positive, neutral, or negative) of a text. Furthermore, different NLP tasks that initially seem unrelated to this problem can be formulated as a classification one such as question answering and sentence entailment, to mention a few.\nText Categorization can be tackled from different perspectives; the one followed here is to treat it as a supervised learning problem. As in any supervised learning problem, the starting point is a set of pairs, where the first element of the pair is the input and the second one corresponds to the output. Let \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text."
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "href": "capitulos/04ClasificacionTexto.html#sec-categorical-distribution",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.2 Modelado Probabilistico (Distribución Categórica)",
    "text": "4.2 Modelado Probabilistico (Distribución Categórica)\n\n4.2.1 Problema Sintético\nThe description of Bayes’ theorem continues with an example of a Categorical distribution. A Categorical distribution can simulate the drawn of \\(K\\) events that can be encoded as characters, and \\(\\ell\\) repetitions can be represented as a sequence of characters. Consequently, the distribution can illustrate the generation sequences associated with different classes, e.g., positive or negative.\nThe first step is to create the dataset. As done previously, two distributions are defined, one for each class; it can be observed that each distribution has different parameters. The second step is to sample these distributions; the distributions are sampled 1000 times with the following procedure. Each time, a random variable representing the number of outcomes taken from each distribution is drawn from a Normal \\(\\mathcal N(15, 3)\\) and stored in the variable length. The random variable indicates the number of outcomes for each Categorical distribution; the results are transformed into a sequence, associated to the label corresponding to the positive and negative class, and stored in the list D.\npos = multinomial(1, [0.20, 0.20, 0.35, 0.25])\nneg = multinomial(1, [0.35, 0.20, 0.25, 0.20])\nlength = norm(loc=15, scale=3)\nD = []\nm = {k: chr(122 - k) for k in range(4)}\nid2w = lambda x: \" \".join([m[_] for _ in x.argmax(axis=1)])\nfor l in length.rvs(size=1000):\n    D.append((id2w(pos.rvs(round(l))), 1))\n    D.append((id2w(neg.rvs(round(l))), 0))\nThe following table shows four examples of this process; the first column contains the sequence, and the second the associated label.\n\n\n\nText\nLabel\n\n\n\n\nx w x x z w y\npositive\n\n\ny w z z z x w\nnegative\n\n\nz x x x z x z w x w\npositive\n\n\nx w z w y z z z z w\nnegative\n\n\n\nAs done previously, the first step is to compute the likelihood given that dataset; considering that the data comes from a Categorical distribution, the procedure to estimate the parameters is similar to the ones used to estimate the prior. The following code estimates the data parameters corresponding to the positive class. It can be observed that the parameters estimated are similar to the ones used to generate the dataset.\nD_pos = []\n[D_pos.extend(data.split()) for data, k in D if k == 1]\nwords, l_pos = np.unique(D_pos, return_counts=True)\nw2id = {v: k for k, v in enumerate(words)}\nl_pos = l_pos / l_pos.sum()\nl_pos\narray([0.25489421, 0.33854064, 0.20773186, 0.1988333 ])\nAn equivalent procedure is performed to calculate the likelihood of the negative class.\nD_neg = []\n[D_neg.extend(data.split()) for data, k in D if k == 0]\n_, l_neg = np.unique(D_neg, return_counts=True)\nl_neg = l_neg / l_neg.sum()\nThe prior is estimated with the following code, equivalent to the one used on all the examples seen so far.\n_, priors = np.unique([k for _, k in D], return_counts=True)\nN = priors.sum()\nprior_pos = priors[1] / N\nprior_neg = priors[0] / N\nOnce the parameters have been identified, these can be used to predict the class of a given sequence. The first step is to compute the likelihood, e.g., \\(\\mathbb P(\\)w w x z\\(\\mid \\mathcal Y)\\). It can be observed that the sequence needs to be transformed into tokens which can be done with the split method. Then, the token is converted into an index using the mapping w2id; once the index is retrieved, it can be used to obtain the parameter associated with the word. The likelihood is the product of all the probabilities; however, this product is computed in log space.\ndef likelihood(params, txt):\n    params = np.log(params)\n    _ = [params[w2id[x]] for x in txt.split()]\n    tot = sum(_)\n    return np.exp(tot)\nThe likelihood combined with the prior for all the classes produces the evidence, which subsequently is used to calculate the posterior distribution. The posterior is then used to predict the class for all the sequences in \\(\\mathcal D\\). The predictions are stored in the variable hy.\npost_pos = [likelihood(l_pos, x) * prior_pos for x, _ in D]\npost_neg = [likelihood(l_neg, x) * prior_neg for x, _ in D]\nevidence = np.vstack([post_pos, post_neg]).sum(axis=0)\npost_pos /= evidence\npost_neg /= evidence\nhy = np.where(post_pos &gt; post_neg, 1, 0)\n\n\n4.2.2 Clasificador de Texto\nThe approach followed on text categorization is to treat it as supervised learning problem where the starting point is a dataset \\(\\mathcal D = \\{(\\text{text}_i, y_i) \\mid i=1,\\ldots, N\\}\\) where \\(y \\in \\{c_1, \\ldots c_K\\}\\) and \\(\\text{text}_i\\) is a text. For example, the next code uses a toy sentiment analysis dataset with four classes: negative (N), neutral (NEU), absence of polarity (NONE), and positive (P).\nD = [(x['text'], x['klass']) for x in tweet_iterator(TWEETS)]\nAs can be observed, \\(\\mathcal D\\) is equivalent to the one used in the Categorical Distribution example. The difference is that sequence of letters is changed with a sentence. Nonetheless, a feasible approach is to obtain the tokens using the split method. Another approach is to retrieve the tokens using a Tokenizer, as covered in the Text Normalization Section.\nThe following code uses the TextModel class to tokenize the text using words as the tokenizer; the tokenized text is stored in the variable D.\ntm = TextModel(token_list=[-1])\ntok = tm.tokenize\nD = [(tok(x), y) for x, y in D]\nBefore estimating the likelihood parameters, it is needed to encode the tokens using an index; by doing it, it is possible to store the parameters in an array and compute everything numpy operations. The following code encodes each token with a unique index; the mapping is in the dictionary w2id.\nwords = set()\n[words.update(x) for x, y in D]\nw2id = {v: k for k, v in enumerate(words)}\nPreviously, the classes have been represented using natural numbers. The positive class has been associated with the number \\(1\\), whereas the negative class with \\(0\\). However, in this dataset, the classes are strings. It was decided to encode them as numbers to facilitate subsequent operations. The encoding process can be performed simultaneously with the estimation of the prior of each class. Please note that the priors are stored using the logarithm in the variable priors.\nuniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\npriors = np.log(priors / priors.sum())\nuniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\nIt is time to estimate the likelihood parameters for each of the classes. It is assumed that the data comes from a Categorical distribution and that each token is independent. The likelihood parameters can be stored in a matrix (variable l_tokens) with \\(K\\) rows, each row contains the parameters of the class, and the number of columns corresponds to the vocabulary’s size. The first step is to calculate the frequency of each token per class which can be done with the following code.\nl_tokens = np.zeros((len(uniq_labels), len(w2id)))\nfor x, y in D:\n    w = l_tokens[uniq_labels[y]]\n    cnt = Counter(x)\n    for i, v in cnt.items():\n        w[w2id[i]] += v\nThe next step is to normalize the frequency. However, before normalizing it, it is being used a Laplace smoothing with a value \\(0.1\\). Therefore, the constant \\(0.1\\) is added to all the matrix elements. The next step is to normalize (second line), and finally, the parameters are stored using the logarithm.\nl_tokens += 0.1\nl_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\nl_tokens = np.log(l_tokens)\n\n4.2.2.1 Prediction\nOnce all the parameters have been estimated, it is time to use the model to classify any text. The following function computes the posterior distribution. The first step is to tokenize the text (second line) and compute the frequency of each token in the text. The frequency stored in the dictionary cnt is converted into the vector x using the mapping function w2id. The final step is to compute the product of the likelihood and the prior. The product is computed in log-space; thus, this is done using the likelihood and the prior sum. The last step is to compute the evidence and normalize the result; the evidence is computed with the function logsumexp.\ndef posterior(txt):\n    x = np.zeros(len(w2id))\n    cnt = Counter(tm.tokenize(txt))\n    for i, v in cnt.items():\n        try:\n            x[w2id[i]] += v\n        except KeyError:\n            continue\n    _ = (x * l_tokens).sum(axis=1) + priors\n    l = np.exp(_ - logsumexp(_))\n    return l\nThe posterior function can predict all the text in \\(\\mathcal D\\); the predictions are used to compute the model’s accuracy. In order to compute the accuracy, the classes in \\(\\mathcal D\\) need to be transformed using the nomenclature of the likelihood matrix and priors vector; this is done with the uniq_labels dictionary (second line).\nhy = np.array([posterior(x).argmax() for x, _ in D])\ny = np.array([uniq_labels[y] for _, y in D])\n(y == hy).mean()\n0.974\n\n\n4.2.2.2 Training\nSolving supervised learning problems requires two phases; one is the training phase, and the other is the prediction. The posterior function handles the later phase, and it is missing to organize the code described in a training function. The following code describes the training function; it requires the dataset’s parameters and an instance of TextModel.\ndef training(D, tm):\n    tok = tm.tokenize\n    D =[(tok(x), y) for x, y in D]\n    words = set()\n    [words.update(x) for x, y in D]\n    w2id = {v: k for k, v in enumerate(words)}\n    uniq_labels, priors = np.unique([k for _, k in D], return_counts=True)\n    priors = np.log(priors / priors.sum())\n    uniq_labels = {str(v): k for k, v in enumerate(uniq_labels)}\n    l_tokens = np.zeros((len(uniq_labels), len(w2id)))\n    for x, y in D:\n        w = l_tokens[uniq_labels[y]]\n        cnt = Counter(x)\n        for i, v in cnt.items():\n            w[w2id[i]] += v\n    l_tokens += 0.1\n    l_tokens = l_tokens / np.atleast_2d(l_tokens.sum(axis=1)).T\n    l_tokens = np.log(l_tokens)\n    return w2id, uniq_labels, l_tokens, priors"
  },
  {
    "objectID": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "href": "capitulos/04ClasificacionTexto.html#sec-tc-vectorial",
    "title": "4  Fundamentos de Clasificación de Texto",
    "section": "4.3 Modelado Vectorial",
    "text": "4.3 Modelado Vectorial\nxxx"
  },
  {
    "objectID": "capitulos/05RepresentacionTexto.html",
    "href": "capitulos/05RepresentacionTexto.html",
    "title": "5  Representación de Texto",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#paquetes-usados",
    "href": "capitulos/06MezclaModelos.html#paquetes-usados",
    "title": "6  Mezcla de Modelos",
    "section": "Paquetes usados",
    "text": "Paquetes usados\n\nfrom EvoMSA import BoW\nfrom microtc.utils import tweet_iterator\n\n\n\nArchive:  delitos.zip\n   creating: delitos/\n  inflating: delitos/delitos_ingeotec_Es_test.json  \n  inflating: delitos/delitos_ingeotec_Es_train.json  \n\n\n\nVideo explicando la unidad"
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#conjunto-de-datos",
    "href": "capitulos/06MezclaModelos.html#conjunto-de-datos",
    "title": "6  Mezcla de Modelos",
    "section": "6.1 Conjunto de Datos",
    "text": "6.1 Conjunto de Datos\nEl conjunto de datos se puede conseguir en la página de Delitos aunque en esta dirección es necesario poblar los textos dado que solamente se encuentra el identificador del Tweet.\nPara leer los datos del conjunto de entrenamiento y prueba se utilizan las siguientes instrucciones. En la variable D se tiene los datos que se utilizarán para entrenar el clasificador basado en la bolsa de palabras y en Dtest los datos del conjunto de prueba, que son usados para medir el rendimiento del clasificador.\n\nfname = 'delitos/delitos_ingeotec_Es_train.json'\nfname_test = 'delitos/delitos_ingeotec_Es_test.json'\nD = list(tweet_iterator(fname))\nDtest = list(tweet_iterator(fname_test))\n\nEn la siguiente instrucción se observa el primer elemento del conjunto de entrenamiento. Se puede observar que en el campo text se encuentra el texto, el campo klass representa la etiqueta o clase, donde \\(0\\) representa la clase negativa y \\(1\\) la clase positiva, es decir, la presencia de un delito. El campo id es el identificador del Tweet y annotations son las clases dadas por los etiquetadores a ese ejemplo.\n\nD[81]\n\n{'annotations': [0, 0, 0],\n 'id': 1107040319986696195,\n 'klass': 0,\n 'text': 'To loco'}"
  },
  {
    "objectID": "capitulos/06MezclaModelos.html#bolsa-de-palabras-dispersa",
    "href": "capitulos/06MezclaModelos.html#bolsa-de-palabras-dispersa",
    "title": "6  Mezcla de Modelos",
    "section": "6.2 Bolsa de Palabras Dispersa",
    "text": "6.2 Bolsa de Palabras Dispersa"
  },
  {
    "objectID": "capitulos/07TareasClasificacion.html",
    "href": "capitulos/07TareasClasificacion.html",
    "title": "7  Tareas de Clasificación de Texto",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/08BasesConocimiento.html",
    "href": "capitulos/08BasesConocimiento.html",
    "title": "8  Bases de Conocimiento",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/09Visualizacion.html",
    "href": "capitulos/09Visualizacion.html",
    "title": "9  Visualización",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/10Conclusiones.html",
    "href": "capitulos/10Conclusiones.html",
    "title": "10  Conclusiones",
    "section": "",
    "text": "El objetivo de la unidad es"
  },
  {
    "objectID": "capitulos/11Referencias.html",
    "href": "capitulos/11Referencias.html",
    "title": "Referencias",
    "section": "",
    "text": "Tellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nOscar S. Siordia, and Elio A. Villaseñor. 2017. “A Case Study of\nSpanish Text Transformations for Twitter Sentiment Analysis.”\nExpert Systems with Applications 81: 457–71. https://doi.org/https://doi.org/10.1016/j.eswa.2017.03.071.\n\n\nTellez, Eric S., Sabino Miranda-Jiménez, Mario Graff, Daniela Moctezuma,\nRanyart R. Suárez, and Oscar S. Siordia. 2017. “A\nSimple Approach to Multilingual\nPolarity Classification in\nTwitter.” Pattern Recognition Letters. https://doi.org/10.1016/j.patrec.2017.05.024.\n\n\nTellez, Eric S., Daniela Moctezuma, Sabino Miranda-Jiménez, and Mario\nGraff. 2018. “An Automated Text Categorization Framework Based on\nHyperparameter Optimization.” Knowledge-Based Systems\n149: 110–23. https://doi.org/10.1016/j.knosys.2018.03.003."
  }
]